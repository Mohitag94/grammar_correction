{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ed6398",
   "metadata": {},
   "source": [
    "#### **7PAM2015-0509-2024 -- Research Methods in Data Science**\n",
    "##### Mult-Task -- Grammar Correctiona and Paraphrasing -- Implementation on T5 through LoRA.\n",
    "---\n",
    "**Mohit Agarwal (Student ID-22031257)**\n",
    "\n",
    "This notebook trains and evaluate the model.\n",
    "\n",
    "\n",
    "##### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70186f37",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af17f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# core T5 training libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# evalution libraries\n",
    "from evaluate import load\n",
    "import nltk\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebeb50",
   "metadata": {},
   "source": [
    "Changing the default styles and palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82506b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "# setting context\n",
    "sns.set_context(\"paper\")\n",
    "# setting palette\n",
    "sns.set_palette(\"deep\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e77b1",
   "metadata": {},
   "source": [
    "### Defining Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffffbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model name\n",
    "MODELNAME = \"t5-small\"\n",
    "# define tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME)\n",
    "# define model\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODELNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6134",
   "metadata": {},
   "source": [
    "### JFLEG Dataset Class for Preparation and Augmentation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    \"\"\"\n",
    "    A comprehensive dataset processor for JFLEG (JHU FLuency-Extended GUG) grammar correction data.\n",
    "\n",
    "    This class handles the complete pipeline for preparing JFLEG data for T5-based grammar correction\n",
    "    training, including data loading, preprocessing, augmentation, tokenization, and train/validation/test\n",
    "    splitting. The JFLEG dataset contains 1,511 examples with 4 human-written corrections each, focusing\n",
    "    on fluency improvements rather than minimal edits.\n",
    "\n",
    "    Key Features:\n",
    "        - Comprehensive text preprocessing to handle formatting issues\n",
    "        - Data augmentation using all 4 JFLEG corrections per sentence\n",
    "        - Proper tokenization for T5 sequence-to-sequence training\n",
    "        - Train/validation/test splitting with preserved evaluation metadata\n",
    "        - Temperature-scaled mixing support for multi-task learning\n",
    "\n",
    "    Dataset Sources:\n",
    "        - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "        - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization\n",
    "        test_split_ratio (float): Proportion of validation data to use for testing\n",
    "        train_data (Dataset): JFLEG validation split used for training\n",
    "        validation_data (Dataset): JFLEG test split used for validation/testing\n",
    "\n",
    "    Example:\n",
    "        >>> from transformers import T5Tokenizer\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> dataset = JFLEGDataset(tokenizer, max_length=256, test_split_ratio=0.10)\n",
    "        >>> train_data, val_data, test_data = dataset.create_train_val_test_datasets()\n",
    "\n",
    "    References:\n",
    "        - JFLEG Paper: Napoles et al. (2017) \"JFLEG: A Fluency Corpus and Benchmark \n",
    "          for Grammatical Error Correction\"\n",
    "        - Dataset: https://huggingface.co/datasets/jhu-clsp/jfleg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length=256, test_split_ratio=0.10):\n",
    "        \"\"\"\n",
    "        Initialize the JFLEG dataset processor with specified configuration.\n",
    "\n",
    "        Sets up the dataset processor with the provided tokenizer and configuration\n",
    "        parameters, then loads the raw JFLEG datasets for subsequent processing.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (T5Tokenizer): HuggingFace T5 tokenizer instance for text processing.\n",
    "                Must be a properly initialized T5 tokenizer (e.g., from t5-base).\n",
    "            max_length (int, optional): Maximum sequence length for tokenization. \n",
    "                Sequences longer than this will be truncated. Defaults to 256.\n",
    "                Recommended range: 128-512 depending on GPU memory constraints.\n",
    "            test_split_ratio (float, optional): Proportion of validation data to reserve \n",
    "                for final testing. Must be between 0.0 and 1.0. Defaults to 0.10 (10%).\n",
    "                The remaining validation data will be used for model validation during training.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If test_split_ratio is not between 0.0 and 1.0\n",
    "            TypeError: If tokenizer is not a valid T5Tokenizer instance\n",
    "\n",
    "        Note:\n",
    "            The JFLEG dataset splits are used as follows:\n",
    "            - JFLEG 'validation' split → Training data (with augmentation)\n",
    "            - JFLEG 'test' split → Validation and test data (split according to test_split_ratio)\n",
    "\n",
    "            This approach follows standard practice since JFLEG's validation split is larger\n",
    "            and more suitable for training, while the test split is reserved for evaluation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        # Validate test_split_ratio\n",
    "        if not 0.0 <= test_split_ratio <= 1.0:\n",
    "            raise ValueError(\n",
    "                f\"test_split_ratio must be between 0.0 and 1.0, got {test_split_ratio}\")\n",
    "\n",
    "        # Load the JFLEG datasets\n",
    "        print(f\"[INFO] Initializing JFLEG Dataset Processor...\")\n",
    "        print(\n",
    "            f\"[INFO] Max length: {max_length}, Test split ratio: {test_split_ratio:.1%}\")\n",
    "\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG validation split: {len(self.train_data)} examples\")\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG test split: {len(self.validation_data)} examples\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "                text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "                str: The preprocessed and normalized text, or the original input if\n",
    "                        it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "                - Removes multiple consecutive dashes (-- → \"\")\n",
    "                - Fixes decimal formatting (0 . 1 → 0.1)\n",
    "                - Fixes fraction formatting (1 / 2 → 1/2)\n",
    "                - Removes leading zeros in decimals (00.5 → 0.5)\n",
    "                - Joins split numbers (1 2 3 4 → 1234)\n",
    "                - Fixes punctuation spacing (word , → word,)\n",
    "                - Normalizes quote spacing (\" word \" → \"word\")\n",
    "                - Collapses multiple spaces to single spaces\n",
    "                - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "\n",
    "        # if not text or not isinstance(text, str):\n",
    "        #     return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 → 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 → 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 → 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_augmentation(self, data, augment=True):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to JFLEG dataset using all available corrections.\n",
    "\n",
    "        This function processes JFLEG examples to create augmented data by utilizing\n",
    "        all 4 human-written corrections per sentence. Each original sentence is paired with\n",
    "        each of its corrections to create multiple training examples, significantly increasing\n",
    "        the dataset size and providing the model with diverse correction targets.\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): List of JFLEG dataset examples, where each example contains:\n",
    "                    - 'sentence' (str): Original grammatically incorrect sentence\n",
    "                    - 'corrections' (List[str]): List of 4 human-written corrections\n",
    "            augment (bool, optional): Whether to use all corrections for augmentation.\n",
    "                    - If True: Creates 4 examples per input (uses all corrections)\n",
    "                    - If False: Creates 1 example per input (uses only first correction)\n",
    "                    Default is True.\n",
    "\n",
    "        Returns:\n",
    "                List[Dict]: Augmented dataset where each dictionary contains:\n",
    "                        - 'input' (str): Preprocessed input with \"grammar: \" prefix\n",
    "                        - 'target' (str): Preprocessed target correction\n",
    "                        - 'processed_sentence' (str): Preprocessed original sentence\n",
    "                        - 'processed_corrections' (List[str]): All 4 preprocessed corrections for evaluation\n",
    "                        - 'raw_original' (str): Unprocessed original sentence (for debugging)\n",
    "                        - 'raw_corrections' (List[str]): Unprocessed corrections (for debugging)\n",
    "        \"\"\"\n",
    "        # storage for augmented data\n",
    "        augmented_data = []\n",
    "        for items in data:\n",
    "            # getting original sentence -- incorrect\n",
    "            original_sentence = items[\"sentence\"]\n",
    "            # formatting the incorrect sentence\n",
    "            processed_sentence = self._preprocess(original_sentence)\n",
    "\n",
    "            # getting all the original corrected sentences\n",
    "            corrections = items[\"corrections\"]\n",
    "\n",
    "            # formatting all the corrected sentences -- evaluation\n",
    "            processed_corrections = []\n",
    "            # looping over all 4 corrections\n",
    "            for correction in corrections:\n",
    "                if correction.strip():  # Skip empty corrections\n",
    "                    # storing all the processed corrections\n",
    "                    processed_corrections.append(self._preprocess(correction))\n",
    "\n",
    "            # looping over processed corrections\n",
    "            for processed_correction in processed_corrections:\n",
    "                # creating a dataset\n",
    "                augmented_data.append({\n",
    "                    \"input\": f\"grammar: {processed_sentence}\",\n",
    "                    \"target\": processed_correction,\n",
    "                    \"processed_sentence\": processed_sentence,\n",
    "                    \"processed_corrections\": processed_corrections,\n",
    "                    \"raw_original\": original_sentence,\n",
    "                    \"raw_corrections\": corrections\n",
    "                })\n",
    "                # checking if to augment or not\n",
    "                if not augment:\n",
    "                    break\n",
    "        # displaying the length of data\n",
    "        print(\"\\t[INFO] Length of Dataset is: \", len(augmented_data))\n",
    "        return augmented_data\n",
    "\n",
    "    def _apply_tokenization(self, data):\n",
    "        \"\"\"\n",
    "        Apply tokenization to preprocessed JFLEG dataset examples for T5 model training.\n",
    "\n",
    "        This function converts text data (input sentences and target corrections) into \n",
    "        tokenized format suitable for T5 model training. It processes both the input \n",
    "        grammar correction task and the target correction, creating the necessary \n",
    "        input_ids, attention_mask, and labels required by the HuggingFace Trainer.\n",
    "\n",
    "        Args:\n",
    "            data (Dict): A single preprocessed example containing:\n",
    "            - 'input' (str): Preprocessed input text with \"grammar: \" prefix\n",
    "            - 'target' (str): Preprocessed target correction text\n",
    "            - 'processed_sentence' (str): Preprocessed original sentence (preserved but not tokenized)\n",
    "            - 'processed_corrections' (List[str]): All preprocessed corrections (preserved but not tokenized)\n",
    "            - 'raw_original' (str): Raw original sentence (preserved but not tokenized)\n",
    "            - 'raw_corrections' (List[str]): Raw corrections (preserved but not tokenized)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                Dict: Tokenized example ready for model training containing:\n",
    "                        - 'input_ids' (List[int]): Token IDs for the input sequence\n",
    "                        - 'attention_mask' (List[int]): Attention mask for input (1 for real tokens, 0 for padding)\n",
    "                        - 'labels' (List[int]): Token IDs for the target sequence (used for loss computation)\n",
    "\n",
    "        Tokenization Settings:\n",
    "                - max_length (int): Maximum sequence length (defined by self.max_length)\n",
    "                - truncation (bool): True - truncates sequences longer than max_length\n",
    "                - padding (bool): False - no padding applied (Trainer handles dynamic padding)\n",
    "                - return_tensors: None - returns Python lists instead of PyTorch tensors\n",
    "        \"\"\"\n",
    "        # tokenizing the input of the dataset\n",
    "        input_encodings = self.tokenizer(data[\"input\"],\n",
    "                                         max_length=self.max_length,\n",
    "                                         truncation=True,\n",
    "                                         padding=False,  # trainer handles the dynamic padding\n",
    "                                         return_tensors=None)  # returns lists not tensor\n",
    "        # tokenizing the target of the dataset\n",
    "        target_encodings = self.tokenizer(data[\"target\"],\n",
    "                                          max_length=self.max_length,\n",
    "                                          truncation=True,\n",
    "                                          padding=False,  # trainer handles the dynamic padding\n",
    "                                          return_tensors=None)  # returns lists not tensor\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]\n",
    "        }\n",
    "\n",
    "    def create_train_val_test_datasets(self):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets with proper augmentation and tokenization.\n",
    "\n",
    "        This function orchestrates the complete data processing pipeline for JFLEG grammar \n",
    "        correction training. It applies data augmentation, converts to HuggingFace datasets,\n",
    "        applies tokenization, and splits the data into appropriate train/validation/test sets\n",
    "        while preserving essential evaluation metadata.\n",
    "\n",
    "        Processing Pipeline:\n",
    "            1. Apply augmentation to training data (4x expansion using all corrections)\n",
    "            2. Apply augmentation to validation data (no expansion, uses first correction only)\n",
    "            3. Convert Python lists to HuggingFace Datasets\n",
    "            4. Apply tokenization using .map() for efficiency\n",
    "            5. Split validation data into validation and test sets (90%/10%)\n",
    "            6. Preserve evaluation metadata for proper GLEU scoring\n",
    "\n",
    "        Data Sources:\n",
    "            - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "            - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dataset, List[Dict], List[Dict]]: A tuple containing:\n",
    "                - train_dataset (Dataset): HuggingFace Dataset with tokenized training examples\n",
    "                - val_data (List[Dict]): List of tokenized validation examples with metadata\n",
    "                - test_data (List[Dict]): List of tokenized test examples with metadata\n",
    "\n",
    "        Data Augmentation Strategy:\n",
    "            - Training: augment=True (uses all 4 JFLEG corrections per sentence)\n",
    "            - Validation: augment=False (uses only first correction per sentence)\n",
    "        \"\"\"\n",
    "\n",
    "        from datasets import Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        print(\"[INFO] Creating datasets with augmentation and tokenization...\")\n",
    "\n",
    "        # Step 1: Apply augmentation (returns Python lists)\n",
    "        print(\"\\n[INFO] Applying augmentation to training data...\")\n",
    "        train_augmented_list = self._apply_augmentation(\n",
    "            self.train_data, augment=True)\n",
    "\n",
    "        print(\"[INFO] Applying augmentation to validation data...\")\n",
    "        val_augmented_list = self._apply_augmentation(\n",
    "            self.validation_data, augment=False)\n",
    "\n",
    "        # Step 2: Convert Python lists to HuggingFace Datasets\n",
    "        train_augmented_data = Dataset.from_list(train_augmented_list)\n",
    "        val_augmented_data = Dataset.from_list(val_augmented_list)\n",
    "\n",
    "        # Step 3: Apply tokenization using map\n",
    "        print(\"\\n[INFO] Tokenizing training data...\")\n",
    "        train_augmented_map_data = train_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Training Data\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Tokenizing validation data...\")\n",
    "        val_augmented_map_data = val_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Validation Data\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Split validation dataset into validation and test sets\n",
    "        print(\n",
    "            f\"\\n[INFO] Splitting Validation Data ({100-self.test_split_ratio*100:.0f}%/{self.test_split_ratio*100:.0f}%)...\")\n",
    "        val_data, test_data = train_test_split(\n",
    "            list(val_augmented_map_data),\n",
    "            test_size=self.test_split_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Convert Python validation and test lists to HuggingFace Datasets\n",
    "        val_data = Dataset.from_list(val_data)\n",
    "        test_data = Dataset.from_list(test_data)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nDataset Creation Complete:\")\n",
    "        print(f\"\\t[INFO] Training Dataset:   {len(train_augmented_map_data)}\")\n",
    "        print(f\"\\t[INFO] Validation Dataset: {len(val_data)}\")\n",
    "        print(f\"\\t[INFO] Test Dataset:       {len(test_data)}\")\n",
    "\n",
    "        return train_augmented_map_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a952adf",
   "metadata": {},
   "source": [
    "### T5 Model Evaluation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f3b6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarEvaluation:\n",
    "    \"\"\"\n",
    "    Comprehensive Grammar Correction Evaluation Framework.\n",
    "\n",
    "    This class provides a complete evaluation suite for grammar correction systems,\n",
    "    implementing industry-standard metrics specifically designed for assessing\n",
    "    grammatical error correction quality. It combines fluency assessment (GLEU),\n",
    "    semantic preservation (BERTScore), linguistic quality (METEOR), and comprehensive\n",
    "    text statistics.\n",
    "\n",
    "    The evaluation framework is designed for transformer-based models like T5, BERT,\n",
    "    and other sequence-to-sequence architectures fine-tuned on datasets such as JFLEG,\n",
    "    BEA-2019, or custom grammar correction corpora.\n",
    "\n",
    "    Attributes:\n",
    "        bertscore: HuggingFace BERTScore evaluator for semantic similarity\n",
    "        meteor: HuggingFace METEOR evaluator for linguistic quality\n",
    "        metrics_result (Dict): Storage for all computed evaluation metrics\n",
    "\n",
    "    Performance Benchmarks:\n",
    "        - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "        - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent\n",
    "        - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "    References:\n",
    "        - GLEU: Napoles et al. (2017) \"JFLEG: A fluency corpus and benchmark\"\n",
    "        - BERTScore: Zhang et al. (2020) \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        - METEOR: Banerjee & Lavie (2005) \"METEOR: An automatic metric for MT evaluation\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Grammar Evaluation framework.\n",
    "\n",
    "        Sets up evaluation metrics and initializes the results storage structure.\n",
    "        Loads pre-trained models for BERTScore and METEOR evaluation from HuggingFace.\n",
    "\n",
    "        Initializes:\n",
    "            - BERTScore evaluator with microsoft/deberta-xlarge-mnli model\n",
    "            - METEOR evaluator with default configuration\n",
    "            - Results dictionary with nested structure for all metrics\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If required packages (pandas) are not installed\n",
    "\n",
    "        Note:\n",
    "            First initialization may take time to download evaluation models.\n",
    "            Internet connection required for downloading pre-trained models.\n",
    "        \"\"\"\n",
    "        # Load HuggingFace evaluation metrics\n",
    "        self.bertscore = load(\"bertscore\")  # Semantic similarity evaluation\n",
    "        self.meteor = load('meteor')        # Linguistic quality evaluation\n",
    "\n",
    "        # Initialize results storage with hierarchical structure\n",
    "        self.metrics_result = {\n",
    "            \"bertscore\": {              # Semantic preservation metrics\n",
    "                \"precision\": 0.0,       # BERTScore precision\n",
    "                \"recall\": 0.0,          # BERTScore recall\n",
    "                # BERTScore F1 (primary semantic metric)\n",
    "                \"f1\": 0.0\n",
    "            },\n",
    "            \"meteor\": 0.0,              # Linguistic quality score\n",
    "            \"gleu\": 0.0,                # Primary grammar correction metric\n",
    "            \"stats\": {}                 # Comprehensive text statistics\n",
    "        }\n",
    "\n",
    "    def compute_gleu(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute GLEU (Generalized Language Evaluation Understanding) scores for grammar correction.\n",
    "\n",
    "        GLEU is specifically designed for grammar correction evaluation and handles multiple\n",
    "        reference corrections better than traditional BLEU. It measures fluency improvement\n",
    "        while accounting for acceptable variation in correction approaches.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner list\n",
    "                contains multiple valid corrections for the same source sentence (e.g., JFLEG\n",
    "                provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average GLEU score in self.metrics_result[\"gleu\"].\n",
    "\n",
    "        Notes:\n",
    "            - Uses NLTK's sentence_gleu function for computation\n",
    "            - Applies lowercase normalization and word tokenization\n",
    "            - Handles empty or invalid references gracefully with exception handling\n",
    "            - Scores range from 0.0 (no match) to 1.0 (perfect match)\n",
    "            - For grammar correction: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "\n",
    "        Raises:\n",
    "            Exception: Catches and handles any GLEU computation errors by assigning 0.0 score.\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for gleu score\n",
    "        gleu_scores = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # converting predictions to tokens\n",
    "            preds_tokens = nltk.word_tokenize(preds.lower())\n",
    "            # converting all reference to tokens\n",
    "            refs_tokens = [nltk.word_tokenize(ref.lower())\n",
    "                           for ref in refs if ref.strip()]\n",
    "\n",
    "            try:\n",
    "                # computing the score for the gleu\n",
    "                score = sentence_gleu(refs_tokens, preds_tokens)\n",
    "                # updating the storage\n",
    "                gleu_scores.append(score)\n",
    "            except Exception:\n",
    "                gleu_scores.append(0.0)\n",
    "\n",
    "        # computing the average gleu\n",
    "        self.metrics_result[\"gleu\"] = np.mean(gleu_scores)\n",
    "\n",
    "    def compute_bertscore(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute BERTScore metrics (precision, recall, F1) for grammar correction evaluation.\n",
    "\n",
    "        BERTScore measures semantic similarity using contextual embeddings, making it ideal\n",
    "        for evaluating whether grammar corrections preserve semantic meaning while improving\n",
    "        fluency. For each prediction, scores are computed against all available references\n",
    "        and then averaged.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Currently stores results in local variables but doesn't persist them.\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute BERTScore against each of its references\n",
    "            2. Average precision, recall, F1 across references for that prediction\n",
    "            3. Collect averaged scores across all predictions\n",
    "\n",
    "        Notes:\n",
    "            - Uses microsoft/deberta-xlarge-mnli for optimal semantic similarity detection\n",
    "            - Skips empty references automatically\n",
    "            - For grammar correction: F1 > 0.85 indicates excellent semantic preservation\n",
    "            - Each BERTScore call processes one prediction-reference pair\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for bertscore metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing bertscore\n",
    "                    score = self.bertscore.compute(predictions=[preds],\n",
    "                                                   references=[ref],\n",
    "                                                   lang=\"en\",\n",
    "                                                   model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "                    # updating the local storage\n",
    "                    precision.append(score[\"precision\"][0])\n",
    "                    recall.append(score[\"recall\"][0])\n",
    "                    f1.append(score[\"f1\"][0])\n",
    "\n",
    "            # updating bertscore mertics with average\n",
    "            precisions.append(np.mean(precision))\n",
    "            recalls.append(np.mean(recall))\n",
    "            f1s.append(np.mean(f1))\n",
    "\n",
    "        # computing the average bertscore\n",
    "        self.metrics_result[\"bertscore\"] = {\n",
    "            \"precision\": np.mean(precisions),\n",
    "            \"recall\": np.mean(recalls),\n",
    "            \"f1\": np.mean(f1s)\n",
    "        }\n",
    "\n",
    "    def compute_meteor(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute METEOR (Metric for Evaluation of Translation with Explicit ORdering) scores \n",
    "        for grammar correction evaluation.\n",
    "\n",
    "        METEOR is particularly valuable for grammar correction as it incorporates:\n",
    "        - Exact word matching\n",
    "        - Stem matching (handles morphological variations like \"running\" vs \"runs\")\n",
    "        - Synonym matching (recognizes semantically equivalent words)\n",
    "        - Word order penalties\n",
    "\n",
    "        This makes it superior to BLEU for grammar correction where morphological changes\n",
    "        and lexical substitutions are common.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average METEOR score in self.metrics_result[\"meteor\"].\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute METEOR against each of its references\n",
    "            2. Average METEOR scores across references for that prediction  \n",
    "            3. Average across all predictions for final score\n",
    "\n",
    "        Notes:\n",
    "            - METEOR scores range from 0.0 to 1.0 (higher is better)\n",
    "            - For grammar correction: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "            - Handles morphological variations better than BLEU\n",
    "            - Includes recall-oriented evaluation (unlike BLEU's precision focus)\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for meteor metrics\n",
    "        meteors = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            meteor = []\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing meteor\n",
    "                    meteor.append(self.meteor.compute(predictions=[preds],\n",
    "                                                      references=[ref])[\"meteor\"])\n",
    "\n",
    "            # updating meteor mertics with avergae\n",
    "            meteors.append(np.mean(meteor))\n",
    "\n",
    "        # computing the average meteor\n",
    "        self.metrics_result[\"meteor\"] = np.mean(meteors)\n",
    "\n",
    "    def compute_stats(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute comprehensive text statistics for grammar correction evaluation.\n",
    "\n",
    "        This method analyzes various aspects of model predictions versus reference corrections,\n",
    "        providing detailed insights into model behavior patterns, text properties, and \n",
    "        vocabulary usage. All statistics are stored in self.metrics_result[\"stats\"].\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: All statistics are stored in self.metrics_result[\"stats\"] dictionary.\n",
    "\n",
    "        Statistics Computed:\n",
    "\n",
    "            **Sample Information:**\n",
    "            - num_samples: Total number of predictions evaluated\n",
    "            - total_references: Total number of reference corrections across all sentences\n",
    "            - avg_references_per_sentence: Average references available per sentence\n",
    "\n",
    "            **Length Statistics (Word-level):**\n",
    "            - avg/min/max/std_prediction_length: Prediction length statistics in words\n",
    "            - avg/min/max/std_reference_length: Reference length statistics in words\n",
    "\n",
    "            **Character-level Statistics:**\n",
    "            - avg/std_prediction_char_length: Character count statistics for predictions\n",
    "            - avg/std_reference_char_length: Character count statistics for references\n",
    "\n",
    "            **Length Change Analysis:**\n",
    "            - avg/std_length_difference: Difference between prediction and first reference lengths\n",
    "            - positive_length_changes: Count where prediction > reference length (expansion)\n",
    "            - negative_length_changes: Count where prediction < reference length (compression)  \n",
    "            - no_length_changes: Count where prediction == reference length (preserved)\n",
    "\n",
    "            **Vocabulary Analysis:**\n",
    "            - unique_words_in_predictions: Unique word count in all predictions\n",
    "            - unique_words_in_references: Unique word count in all references\n",
    "            - vocab_overlap: Common words between predictions and references\n",
    "            - vocab_overlap_ratio: Overlap ratio (intersection/union of vocabularies)\n",
    "\n",
    "        Notes:\n",
    "            - Length differences computed against first reference for each sentence\n",
    "            - Word counting uses lowercase normalization\n",
    "            - Empty references are skipped in processing\n",
    "            - Vocabulary analysis helps assess model's lexical diversity\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for computing statistics\n",
    "        pred_lengths = []                   # predictions length\n",
    "        pred_char_lengths = []              # predictions char length\n",
    "        all_ref_lengths = []                # reference length\n",
    "        all_ref_char_lengths = []           # reference char length\n",
    "        ref_counts = []                     # np. of reference per sentence\n",
    "        length_diffs = []                   # word difference in preds and refs\n",
    "        pred_word_counts = Counter()        # unique word counts in prediction\n",
    "        ref_word_counts = Counter()         # unique word counts in reference\n",
    "        # no. of len(prediction) > len(reference)\n",
    "        positive_changes = 0\n",
    "        # no. of len(prediction) < len(reference)\n",
    "        negative_changes = 0\n",
    "        # no. of len(prediction) == len(reference)\n",
    "        no_changes = 0\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for pred, refs in zip(predictions, references):\n",
    "            # prediction statistics\n",
    "            pred_len = len(pred.split())\n",
    "            pred_char_len = len(pred)\n",
    "            pred_lengths.append(pred_len)\n",
    "            pred_char_lengths.append(pred_char_len)\n",
    "\n",
    "            # prediction word counts\n",
    "            pred_word_counts.update(pred.lower().split())\n",
    "\n",
    "            # reference statistics\n",
    "            ref_counts.append(len(refs))\n",
    "\n",
    "            # looping for all references for this prediction\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    ref_len = len(ref.split())\n",
    "                    ref_char_len = len(ref)\n",
    "                    all_ref_lengths.append(ref_len)\n",
    "                    all_ref_char_lengths.append(ref_char_len)\n",
    "                    ref_word_counts.update(ref.lower().split())\n",
    "\n",
    "            # length difference analysis (compare with first reference)\n",
    "            if refs:\n",
    "                ref_len = len(refs[0].split()) if refs[0].strip() else 0\n",
    "                length_diff = pred_len - ref_len\n",
    "                length_diffs.append(length_diff)\n",
    "\n",
    "                # counting changes\n",
    "                if length_diff > 0:\n",
    "                    positive_changes += 1\n",
    "                elif length_diff < 0:\n",
    "                    negative_changes += 1\n",
    "                else:\n",
    "                    no_changes += 1\n",
    "\n",
    "        # updating the stats in the metrics_result...\n",
    "\n",
    "        # Sample information (standardized naming)\n",
    "        self.metrics_result[\"stats\"][\"num_samples\"] = len(predictions)\n",
    "        self.metrics_result[\"stats\"][\"total_references\"] = sum(ref_counts)\n",
    "        self.metrics_result[\"stats\"][\"avg_references_per_sentence\"] = np.mean(\n",
    "            ref_counts)\n",
    "\n",
    "        # Prediction statistics\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_length\"] = np.mean(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"min_prediction_length\"] = np.min(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"max_prediction_length\"] = np.max(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_length\"] = np.std(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_char_length\"] = np.mean(\n",
    "            pred_char_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_char_length\"] = np.std(\n",
    "            pred_char_lengths)\n",
    "\n",
    "        # Reference statistics\n",
    "        if all_ref_lengths:  # Handle empty case\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = np.mean(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = np.min(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = np.max(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = np.std(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = np.mean(\n",
    "                all_ref_char_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = np.std(\n",
    "                all_ref_char_lengths)\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = 0.0\n",
    "\n",
    "        # Length difference statistics\n",
    "        if length_diffs:\n",
    "            self.metrics_result[\"stats\"][\"avg_length_difference\"] = np.mean(\n",
    "                length_diffs)\n",
    "            self.metrics_result[\"stats\"][\"std_length_difference\"] = np.std(\n",
    "                length_diffs)\n",
    "            # FIXED: Use pre-calculated variables instead of redundant sum() operations\n",
    "            self.metrics_result[\"stats\"][\"positive_length_changes\"] = positive_changes\n",
    "            self.metrics_result[\"stats\"][\"negative_length_changes\"] = negative_changes\n",
    "            self.metrics_result[\"stats\"][\"no_length_changes\"] = no_changes\n",
    "\n",
    "        # Vocabulary statistics\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_predictions\"] = len(\n",
    "            pred_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_references\"] = len(\n",
    "            ref_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"vocab_overlap\"] = len(\n",
    "            set(pred_word_counts.keys()) & set(ref_word_counts.keys()))\n",
    "\n",
    "        # Vocabulary overlap ratio\n",
    "        if len(pred_word_counts) > 0 and len(ref_word_counts) > 0:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = (\n",
    "                self.metrics_result[\"stats\"][\"vocab_overlap\"] /\n",
    "                len(set(pred_word_counts.keys()) | set(ref_word_counts.keys()))\n",
    "            )\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = 0.0\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Perform comprehensive evaluation of grammar correction predictions.\n",
    "\n",
    "        This is the main evaluation method that computes all metrics and statistics\n",
    "        for grammar correction assessment. It provides a complete analysis including\n",
    "        fluency (GLEU), semantic preservation (BERTScore), linguistic quality (METEOR),\n",
    "        and comprehensive text statistics.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete evaluation results containing:\n",
    "                - \"gleu\": GLEU score (float)\n",
    "                - \"meteor\": METEOR score (float) \n",
    "                - \"bertscore\": Dict with precision, recall, f1 scores\n",
    "                - \"stats\": Dict with comprehensive text statistics\n",
    "\n",
    "        Evaluation Metrics Computed:\n",
    "\n",
    "            **Core Grammar Correction Metrics:**\n",
    "            - GLEU: Primary metric for grammar correction fluency assessment\n",
    "            - BERTScore: Semantic preservation evaluation (precision, recall, F1)\n",
    "            - METEOR: Linguistic quality with morphological awareness\n",
    "\n",
    "            **Comprehensive Statistics:**\n",
    "            - Sample counts and reference information\n",
    "            - Length statistics (words and characters)\n",
    "            - Length change analysis\n",
    "            - Vocabulary analysis and overlap metrics\n",
    "\n",
    "        Performance Benchmarks:\n",
    "            - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "            - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent  \n",
    "            - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "        Notes:\n",
    "            - Progress information is printed during computation\n",
    "            - Results are stored in self.metrics_result and returned\n",
    "            - Statistics are displayed as formatted pandas DataFrame\n",
    "            - All computations handle edge cases gracefully\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Evaluating {len(predictions)} Predictions...\")\n",
    "        # gleu score -- primary for grammar correction\n",
    "        print(\"\\t[INFO] Computing GLEU Score...\")\n",
    "        self.compute_gleu(predictions, references)\n",
    "\n",
    "        # bertscore -- sementic preservation\n",
    "        print(\"\\t[INFO] Computing BERTScore--Precision Recall & F1...\")\n",
    "        self.compute_bertscore(predictions, references)\n",
    "\n",
    "        # meteor -- linquistic quality\n",
    "        print(\"\\t[INFO] Computing METEOR Score...\")\n",
    "        self.compute_meteor(predictions, references)\n",
    "\n",
    "        # statistics\n",
    "        print(\"\\t[INFO] Computing Comprehensive Statistics...\")\n",
    "        self.compute_stats(predictions, references)\n",
    "\n",
    "        print(\"Evaluation Complete:\")\n",
    "        # printing the metrics...\n",
    "        print(f\"\\t[INFO] GLEU: {self.metrics_result['gleu']:.4f}\")\n",
    "        print(f\"\\t[INFO] METEOR: {self.metrics_result['meteor']:.4f}\")\n",
    "        print(\"\\t[INFO] BERTSCORE:\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Precision: {self.metrics_result['bertscore']['precision']:.4f}\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Recall: {self.metrics_result['bertscore']['recall']:.4f}\")\n",
    "        print(f\"\\t\\t[INFO] F1: {self.metrics_result['bertscore']['f1']:.4f}\")\n",
    "\n",
    "        # printing the statistics...\n",
    "        print(\"\\n\\t[INFO] Statistics:\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            stats_df = pd.DataFrame(\n",
    "                list(self.metrics_result[\"stats\"].items()),\n",
    "                columns=[\"Metric\", \"Value\"]\n",
    "            )\n",
    "            print(stats_df.to_string(index=False))\n",
    "        except ImportError:\n",
    "            print(\"\\t[WARNING] Pandas not available, printing raw statistics:\")\n",
    "            for key, value in self.metrics_result[\"stats\"].items():\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "        # Return complete results for further processing\n",
    "        return self.metrics_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84f95e",
   "metadata": {},
   "source": [
    "### T5 LoRA Model Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5LoRATrainer:\n",
    "    \"\"\"\n",
    "    Parameter-efficient fine-tuning trainer for T5 models using LoRA for grammar correction.\n",
    "\n",
    "    Implements complete training pipeline with research-proven hyperparameters, comprehensive \n",
    "    evaluation metrics (GLEU, BERTScore, METEOR), and multi-format logging for academic projects.\n",
    "\n",
    "    Key Features:\n",
    "        - 99%+ parameter reduction with LoRA (Low-Rank Adaptation)\n",
    "        - Research-validated hyperparameters (no search required)\n",
    "        - Comprehensive evaluation framework with graceful fallback\n",
    "        - Multi-format outputs (JSON, CSV, TXT) for analysis and reporting\n",
    "        - Cross-platform compatibility and robust error handling\n",
    "\n",
    "    Performance: 50-60+ GLEU, 0.80-0.90 BERTScore F1, 15-25 min training (T5-base, GPU)\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initialize trainer with model, tokenizer, and LoRA configuration\n",
    "        _setup_lora: Apply LoRA to model and create data collator\n",
    "        clean_prediction: Remove task prefixes from model outputs\n",
    "        compute_metrics: Calculate grammar correction evaluation metrics\n",
    "        save_training_log: Generate comprehensive training documentation\n",
    "        trainer: Execute complete training pipeline with optimal hyperparameters\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "        model (PeftModel): T5 model wrapped with LoRA adapters\n",
    "        output_dir (str): Directory for saving training artifacts\n",
    "        lora_rank (int): LoRA adaptation rank (default: 8)\n",
    "        lora_alpha (int): LoRA scaling parameter (default: 16)\n",
    "        lora_dropout (float): LoRA dropout rate (default: 0.1)\n",
    "        device (str): Training device (\"cuda\" or \"cpu\")\n",
    "        model_name (str): Original model identifier\n",
    "        grammar_evaluator (GrammarEvaluation): Evaluation framework instance\n",
    "        lora_config (LoraConfig): LoRA configuration object\n",
    "        data_collator (DataCollatorForSeq2Seq): Dynamic padding collator\n",
    "\n",
    "    References:\n",
    "        - LoRA Paper: https://arxiv.org/abs/2106.09685\n",
    "        - T5 Paper: https://arxiv.org/abs/1910.10683\n",
    "        - JFLEG Dataset: https://arxiv.org/abs/1702.04066\n",
    "        - BERTScore: https://arxiv.org/abs/1904.09675\n",
    "        - HuggingFace PEFT: https://huggingface.co/docs/peft/\n",
    "\n",
    "    Example:\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-base\") \n",
    "        >>> trainer = T5LoRATrainer(tokenizer, model)\n",
    "        >>> trainer, result = trainer.trainer(train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 model,\n",
    "                 output_dir=r\"D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\",\n",
    "                 lora_rank=8,\n",
    "                 lora_alpha=16,\n",
    "                 lora_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the T5 LoRA Trainer with pre-loaded model and tokenizer.\n",
    "\n",
    "        This constructor sets up the training environment for parameter-efficient fine-tuning\n",
    "        using Low-Rank Adaptation (LoRA) on T5 models for grammar correction tasks.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: Pre-initialized T5 tokenizer for text processing and encoding.\n",
    "                Should be loaded using T5Tokenizer.from_pretrained().\n",
    "            model: Pre-initialized T5 model for conditional generation.\n",
    "                Should be loaded using T5ForConditionalGeneration.from_pretrained().\n",
    "            output_dir (str, optional): Directory path for saving model checkpoints, logs,\n",
    "                and final trained model. Defaults to Windows assignment directory.\n",
    "                Will be created if it doesn't exist.\n",
    "            lora_rank (int, optional): Rank of LoRA adaptation matrices. Lower values use\n",
    "                less memory but may reduce model capacity. Defaults to 8.\n",
    "            lora_alpha (int, optional): LoRA scaling parameter. Typically 2x the rank.\n",
    "                Controls the magnitude of LoRA updates. Defaults to 16.\n",
    "            lora_dropout (float, optional): Dropout rate for LoRA layers to prevent\n",
    "                overfitting. Should be between 0.0-0.3. Defaults to 0.1.\n",
    "\n",
    "        Attributes:\n",
    "            tokenizer: T5 tokenizer for text processing\n",
    "            model: T5 model that will have LoRA applied\n",
    "            output_dir: Path where training artifacts will be saved\n",
    "            lora_rank: LoRA adaptation rank\n",
    "            lora_alpha: LoRA scaling factor\n",
    "            lora_dropout: LoRA dropout rate\n",
    "            device: Automatically detected device (\"cuda\" or \"cpu\")\n",
    "            grammar_evaluator: Comprehensive evaluation class for grammar correction metrics\n",
    "\n",
    "        Notes:\n",
    "            - LoRA configuration is stored but LoRA setup happens in _setup_lora()\n",
    "            - Device detection prioritizes CUDA if available for faster training\n",
    "            - Default output directory is Windows-specific; adjust for other OS\n",
    "            - Model name is automatically extracted from model.config.name_or_path\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If model doesn't have config.name_or_path attribute\n",
    "            OSError: If output_dir path is invalid or lacks write permissions\n",
    "        \"\"\"\n",
    "        # store tokenizer and model references\n",
    "        self.tokenizer = tokenizer              # T5 tokenizer for text processing\n",
    "        self.model = model                      # T5 model for grammar correction\n",
    "\n",
    "        # training configuration\n",
    "        self.output_dir = output_dir            # directory for saving training artifacts\n",
    "\n",
    "        # LoRA hyperparameters for parameter-efficient fine-tuning\n",
    "        self.lora_rank = lora_rank              # rank of LoRA adaptation matrices\n",
    "        # LoRA scaling parameter (typically 2x rank)\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout        # dropout rate for LoRA layers\n",
    "\n",
    "        # automatic device detection for optimal performance\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # name of the model\n",
    "        self.model_name = self.model.config.name_or_path\n",
    "        # initializing grammer evaluation class\n",
    "        self.grammar_evaluator = GrammarEvaluation()\n",
    "        # print initialization information for user feedback\n",
    "        print(f\"[INFO] Initializing T5 LoRA Trainer\")\n",
    "        # extract model name automatically\n",
    "        print(f\"\\t[INFO] Model: {self.model_name}\")\n",
    "        print(f\"\\t[INFO] Device: {self.device}\")\n",
    "        print(\n",
    "            f\"\\t[INFO] LoRA Config: rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout}\")\n",
    "        print(f\"\\t[INFO] Output Directory: {output_dir}\")\n",
    "\n",
    "        # Note: LoRA setup and model modification happens in _setup_lora() method\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self._setup_lora()\n",
    "        print(\"\\nT5 LoRA Trainer initialized successfully!\")\n",
    "\n",
    "    def _setup_lora(self):\n",
    "        \"\"\"\n",
    "        Apply LoRA configuration to T5 model and create data collator.\n",
    "\n",
    "        Configures and applies LoRA adapters to query and value attention matrices,\n",
    "        enabling parameter-efficient fine-tuning with ~99% parameter reduction.\n",
    "\n",
    "        Attributes Modified:\n",
    "            - model: Wrapped with LoRA adapters (becomes PeftModel)\n",
    "            - lora_config: LoRA configuration object created and stored\n",
    "            - data_collator: DataCollatorForSeq2Seq for dynamic padding\n",
    "\n",
    "        Side Effects:\n",
    "            - Model moved to self.device\n",
    "            - Prints trainable parameter statistics\n",
    "            - Prints setup confirmation\n",
    "\n",
    "        LoRA Config:\n",
    "            - task_type: SEQ_2_SEQ_LM\n",
    "            - target_modules: [\"q\", \"v\"] (query and value attention)\n",
    "            - r: self.lora_rank, alpha: self.lora_alpha, dropout: self.lora_dropout\n",
    "\n",
    "        References:\n",
    "            Hu et al. (2021) \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "            HuggingFace PEFT Documentation: https://huggingface.co/docs/peft/\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"[INFO] Apply LoRA to T5...\")\n",
    "\n",
    "        # setup LoRA configuration using instance parameters\n",
    "        self.lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,    # sequence-to-sequence language modeling\n",
    "            inference_mode=False,               # enable training mode\n",
    "            r=self.lora_rank,                   # rank of LoRA adaptation matrices\n",
    "            lora_alpha=self.lora_alpha,         # scaling factor for LoRA updates\n",
    "            lora_dropout=self.lora_dropout,     # dropout rate for regularization\n",
    "            # Query and Value attention matrices\n",
    "            target_modules=[\"q\", \"v\"],\n",
    "            bias=\"none\"                         # don't adapt bias parameters\n",
    "        )\n",
    "\n",
    "        # apply LoRA adapters to the model (wraps model with PEFT)\n",
    "        self.model = get_peft_model(self.model, self.lora_config)\n",
    "        # move enhanced model to appropriate device for training\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # create data collator once for reuse (dynamic padding)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,           # use LoRA-enhanced model\n",
    "            padding=True,               # enable dynamic padding\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # print trainable parameters statistics for verification\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "        print(\n",
    "            f\"\\t[INFO] LoRA applied successfully to {len(self.lora_config.target_modules)} target modules\")\n",
    "        print(f\"\\t[INFO] Model moved to device: {self.device}\")\n",
    "        print(f\"\\t[INFO] Data collator configured for dynamic padding\")\n",
    "\n",
    "    def clean_prediction(self, prediction):\n",
    "        \"\"\"\n",
    "        Clean prediction text by removing task prefixes and extra whitespace.\n",
    "\n",
    "        This method removes task-specific prefixes (like \"grammar:\") that the model\n",
    "        might include in its predictions, ensuring clean text for evaluation.\n",
    "        Can be used both during training evaluation and external testing.\n",
    "\n",
    "        Args:\n",
    "            prediction (str): Raw model prediction text\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned prediction without task prefix\n",
    "        \"\"\"\n",
    "\n",
    "        # handle None or empty inputs\n",
    "        if not prediction:\n",
    "            return \"\"\n",
    "\n",
    "        # remove whitespace\n",
    "        cleaned = str(prediction).strip()\n",
    "\n",
    "        # define possible task prefixes to remove\n",
    "        prefixes_to_remove = [\n",
    "            \"grammar:\",\n",
    "            \"correct:\",\n",
    "            \"fix:\",\n",
    "            \"paraphrase:\",\n",
    "            \"translate:\",\n",
    "        ]\n",
    "\n",
    "        # remove any matching prefix (case-insensitive)\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if cleaned.lower().startswith(prefix.lower()):\n",
    "                cleaned = cleaned[len(prefix):].strip()\n",
    "                break  # only remove first matching prefix\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"\n",
    "        Compute comprehensive evaluation metrics for grammar correction during training.\n",
    "\n",
    "        This method processes model predictions and reference labels to calculate grammar\n",
    "        correction metrics including GLEU, BERTScore, and METEOR. It implements a robust\n",
    "        two-tier evaluation system: comprehensive metrics when possible, with automatic\n",
    "        fallback to basic metrics if the comprehensive evaluation fails.\n",
    "\n",
    "        Args:\n",
    "            eval_preds (tuple): Tuple containing (predictions, labels) from Trainer evaluation.\n",
    "                predictions (np.ndarray or tuple): Model output logits or generated sequences.\n",
    "                    If tuple, uses first element (generated sequences).\n",
    "                labels (np.ndarray): Reference labels with -100 for padded positions.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics with the following structure:\n",
    "\n",
    "            Comprehensive evaluation (when successful):\n",
    "                - gleu (float): GLEU score for grammar correction fluency\n",
    "                - meteor (float): METEOR score for linguistic quality\n",
    "                - bertscore_f1 (float): BERTScore F1 for semantic preservation\n",
    "                - bertscore_precision (float): BERTScore precision\n",
    "                - bertscore_recall (float): BERTScore recall\n",
    "                - prediction_length (float): Average prediction length in words\n",
    "                - reference_length (float): Average reference length in words\n",
    "                - vocab_overlap_ratio (float): Vocabulary overlap between preds/refs\n",
    "                - positive_length_changes (int): Count of predictions longer than references\n",
    "                - evaluation_type (str): \"comprehensive\"\n",
    "                - num_samples (int): Number of evaluated samples\n",
    "\n",
    "            Basic fallback evaluation (when comprehensive fails):\n",
    "                - prediction_length (float): Average prediction length in words\n",
    "                - reference_length (float): Average reference length in words\n",
    "                - length_difference (float): Average length difference (pred - ref)\n",
    "                - positive_length_changes (int): Count of longer predictions\n",
    "                - predictions_count (int): Total number of predictions\n",
    "                - gleu, meteor, bertscore_f1 (float): Set to 0.0 (placeholders)\n",
    "                - vocab_overlap_ratio (float): Set to 0.0 (placeholder)\n",
    "                - evaluation_type (str): \"basic_fallback\"\n",
    "                - num_samples (int): Number of evaluated samples\n",
    "\n",
    "        Process:\n",
    "            1. Handle tuple outputs, decode predictions and labels\n",
    "            2. Clean predictions (remove task prefixes)\n",
    "            3. Attempt comprehensive evaluation via self.grammar_evaluator\n",
    "            4. Fallback to basic metrics if comprehensive evaluation fails\n",
    "        \"\"\"\n",
    "\n",
    "        # getting labels and predictions\n",
    "        predictions, labels = eval_preds\n",
    "\n",
    "        # handle tuple generated output\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        # replacing -100 labels with padded tokens for decoding -- required\n",
    "        # dynamic padding creates -100 in labels, which is not valid token ID\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "\n",
    "        # decoding the predictions and labels\n",
    "        decoded_preds = self.tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True)\n",
    "        decoded_labels = self.tokenizer.batch_decode(\n",
    "            labels, skip_special_tokens=True)\n",
    "\n",
    "        # cleaning predictions (remove task prefixes) and labels\n",
    "        decoded_preds = [self.clean_prediction(pred) for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        try:\n",
    "            # calculating the comprehensive evaluation mertics\n",
    "            print(\"\\t[INFO] Computing Comprehensive Evaluation Metrics...\")\n",
    "\n",
    "            # convert to format expected by GrammarEvaluation (list of lists for references)\n",
    "            # single reference per predictio\n",
    "            references = [[label] for label in decoded_labels]\n",
    "\n",
    "            # calling the GrammarEvaluation class\n",
    "            results = self.grammar_evaluator.evaluate(predictions=decoded_preds,\n",
    "                                                      references=references)\n",
    "\n",
    "            print(\"\\t[INFO] Comprehensive Evaluation Successful\")\n",
    "\n",
    "            # return comprehensive metrics for trainer\n",
    "            return {\n",
    "                # primary grammar correction metrics\n",
    "                \"gleu\": results[\"gleu\"],\n",
    "                \"meteor\": results[\"meteor\"],\n",
    "                \"bertscore_f1\": results[\"bertscore\"][\"f1\"],\n",
    "                \"bertscore_precision\": results[\"bertscore\"][\"precision\"],\n",
    "                \"bertscore_recall\": results[\"bertscore\"][\"recall\"],\n",
    "\n",
    "                # key statistics from comprehensive analysis\n",
    "                \"prediction_length\": results[\"stats\"][\"avg_prediction_length\"],\n",
    "                \"reference_length\": results[\"stats\"][\"avg_reference_length\"],\n",
    "                \"vocab_overlap_ratio\": results[\"stats\"][\"vocab_overlap_ratio\"],\n",
    "                \"positive_length_changes\": results[\"stats\"][\"positive_length_changes\"],\n",
    "\n",
    "                # metadata\n",
    "                \"evaluation_type\": \"comprehensive\",\n",
    "                \"num_samples\": results[\"stats\"][\"num_samples\"]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # fallback to basic metrics if comprehensive evaluation fails\n",
    "            print(f\"\\t[WARNING] Comprehensive Evaluation Failed: {e}\")\n",
    "            print(\"\\t[INFO] Falling back to Basic Metrics...\")\n",
    "\n",
    "        # calculate basic metrics manually (fallback)\n",
    "        pred_lengths = [len(pred.split()) for pred in decoded_preds]\n",
    "        ref_lengths = [len(ref.split()) for ref in decoded_labels]\n",
    "\n",
    "        # basic length change analysis\n",
    "        length_diffs = [len(pred.split()) - len(ref.split())\n",
    "                        for pred, ref in zip(decoded_preds, decoded_labels)]\n",
    "        positive_changes = sum(1 for diff in length_diffs if diff > 0)\n",
    "\n",
    "        print(\"\\t[INFO] Basic metrics computed successfully\")\n",
    "\n",
    "        # return basic fallback metrics\n",
    "        return {\n",
    "            # basic monitoring metrics\n",
    "            \"prediction_length\": np.mean(pred_lengths),\n",
    "            \"reference_length\": np.mean(ref_lengths),\n",
    "            \"length_difference\": np.mean(length_diffs),\n",
    "            \"positive_length_changes\": positive_changes,\n",
    "            \"predictions_count\": len(decoded_preds),\n",
    "\n",
    "            # placeholder values for missing comprehensive metrics\n",
    "            \"gleu\": 0.0,\n",
    "            \"meteor\": 0.0,\n",
    "            \"bertscore_f1\": 0.0,\n",
    "            \"vocab_overlap_ratio\": 0.0,\n",
    "\n",
    "            # metadata\n",
    "            \"evaluation_type\": \"basic_fallback\",\n",
    "            \"num_samples\": len(decoded_preds)\n",
    "        }\n",
    "\n",
    "    def save_training_log(self, trainer, training_time_minutes):\n",
    "        \"\"\"\n",
    "        Save comprehensive training logs in multiple formats.\n",
    "\n",
    "        Args:\n",
    "            trainer (transformers.Trainer): Trained Trainer with log history\n",
    "            training_time_minutes (float): Total training time in minutes\n",
    "\n",
    "        Returns:\n",
    "            str: Path to main JSON log file\n",
    "\n",
    "        Creates Files in self.output_dir:\n",
    "            - training_log.json: Structured training data and metadata\n",
    "            - training_metrics.csv: Training progress (if pandas available)\n",
    "            - evaluation_metrics.csv: Evaluation metrics (if pandas available)\n",
    "            - training_summary.txt: Human-readable summary for reports\n",
    "\n",
    "        Log Contents:\n",
    "            - Training metadata: model, LoRA config, timing, device\n",
    "            - Training progress: complete loss/metrics history by epoch\n",
    "            - Final metrics: last recorded values for all metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # extracting history from trainer\n",
    "        history = trainer.state.log_history\n",
    "\n",
    "        # separating training and validation logs\n",
    "        train_logs = [\n",
    "            log for log in history if \"train_loss\" in log and \"epoch\" in log]\n",
    "        eval_logs = [\n",
    "            log for log in history if \"eval_loss\" in log and \"epoch\" in log]\n",
    "\n",
    "        # creating comprehensive training log\n",
    "        training_log = {\n",
    "            \"training_metadata\": {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"lora_config\": {\n",
    "                    \"rank\": self.lora_rank,\n",
    "                    \"alpha\": self.lora_alpha,\n",
    "                    \"dropout\": self.lora_dropout,\n",
    "                    \"target_modules\": [\"q\", \"v\"]\n",
    "                },\n",
    "                \"training_time_minutes\": training_time_minutes,\n",
    "                \"device\": self.device,\n",
    "                \"output_directory\": self.output_dir\n",
    "            },\n",
    "            \"training_progress\": {\n",
    "                \"train_logs\": train_logs,\n",
    "                \"eval_logs\": eval_logs,\n",
    "                \"total_epochs\": len(train_logs),\n",
    "                \"total_eval_steps\": len(eval_logs)\n",
    "            },\n",
    "            \"final_metrics\": {}\n",
    "        }\n",
    "\n",
    "        # extracting final metrics if available\n",
    "        if eval_logs:\n",
    "            final_eval = eval_logs[-1]\n",
    "            for key, value in final_eval.items():\n",
    "                if key.startswith('eval_') and isinstance(value, (int, float)):\n",
    "                    training_log[\"final_metrics\"][key] = value\n",
    "\n",
    "        if train_logs:\n",
    "            final_train = train_logs[-1]\n",
    "            training_log[\"final_metrics\"][\"final_train_loss\"] = final_train.get(\n",
    "                \"train_loss\", \"N/A\")\n",
    "\n",
    "        # saving detailed training log as JSON\n",
    "        log_file = os.path.join(self.output_dir, \"training_log.json\")\n",
    "        with open(log_file, \"w\") as f:\n",
    "            json.dump(training_log, f, indent=2)\n",
    "\n",
    "            # Save CSV format for easy analysis\n",
    "        try:\n",
    "            import pandas as pd\n",
    "\n",
    "            # Training metrics CSV\n",
    "            if train_logs:\n",
    "                train_df = pd.DataFrame(train_logs)\n",
    "                train_df.to_csv(os.path.join(self.output_dir,\n",
    "                                \"training_metrics.csv\"), index=False)\n",
    "                print(f\"\\t[INFO] Training metrics saved to: training_metrics.csv\")\n",
    "\n",
    "            # Evaluation metrics CSV\n",
    "            if eval_logs:\n",
    "                eval_df = pd.DataFrame(eval_logs)\n",
    "                eval_df.to_csv(os.path.join(self.output_dir,\n",
    "                               \"evaluation_metrics.csv\"), index=False)\n",
    "                print(\n",
    "                    f\"\\t[INFO] Evaluation metrics saved to: evaluation_metrics.csv\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(f\"\\t[WARNING] Pandas not available, skipping CSV export\")\n",
    "\n",
    "        # save simple text summary\n",
    "        summary_file = os.path.join(self.output_dir, \"training_summary.txt\")\n",
    "        with open(summary_file, \"w\") as f:\n",
    "            f.write(\"T5 LoRA Grammar Correction Training Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Model: {self.model_name}\\n\")\n",
    "            f.write(f\"Training Time: {training_time_minutes:.1f} minutes\\n\")\n",
    "            f.write(f\"Device: {self.device}\\n\")\n",
    "            f.write(f\"LoRA Rank: {self.lora_rank}\\n\")\n",
    "            f.write(f\"LoRA Alpha: {self.lora_alpha}\\n\")\n",
    "            f.write(f\"Total Epochs: {len(train_logs)}\\n\\n\")\n",
    "\n",
    "            if training_log[\"final_metrics\"]:\n",
    "                f.write(\"Final Metrics:\\n\")\n",
    "                for key, value in training_log[\"final_metrics\"].items():\n",
    "                    if isinstance(value, float):\n",
    "                        f.write(f\"  {key}: {value:.4f}\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(f\"\\t[INFO] Training logs saved:\")\n",
    "        print(f\"\\t\\t- Detailed log: training_log.json\")\n",
    "        print(f\"\\t\\t- Text summary: training_summary.txt\")\n",
    "        print(f\"\\t\\t- Output directory: {self.output_dir}\")\n",
    "\n",
    "    def trainer(self,\n",
    "                train_dataset,\n",
    "                val_dataset,\n",
    "                learning_rate=1e-4,\n",
    "                batch_size=4,\n",
    "                num_epochs=3,\n",
    "                warmup_ratio=0.1,\n",
    "                weight_decay=0.01):\n",
    "        \"\"\"\n",
    "        Execute complete T5 LoRA training pipeline with research-proven hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            train_dataset (Dataset): Tokenized training data with input_ids, attention_mask, labels\n",
    "            val_dataset (Dataset): Tokenized validation data in same format\n",
    "            learning_rate (float, optional): AdamW learning rate. Defaults to 1e-4.\n",
    "            batch_size (int, optional): Per-device batch size. Defaults to 4.\n",
    "            num_epochs (int, optional): Training epochs. Defaults to 3.\n",
    "            warmup_ratio (float, optional): LR warmup proportion. Defaults to 0.1.\n",
    "            weight_decay (float, optional): L2 regularization. Defaults to 0.01.\n",
    "\n",
    "        Returns:\n",
    "            transformers.Trainer: Trained Trainer object with fine-tuned LoRA model\n",
    "            train_results: Trained Results\n",
    "\n",
    "        Training Configuration:\n",
    "            - Evaluation strategy: Every epoch\n",
    "            - Early stopping: 2 epochs patience on eval_gleu\n",
    "            - Mixed precision: fp16 if CUDA available\n",
    "            - Gradient accumulation: 2 steps (effective batch size = batch_size * 2)\n",
    "            - Best model selection: Highest eval_gleu score\n",
    "\n",
    "        Created Files:\n",
    "            - Model files: LoRA adapters + tokenizer in self.output_dir\n",
    "            - Training logs: JSON, CSV, TXT formats via save_training_log()\n",
    "            - Hyperparameters: hyperparameters_and_results.json\n",
    "\n",
    "        Hyperparameter Sources:\n",
    "            - Learning rate: LoRA paper (Hu et al., 2021)\n",
    "            - Batch size: T5 paper + memory constraints\n",
    "            - Epochs: LoRA convergence studies\n",
    "            - Warmup: Transformer fine-tuning best practices\n",
    "\n",
    "        References:\n",
    "            - Hu et al. (2021): \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "            - Raffel et al. (2019): \"T5: Text-to-Text Transfer Transformer\"\n",
    "            - Napoles et al. (2017): \"JFLEG: A Fluency Corpus for Grammar Correction\"\n",
    "            - Zhang et al. (2020): \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"[INFO] Starting Training with Hyperparameter:\")\n",
    "        print(f\"\\t[INFO] Learning Rate: {learning_rate} (LoRA optimal range)\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Batch Size: {batch_size} (effective: {batch_size * 2} with grad accum)\")\n",
    "        print(f\"\\t[INFO] Epochs: {num_epochs} (LoRA converges fast)\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Warmup Ratio: {warmup_ratio} (standard transformer practice)\")\n",
    "        print(f\"\\t[INFO] Weight Decay: {weight_decay} (light regularization)\")\n",
    "\n",
    "        # Create training arguments with research-proven hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=os.path.join(self.output_dir, \"logs\"),\n",
    "            logging_steps=50,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_gleu\",\n",
    "            greater_is_better=True,\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            gradient_accumulation_steps=2,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=True,\n",
    "            label_names=[\"labels\"],  # explicit for PEFT models\n",
    "\n",
    "            # best hyperparameters\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        # create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        # start training\n",
    "        start_time = time.time()\n",
    "        print(f\"[INFO] Starting Training...\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Dataset size: {len(train_dataset):,} training examples\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Dataset size: {len(val_dataset):,} validation examples\")\n",
    "\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # calculate actual training time\n",
    "        training_time = (time.time() - start_time) / 60\n",
    "\n",
    "        # save model and tokenizer\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "        # save comprehensive training logs\n",
    "        self.save_training_log(trainer, training_time)\n",
    "\n",
    "        # print comprehensive results\n",
    "        print(f\"[INFO] Training Completed Successfully!\")\n",
    "        print(f\"\\t[INFO] Actual training time: {training_time:.1f} minutes\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Final training loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "\n",
    "        # print evaluation metrics if available\n",
    "        eval_metrics = [\"eval_gleu\", \"eval_meteor\", \"eval_bertscore_f1\"]\n",
    "        for metric in eval_metrics:\n",
    "            if metric in train_result.metrics:\n",
    "                print(\n",
    "                    f\"\\t[INFO] Final {metric}: {train_result.metrics[metric]:.4f}\")\n",
    "\n",
    "        print(f\"\\t[INFO] Model saved to: {self.output_dir}\")\n",
    "        print(f\"\\t[INFO] Tokenizer saved to: {self.output_dir}\")\n",
    "        print(f\"\\t[INFO] Training logs saved to: {self.output_dir}\")\n",
    "\n",
    "        # Save training summary for reference\n",
    "        training_summary = {\n",
    "            \"hyperparameters\": {\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"warmup_ratio\": warmup_ratio,\n",
    "                \"weight_decay\": weight_decay\n",
    "            },\n",
    "            \"training_results\": train_result.metrics,\n",
    "            \"training_time_minutes\": training_time,\n",
    "            \"dataset_size\": len(train_dataset),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"lora_config\": {\n",
    "                \"rank\": self.lora_rank,\n",
    "                \"alpha\": self.lora_alpha,\n",
    "                \"dropout\": self.lora_dropout\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.output_dir, \"hyperparameters_and_results.json\"), \"w\") as f:\n",
    "            json.dump(training_summary, f, indent=2)\n",
    "\n",
    "        print(\n",
    "            f\"\\t[INFO] Hyperparameters and results saved to: hyperparameters_and_results.json\")\n",
    "\n",
    "        return trainer, train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d3882",
   "metadata": {},
   "source": [
    "### Getting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062ab593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing JFLEG Dataset Processor...\n",
      "[INFO] Max length: 256, Test split ratio: 10.0%\n",
      "[INFO] Loaded JFLEG validation split: 755 examples\n",
      "[INFO] Loaded JFLEG test split: 748 examples\n"
     ]
    }
   ],
   "source": [
    "# intializing dataset instance\n",
    "dataset = JFLEGDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8ad332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating datasets with augmentation and tokenization...\n",
      "\n",
      "[INFO] Applying augmentation to training data...\n",
      "\t[INFO] Length of Dataset is:  3016\n",
      "[INFO] Applying augmentation to validation data...\n",
      "\t[INFO] Length of Dataset is:  747\n",
      "\n",
      "[INFO] Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801606c315f746559369b300ba71c84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Training Data:   0%|          | 0/3016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f813c39b2d1144b8b7ef6366ac6daaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Validation Data:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Splitting Validation Data (90%/10%)...\n",
      "\n",
      "Dataset Creation Complete:\n",
      "\t[INFO] Training Dataset:   3016\n",
      "\t[INFO] Validation Dataset: 672\n",
      "\t[INFO] Test Dataset:       75\n"
     ]
    }
   ],
   "source": [
    "# get train, validation and test set\n",
    "train_dataset, val_dataset, test_dataset = dataset.create_train_val_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac39f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So I think we can not live if old people could not find siences and tecnologies and they did not developped.\n",
      "['So I think we would not be alive if our ancestors did not develop sciences and technologies.', 'So I think we could not live if older people did not develop science and technologies.', 'So I think we can not live if old people could not find science and technologies and they did not develop.', 'So I think we can not live if old people can not find the science and technology that has not been developed.']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"processed_sentence\"][0])\n",
    "print(train_dataset[\"processed_corrections\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59b47b",
   "metadata": {},
   "source": [
    "### Fine-Tune T5 LoRA with the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f942010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing T5 LoRA Trainer\n",
      "\t[INFO] Model: t5-small\n",
      "\t[INFO] Device: cpu\n",
      "\t[INFO] LoRA Config: rank=8, alpha=16, dropout=0.1\n",
      "\t[INFO] Output Directory: D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\n",
      "[INFO] Apply LoRA to T5...\n",
      "trainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850\n",
      "\t[INFO] LoRA applied successfully to 2 target modules\n",
      "\t[INFO] Model moved to device: cpu\n",
      "\t[INFO] Data collator configured for dynamic padding\n",
      "\n",
      "T5 LoRA Trainer initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# intializing training instance\n",
    "model_trainer = T5LoRATrainer(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec079c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Training with Hyperparameter:\n",
      "\t[INFO] Learning Rate: 0.0001 (LoRA optimal range)\n",
      "\t[INFO] Batch Size: 4 (effective: 8 with grad accum)\n",
      "\t[INFO] Epochs: 3 (LoRA converges fast)\n",
      "\t[INFO] Warmup Ratio: 0.1 (standard transformer practice)\n",
      "\t[INFO] Weight Decay: 0.01 (light regularization)\n",
      "[INFO] Starting Training...\n",
      "\t[INFO] Dataset size: 3,016 training examples\n",
      "\t[INFO] Dataset size: 672 validation examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agarw\\AppData\\Local\\Temp\\ipykernel_3936\\3914257901.py:579: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='378' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 378/1131 11:43 < 23:28, 0.53 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/168 08:23 < 02:38, 0.25 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "trainer, result = model_trainer.trainer(train_dataset=train_dataset,\n",
    "                                        val_dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dc686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
