{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ed6398",
   "metadata": {},
   "source": [
    "#### **7PAM2015-0509-2024 -- Research Methods in Data Science**\n",
    "##### Mult-Task -- Grammar Correctiona and Paraphrasing -- Implementation on T5 through LoRA.\n",
    "---\n",
    "**Mohit Agarwal (Student ID-22031257)**\n",
    "\n",
    "This notebook trains and evaluate the model.\n",
    "\n",
    "\n",
    "##### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70186f37",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af17f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Core T5 training libraries\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    TrainingArguments, Trainer, DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebeb50",
   "metadata": {},
   "source": [
    "Changing the default styles and palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82506b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "# setting context\n",
    "sns.set_context(\"paper\")\n",
    "# setting palette\n",
    "sns.set_palette(\"deep\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6134",
   "metadata": {},
   "source": [
    "### JFLEG Dataset Class for Preparation and Augmentation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    \"\"\"\n",
    "    A comprehensive dataset processor for JFLEG (JHU FLuency-Extended GUG) grammar correction data.\n",
    "\n",
    "    This class handles the complete pipeline for preparing JFLEG data for T5-based grammar correction\n",
    "    training, including data loading, preprocessing, augmentation, tokenization, and train/validation/test\n",
    "    splitting. The JFLEG dataset contains 1,511 examples with 4 human-written corrections each, focusing\n",
    "    on fluency improvements rather than minimal edits.\n",
    "\n",
    "    Key Features:\n",
    "        - Comprehensive text preprocessing to handle formatting issues\n",
    "        - Data augmentation using all 4 JFLEG corrections per sentence\n",
    "        - Proper tokenization for T5 sequence-to-sequence training\n",
    "        - Train/validation/test splitting with preserved evaluation metadata\n",
    "        - Temperature-scaled mixing support for multi-task learning\n",
    "\n",
    "    Dataset Sources:\n",
    "        - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "        - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization\n",
    "        test_split_ratio (float): Proportion of validation data to use for testing\n",
    "        train_data (Dataset): JFLEG validation split used for training\n",
    "        validation_data (Dataset): JFLEG test split used for validation/testing\n",
    "\n",
    "    Example:\n",
    "        >>> from transformers import T5Tokenizer\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> dataset = JFLEGDataset(tokenizer, max_length=256, test_split_ratio=0.10)\n",
    "        >>> train_data, val_data, test_data = dataset.create_train_val_test_datasets()\n",
    "\n",
    "    References:\n",
    "        - JFLEG Paper: Napoles et al. (2017) \"JFLEG: A Fluency Corpus and Benchmark \n",
    "          for Grammatical Error Correction\"\n",
    "        - Dataset: https://huggingface.co/datasets/jhu-clsp/jfleg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length=256, test_split_ratio=0.10):\n",
    "        \"\"\"\n",
    "        Initialize the JFLEG dataset processor with specified configuration.\n",
    "\n",
    "        Sets up the dataset processor with the provided tokenizer and configuration\n",
    "        parameters, then loads the raw JFLEG datasets for subsequent processing.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (T5Tokenizer): HuggingFace T5 tokenizer instance for text processing.\n",
    "                Must be a properly initialized T5 tokenizer (e.g., from t5-base).\n",
    "            max_length (int, optional): Maximum sequence length for tokenization. \n",
    "                Sequences longer than this will be truncated. Defaults to 256.\n",
    "                Recommended range: 128-512 depending on GPU memory constraints.\n",
    "            test_split_ratio (float, optional): Proportion of validation data to reserve \n",
    "                for final testing. Must be between 0.0 and 1.0. Defaults to 0.10 (10%).\n",
    "                The remaining validation data will be used for model validation during training.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If test_split_ratio is not between 0.0 and 1.0\n",
    "            TypeError: If tokenizer is not a valid T5Tokenizer instance\n",
    "\n",
    "        Note:\n",
    "            The JFLEG dataset splits are used as follows:\n",
    "            - JFLEG 'validation' split → Training data (with augmentation)\n",
    "            - JFLEG 'test' split → Validation and test data (split according to test_split_ratio)\n",
    "\n",
    "            This approach follows standard practice since JFLEG's validation split is larger\n",
    "            and more suitable for training, while the test split is reserved for evaluation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        # Validate test_split_ratio\n",
    "        if not 0.0 <= test_split_ratio <= 1.0:\n",
    "            raise ValueError(\n",
    "                f\"test_split_ratio must be between 0.0 and 1.0, got {test_split_ratio}\")\n",
    "\n",
    "        # Load the JFLEG datasets\n",
    "        print(f\"[INFO] Initializing JFLEG Dataset Processor...\")\n",
    "        print(\n",
    "            f\"[INFO] Max length: {max_length}, Test split ratio: {test_split_ratio:.1%}\")\n",
    "\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG validation split: {len(self.train_data)} examples\")\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG test split: {len(self.validation_data)} examples\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "                text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "                str: The preprocessed and normalized text, or the original input if\n",
    "                        it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "                - Removes multiple consecutive dashes (-- → \"\")\n",
    "                - Fixes decimal formatting (0 . 1 → 0.1)\n",
    "                - Fixes fraction formatting (1 / 2 → 1/2)\n",
    "                - Removes leading zeros in decimals (00.5 → 0.5)\n",
    "                - Joins split numbers (1 2 3 4 → 1234)\n",
    "                - Fixes punctuation spacing (word , → word,)\n",
    "                - Normalizes quote spacing (\" word \" → \"word\")\n",
    "                - Collapses multiple spaces to single spaces\n",
    "                - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "\n",
    "        # if not text or not isinstance(text, str):\n",
    "        #     return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 → 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 → 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 → 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_augmentation(self, data, augment=True):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to JFLEG dataset using all available corrections.\n",
    "\n",
    "        This function processes JFLEG examples to create augmented data by utilizing\n",
    "        all 4 human-written corrections per sentence. Each original sentence is paired with\n",
    "        each of its corrections to create multiple training examples, significantly increasing\n",
    "        the dataset size and providing the model with diverse correction targets.\n",
    "\n",
    "        Args:\n",
    "                data (List[Dict]): List of JFLEG dataset examples, where each example contains:\n",
    "                        - 'sentence' (str): Original grammatically incorrect sentence\n",
    "                        - 'corrections' (List[str]): List of 4 human-written corrections\n",
    "                augment (bool, optional): Whether to use all corrections for augmentation.\n",
    "                        - If True: Creates 4 examples per input (uses all corrections)\n",
    "                        - If False: Creates 1 example per input (uses only first correction)\n",
    "                        Default is True.\n",
    "\n",
    "        Returns:\n",
    "                List[Dict]: Augmented dataset where each dictionary contains:\n",
    "                        - 'input' (str): Preprocessed input with \"grammar: \" prefix\n",
    "                        - 'target' (str): Preprocessed target correction\n",
    "                        - 'processed_sentence' (str): Preprocessed original sentence\n",
    "                        - 'processed_corrections' (List[str]): All 4 preprocessed corrections for evaluation\n",
    "                        - 'raw_original' (str): Unprocessed original sentence (for debugging)\n",
    "                        - 'raw_corrections' (List[str]): Unprocessed corrections (for debugging)\n",
    "        \"\"\"\n",
    "        # storage for augmented data\n",
    "        augmented_data = []\n",
    "        for items in data:\n",
    "            # getting original sentence -- incorrect\n",
    "            original_sentence = items[\"sentence\"]\n",
    "            # formatting the incorrect sentence\n",
    "            processed_sentence = self._preprocess(original_sentence)\n",
    "\n",
    "            # getting all the original corrected sentences\n",
    "            corrections = items[\"corrections\"]\n",
    "\n",
    "            # formatting all the corrected sentences -- evaluation\n",
    "            processed_corrections = []\n",
    "            # looping over all 4 corrections\n",
    "            for correction in corrections:\n",
    "                if correction.strip():  # Skip empty corrections\n",
    "                    # storing all the processed corrections\n",
    "                    processed_corrections.append(self._preprocess(correction))\n",
    "\n",
    "            # looping over processed corrections\n",
    "            for processed_correction in processed_corrections:\n",
    "                # creating a dataset\n",
    "                augmented_data.append({\n",
    "                    \"input\": f\"grammar: {processed_sentence}\",\n",
    "                    \"target\": processed_correction,\n",
    "                    \"processed_sentence\": processed_sentence,\n",
    "                    \"processed_corrections\": processed_corrections,\n",
    "                    \"raw_original\": original_sentence,\n",
    "                    \"raw_corrections\": corrections\n",
    "                })\n",
    "                # checking if to augment or not\n",
    "                if not augment:\n",
    "                    break\n",
    "        # displaying the length of data\n",
    "        print(\"\\t[INFO] Length of Dataset is: \", len(augmented_data))\n",
    "        return augmented_data\n",
    "\n",
    "    def _apply_tokenization(self, data):\n",
    "        \"\"\"\n",
    "        Apply tokenization to preprocessed JFLEG dataset examples for T5 model training.\n",
    "\n",
    "        This function converts text data (input sentences and target corrections) into \n",
    "        tokenized format suitable for T5 model training. It processes both the input \n",
    "        grammar correction task and the target correction, creating the necessary \n",
    "        input_ids, attention_mask, and labels required by the HuggingFace Trainer.\n",
    "\n",
    "        Args:\n",
    "                data (Dict): A single preprocessed example containing:\n",
    "                - 'input' (str): Preprocessed input text with \"grammar: \" prefix\n",
    "                - 'target' (str): Preprocessed target correction text\n",
    "                - 'processed_sentence' (str): Preprocessed original sentence (preserved but not tokenized)\n",
    "                - 'processed_corrections' (List[str]): All preprocessed corrections (preserved but not tokenized)\n",
    "                - 'raw_original' (str): Raw original sentence (preserved but not tokenized)\n",
    "                - 'raw_corrections' (List[str]): Raw corrections (preserved but not tokenized)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                Dict: Tokenized example ready for model training containing:\n",
    "                        - 'input_ids' (List[int]): Token IDs for the input sequence\n",
    "                        - 'attention_mask' (List[int]): Attention mask for input (1 for real tokens, 0 for padding)\n",
    "                        - 'labels' (List[int]): Token IDs for the target sequence (used for loss computation)\n",
    "\n",
    "        Tokenization Settings:\n",
    "                - max_length (int): Maximum sequence length (defined by self.max_length)\n",
    "                - truncation (bool): True - truncates sequences longer than max_length\n",
    "                - padding (bool): False - no padding applied (Trainer handles dynamic padding)\n",
    "                - return_tensors: None - returns Python lists instead of PyTorch tensors\n",
    "        \"\"\"\n",
    "        # tokenizing the input of the dataset\n",
    "        input_encodings = self.tokenizer(data[\"input\"],\n",
    "                                         max_length=self.max_length,\n",
    "                                         truncation=True,\n",
    "                                         padding=False,  # trainer handles the dynamic padding\n",
    "                                         return_tensors=None)  # returns lists not tensor\n",
    "        # tokenizing the target of the dataset\n",
    "        target_encodings = self.tokenizer(data[\"target\"],\n",
    "                                          max_length=self.max_length,\n",
    "                                          truncation=True,\n",
    "                                          padding=False,  # trainer handles the dynamic padding\n",
    "                                          return_tensors=None)  # returns lists not tensor\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]\n",
    "        }\n",
    "\n",
    "    def create_train_val_test_datasets(self):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets with proper augmentation and tokenization.\n",
    "\n",
    "        This function orchestrates the complete data processing pipeline for JFLEG grammar \n",
    "        correction training. It applies data augmentation, converts to HuggingFace datasets,\n",
    "        applies tokenization, and splits the data into appropriate train/validation/test sets\n",
    "        while preserving essential evaluation metadata.\n",
    "\n",
    "        Processing Pipeline:\n",
    "                1. Apply augmentation to training data (4x expansion using all corrections)\n",
    "                2. Apply augmentation to validation data (no expansion, uses first correction only)\n",
    "                3. Convert Python lists to HuggingFace Datasets\n",
    "                4. Apply tokenization using .map() for efficiency\n",
    "                5. Split validation data into validation and test sets (90%/10%)\n",
    "                6. Preserve evaluation metadata for proper GLEU scoring\n",
    "\n",
    "        Data Sources:\n",
    "                - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "                - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "        Returns:\n",
    "                Tuple[Dataset, List[Dict], List[Dict]]: A tuple containing:\n",
    "                        - train_dataset (Dataset): HuggingFace Dataset with tokenized training examples\n",
    "                        - val_data (List[Dict]): List of tokenized validation examples with metadata\n",
    "                        - test_data (List[Dict]): List of tokenized test examples with metadata\n",
    "\n",
    "        Data Augmentation Strategy:\n",
    "                - Training: augment=True (uses all 4 JFLEG corrections per sentence)\n",
    "                - Validation: augment=False (uses only first correction per sentence)\n",
    "        \"\"\"\n",
    "\n",
    "        from datasets import Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        print(\"[INFO] Creating datasets with augmentation and tokenization...\")\n",
    "\n",
    "        # Step 1: Apply augmentation (returns Python lists)\n",
    "        print(\"\\n[INFO] Applying augmentation to training data...\")\n",
    "        train_augmented_list = self._apply_augmentation(\n",
    "            self.train_data, augment=True)\n",
    "\n",
    "        print(\"[INFO] Applying augmentation to validation data...\")\n",
    "        val_augmented_list = self._apply_augmentation(\n",
    "            self.validation_data, augment=False)\n",
    "\n",
    "        # Step 2: Convert Python lists to HuggingFace Datasets\n",
    "        train_augmented_data = Dataset.from_list(train_augmented_list)\n",
    "        val_augmented_data = Dataset.from_list(val_augmented_list)\n",
    "\n",
    "        # Step 3: Apply tokenization using map\n",
    "        print(\"\\n[INFO] Tokenizing training data...\")\n",
    "        train_augmented_map_data = train_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Training Data\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Tokenizing validation data...\")\n",
    "        val_augmented_map_data = val_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Validation Data\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Split validation dataset into validation and test sets\n",
    "        print(\n",
    "            f\"\\n[INFO] Splitting Validation Data ({100-self.test_split_ratio*100:.0f}%/{self.test_split_ratio*100:.0f}%)...\")\n",
    "        val_data, test_data = train_test_split(\n",
    "            list(val_augmented_map_data),\n",
    "            test_size=self.test_split_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Convert Python validation and test lists to HuggingFace Datasets\n",
    "        val_data = Dataset.from_list(val_data)\n",
    "        test_data = Dataset.from_list(test_data)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nDataset Creation Complete:\")\n",
    "        print(f\"\\t[INFO] Training Dataset:   {len(train_augmented_map_data)}\")\n",
    "        print(f\"\\t[INFO] Validation Dataset: {len(val_data)}\")\n",
    "        print(f\"\\t[INFO] Test Dataset:       {len(test_data)}\")\n",
    "\n",
    "        return train_augmented_map_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc796ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = JFLEGDataset(\u001b[43mtokenizer\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = JFLEGDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b6638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
