{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ed6398",
   "metadata": {},
   "source": [
    "#### **7PAM2015-0509-2024 -- Research Methods in Data Science**\n",
    "##### Mult-Task -- Grammar Correctiona and Paraphrasing -- Implementation on T5 through LoRA.\n",
    "---\n",
    "**Mohit Agarwal (Student ID-22031257)**\n",
    "\n",
    "This notebook trains and evaluate the model.\n",
    "\n",
    "\n",
    "##### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70186f37",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# core T5 training libraries\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    TrainingArguments, Trainer, DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# evalution libraries\n",
    "from evaluate import load\n",
    "import nltk\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebeb50",
   "metadata": {},
   "source": [
    "Changing the default styles and palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82506b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting style\n",
    "sns.set_style(\"darkgrid\")\n",
    "# setting context\n",
    "sns.set_context(\"paper\")\n",
    "# setting palette\n",
    "sns.set_palette(\"deep\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d6134",
   "metadata": {},
   "source": [
    "### JFLEG Dataset Class for Preparation and Augmentation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    \"\"\"\n",
    "    A comprehensive dataset processor for JFLEG (JHU FLuency-Extended GUG) grammar correction data.\n",
    "\n",
    "    This class handles the complete pipeline for preparing JFLEG data for T5-based grammar correction\n",
    "    training, including data loading, preprocessing, augmentation, tokenization, and train/validation/test\n",
    "    splitting. The JFLEG dataset contains 1,511 examples with 4 human-written corrections each, focusing\n",
    "    on fluency improvements rather than minimal edits.\n",
    "\n",
    "    Key Features:\n",
    "        - Comprehensive text preprocessing to handle formatting issues\n",
    "        - Data augmentation using all 4 JFLEG corrections per sentence\n",
    "        - Proper tokenization for T5 sequence-to-sequence training\n",
    "        - Train/validation/test splitting with preserved evaluation metadata\n",
    "        - Temperature-scaled mixing support for multi-task learning\n",
    "\n",
    "    Dataset Sources:\n",
    "        - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "        - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "        max_length (int): Maximum sequence length for tokenization\n",
    "        test_split_ratio (float): Proportion of validation data to use for testing\n",
    "        train_data (Dataset): JFLEG validation split used for training\n",
    "        validation_data (Dataset): JFLEG test split used for validation/testing\n",
    "\n",
    "    Example:\n",
    "        >>> from transformers import T5Tokenizer\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> dataset = JFLEGDataset(tokenizer, max_length=256, test_split_ratio=0.10)\n",
    "        >>> train_data, val_data, test_data = dataset.create_train_val_test_datasets()\n",
    "\n",
    "    References:\n",
    "        - JFLEG Paper: Napoles et al. (2017) \"JFLEG: A Fluency Corpus and Benchmark \n",
    "          for Grammatical Error Correction\"\n",
    "        - Dataset: https://huggingface.co/datasets/jhu-clsp/jfleg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length=256, test_split_ratio=0.10):\n",
    "        \"\"\"\n",
    "        Initialize the JFLEG dataset processor with specified configuration.\n",
    "\n",
    "        Sets up the dataset processor with the provided tokenizer and configuration\n",
    "        parameters, then loads the raw JFLEG datasets for subsequent processing.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (T5Tokenizer): HuggingFace T5 tokenizer instance for text processing.\n",
    "                Must be a properly initialized T5 tokenizer (e.g., from t5-base).\n",
    "            max_length (int, optional): Maximum sequence length for tokenization. \n",
    "                Sequences longer than this will be truncated. Defaults to 256.\n",
    "                Recommended range: 128-512 depending on GPU memory constraints.\n",
    "            test_split_ratio (float, optional): Proportion of validation data to reserve \n",
    "                for final testing. Must be between 0.0 and 1.0. Defaults to 0.10 (10%).\n",
    "                The remaining validation data will be used for model validation during training.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If test_split_ratio is not between 0.0 and 1.0\n",
    "            TypeError: If tokenizer is not a valid T5Tokenizer instance\n",
    "\n",
    "        Note:\n",
    "            The JFLEG dataset splits are used as follows:\n",
    "            - JFLEG 'validation' split → Training data (with augmentation)\n",
    "            - JFLEG 'test' split → Validation and test data (split according to test_split_ratio)\n",
    "\n",
    "            This approach follows standard practice since JFLEG's validation split is larger\n",
    "            and more suitable for training, while the test split is reserved for evaluation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        # Validate test_split_ratio\n",
    "        if not 0.0 <= test_split_ratio <= 1.0:\n",
    "            raise ValueError(\n",
    "                f\"test_split_ratio must be between 0.0 and 1.0, got {test_split_ratio}\")\n",
    "\n",
    "        # Load the JFLEG datasets\n",
    "        print(f\"[INFO] Initializing JFLEG Dataset Processor...\")\n",
    "        print(\n",
    "            f\"[INFO] Max length: {max_length}, Test split ratio: {test_split_ratio:.1%}\")\n",
    "\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG validation split: {len(self.train_data)} examples\")\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG test split: {len(self.validation_data)} examples\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "                text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "                str: The preprocessed and normalized text, or the original input if\n",
    "                        it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "                - Removes multiple consecutive dashes (-- → \"\")\n",
    "                - Fixes decimal formatting (0 . 1 → 0.1)\n",
    "                - Fixes fraction formatting (1 / 2 → 1/2)\n",
    "                - Removes leading zeros in decimals (00.5 → 0.5)\n",
    "                - Joins split numbers (1 2 3 4 → 1234)\n",
    "                - Fixes punctuation spacing (word , → word,)\n",
    "                - Normalizes quote spacing (\" word \" → \"word\")\n",
    "                - Collapses multiple spaces to single spaces\n",
    "                - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "\n",
    "        # if not text or not isinstance(text, str):\n",
    "        #     return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 → 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 → 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 → 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_augmentation(self, data, augment=True):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to JFLEG dataset using all available corrections.\n",
    "\n",
    "        This function processes JFLEG examples to create augmented data by utilizing\n",
    "        all 4 human-written corrections per sentence. Each original sentence is paired with\n",
    "        each of its corrections to create multiple training examples, significantly increasing\n",
    "        the dataset size and providing the model with diverse correction targets.\n",
    "\n",
    "        Args:\n",
    "                data (List[Dict]): List of JFLEG dataset examples, where each example contains:\n",
    "                        - 'sentence' (str): Original grammatically incorrect sentence\n",
    "                        - 'corrections' (List[str]): List of 4 human-written corrections\n",
    "                augment (bool, optional): Whether to use all corrections for augmentation.\n",
    "                        - If True: Creates 4 examples per input (uses all corrections)\n",
    "                        - If False: Creates 1 example per input (uses only first correction)\n",
    "                        Default is True.\n",
    "\n",
    "        Returns:\n",
    "                List[Dict]: Augmented dataset where each dictionary contains:\n",
    "                        - 'input' (str): Preprocessed input with \"grammar: \" prefix\n",
    "                        - 'target' (str): Preprocessed target correction\n",
    "                        - 'processed_sentence' (str): Preprocessed original sentence\n",
    "                        - 'processed_corrections' (List[str]): All 4 preprocessed corrections for evaluation\n",
    "                        - 'raw_original' (str): Unprocessed original sentence (for debugging)\n",
    "                        - 'raw_corrections' (List[str]): Unprocessed corrections (for debugging)\n",
    "        \"\"\"\n",
    "        # storage for augmented data\n",
    "        augmented_data = []\n",
    "        for items in data:\n",
    "            # getting original sentence -- incorrect\n",
    "            original_sentence = items[\"sentence\"]\n",
    "            # formatting the incorrect sentence\n",
    "            processed_sentence = self._preprocess(original_sentence)\n",
    "\n",
    "            # getting all the original corrected sentences\n",
    "            corrections = items[\"corrections\"]\n",
    "\n",
    "            # formatting all the corrected sentences -- evaluation\n",
    "            processed_corrections = []\n",
    "            # looping over all 4 corrections\n",
    "            for correction in corrections:\n",
    "                if correction.strip():  # Skip empty corrections\n",
    "                    # storing all the processed corrections\n",
    "                    processed_corrections.append(self._preprocess(correction))\n",
    "\n",
    "            # looping over processed corrections\n",
    "            for processed_correction in processed_corrections:\n",
    "                # creating a dataset\n",
    "                augmented_data.append({\n",
    "                    \"input\": f\"grammar: {processed_sentence}\",\n",
    "                    \"target\": processed_correction,\n",
    "                    \"processed_sentence\": processed_sentence,\n",
    "                    \"processed_corrections\": processed_corrections,\n",
    "                    \"raw_original\": original_sentence,\n",
    "                    \"raw_corrections\": corrections\n",
    "                })\n",
    "                # checking if to augment or not\n",
    "                if not augment:\n",
    "                    break\n",
    "        # displaying the length of data\n",
    "        print(\"\\t[INFO] Length of Dataset is: \", len(augmented_data))\n",
    "        return augmented_data\n",
    "\n",
    "    def _apply_tokenization(self, data):\n",
    "        \"\"\"\n",
    "        Apply tokenization to preprocessed JFLEG dataset examples for T5 model training.\n",
    "\n",
    "        This function converts text data (input sentences and target corrections) into \n",
    "        tokenized format suitable for T5 model training. It processes both the input \n",
    "        grammar correction task and the target correction, creating the necessary \n",
    "        input_ids, attention_mask, and labels required by the HuggingFace Trainer.\n",
    "\n",
    "        Args:\n",
    "                data (Dict): A single preprocessed example containing:\n",
    "                - 'input' (str): Preprocessed input text with \"grammar: \" prefix\n",
    "                - 'target' (str): Preprocessed target correction text\n",
    "                - 'processed_sentence' (str): Preprocessed original sentence (preserved but not tokenized)\n",
    "                - 'processed_corrections' (List[str]): All preprocessed corrections (preserved but not tokenized)\n",
    "                - 'raw_original' (str): Raw original sentence (preserved but not tokenized)\n",
    "                - 'raw_corrections' (List[str]): Raw corrections (preserved but not tokenized)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                Dict: Tokenized example ready for model training containing:\n",
    "                        - 'input_ids' (List[int]): Token IDs for the input sequence\n",
    "                        - 'attention_mask' (List[int]): Attention mask for input (1 for real tokens, 0 for padding)\n",
    "                        - 'labels' (List[int]): Token IDs for the target sequence (used for loss computation)\n",
    "\n",
    "        Tokenization Settings:\n",
    "                - max_length (int): Maximum sequence length (defined by self.max_length)\n",
    "                - truncation (bool): True - truncates sequences longer than max_length\n",
    "                - padding (bool): False - no padding applied (Trainer handles dynamic padding)\n",
    "                - return_tensors: None - returns Python lists instead of PyTorch tensors\n",
    "        \"\"\"\n",
    "        # tokenizing the input of the dataset\n",
    "        input_encodings = self.tokenizer(data[\"input\"],\n",
    "                                         max_length=self.max_length,\n",
    "                                         truncation=True,\n",
    "                                         padding=False,  # trainer handles the dynamic padding\n",
    "                                         return_tensors=None)  # returns lists not tensor\n",
    "        # tokenizing the target of the dataset\n",
    "        target_encodings = self.tokenizer(data[\"target\"],\n",
    "                                          max_length=self.max_length,\n",
    "                                          truncation=True,\n",
    "                                          padding=False,  # trainer handles the dynamic padding\n",
    "                                          return_tensors=None)  # returns lists not tensor\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]\n",
    "        }\n",
    "\n",
    "    def create_train_val_test_datasets(self):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets with proper augmentation and tokenization.\n",
    "\n",
    "        This function orchestrates the complete data processing pipeline for JFLEG grammar \n",
    "        correction training. It applies data augmentation, converts to HuggingFace datasets,\n",
    "        applies tokenization, and splits the data into appropriate train/validation/test sets\n",
    "        while preserving essential evaluation metadata.\n",
    "\n",
    "        Processing Pipeline:\n",
    "                1. Apply augmentation to training data (4x expansion using all corrections)\n",
    "                2. Apply augmentation to validation data (no expansion, uses first correction only)\n",
    "                3. Convert Python lists to HuggingFace Datasets\n",
    "                4. Apply tokenization using .map() for efficiency\n",
    "                5. Split validation data into validation and test sets (90%/10%)\n",
    "                6. Preserve evaluation metadata for proper GLEU scoring\n",
    "\n",
    "        Data Sources:\n",
    "                - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "                - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "        Returns:\n",
    "                Tuple[Dataset, List[Dict], List[Dict]]: A tuple containing:\n",
    "                        - train_dataset (Dataset): HuggingFace Dataset with tokenized training examples\n",
    "                        - val_data (List[Dict]): List of tokenized validation examples with metadata\n",
    "                        - test_data (List[Dict]): List of tokenized test examples with metadata\n",
    "\n",
    "        Data Augmentation Strategy:\n",
    "                - Training: augment=True (uses all 4 JFLEG corrections per sentence)\n",
    "                - Validation: augment=False (uses only first correction per sentence)\n",
    "        \"\"\"\n",
    "\n",
    "        from datasets import Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        print(\"[INFO] Creating datasets with augmentation and tokenization...\")\n",
    "\n",
    "        # Step 1: Apply augmentation (returns Python lists)\n",
    "        print(\"\\n[INFO] Applying augmentation to training data...\")\n",
    "        train_augmented_list = self._apply_augmentation(\n",
    "            self.train_data, augment=True)\n",
    "\n",
    "        print(\"[INFO] Applying augmentation to validation data...\")\n",
    "        val_augmented_list = self._apply_augmentation(\n",
    "            self.validation_data, augment=False)\n",
    "\n",
    "        # Step 2: Convert Python lists to HuggingFace Datasets\n",
    "        train_augmented_data = Dataset.from_list(train_augmented_list)\n",
    "        val_augmented_data = Dataset.from_list(val_augmented_list)\n",
    "\n",
    "        # Step 3: Apply tokenization using map\n",
    "        print(\"\\n[INFO] Tokenizing training data...\")\n",
    "        train_augmented_map_data = train_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Training Data\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Tokenizing validation data...\")\n",
    "        val_augmented_map_data = val_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Validation Data\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Split validation dataset into validation and test sets\n",
    "        print(\n",
    "            f\"\\n[INFO] Splitting Validation Data ({100-self.test_split_ratio*100:.0f}%/{self.test_split_ratio*100:.0f}%)...\")\n",
    "        val_data, test_data = train_test_split(\n",
    "            list(val_augmented_map_data),\n",
    "            test_size=self.test_split_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Convert Python validation and test lists to HuggingFace Datasets\n",
    "        val_data = Dataset.from_list(val_data)\n",
    "        test_data = Dataset.from_list(test_data)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nDataset Creation Complete:\")\n",
    "        print(f\"\\t[INFO] Training Dataset:   {len(train_augmented_map_data)}\")\n",
    "        print(f\"\\t[INFO] Validation Dataset: {len(val_data)}\")\n",
    "        print(f\"\\t[INFO] Test Dataset:       {len(test_data)}\")\n",
    "\n",
    "        return train_augmented_map_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a952adf",
   "metadata": {},
   "source": [
    "### T5 Model Evaluation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f3b6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarEvaluation:\n",
    "    \"\"\"\n",
    "    Comprehensive Grammar Correction Evaluation Framework.\n",
    "\n",
    "    This class provides a complete evaluation suite for grammar correction systems,\n",
    "    implementing industry-standard metrics specifically designed for assessing\n",
    "    grammatical error correction quality. It combines fluency assessment (GLEU),\n",
    "    semantic preservation (BERTScore), linguistic quality (METEOR), and comprehensive\n",
    "    text statistics.\n",
    "\n",
    "    The evaluation framework is designed for transformer-based models like T5, BERT,\n",
    "    and other sequence-to-sequence architectures fine-tuned on datasets such as JFLEG,\n",
    "    BEA-2019, or custom grammar correction corpora.\n",
    "\n",
    "    Attributes:\n",
    "        bertscore: HuggingFace BERTScore evaluator for semantic similarity\n",
    "        meteor: HuggingFace METEOR evaluator for linguistic quality\n",
    "        metrics_result (Dict): Storage for all computed evaluation metrics\n",
    "\n",
    "    Performance Benchmarks:\n",
    "        - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "        - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent\n",
    "        - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "    References:\n",
    "        - GLEU: Napoles et al. (2017) \"JFLEG: A fluency corpus and benchmark\"\n",
    "        - BERTScore: Zhang et al. (2020) \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        - METEOR: Banerjee & Lavie (2005) \"METEOR: An automatic metric for MT evaluation\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Grammar Evaluation framework.\n",
    "\n",
    "        Sets up evaluation metrics and initializes the results storage structure.\n",
    "        Loads pre-trained models for BERTScore and METEOR evaluation from HuggingFace.\n",
    "\n",
    "        Initializes:\n",
    "            - BERTScore evaluator with microsoft/deberta-xlarge-mnli model\n",
    "            - METEOR evaluator with default configuration\n",
    "            - Results dictionary with nested structure for all metrics\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If required packages (pandas) are not installed\n",
    "\n",
    "        Note:\n",
    "            First initialization may take time to download evaluation models.\n",
    "            Internet connection required for downloading pre-trained models.\n",
    "        \"\"\"\n",
    "        # Load HuggingFace evaluation metrics\n",
    "        self.bertscore = load(\"bertscore\")  # Semantic similarity evaluation\n",
    "        self.meteor = load('meteor')        # Linguistic quality evaluation\n",
    "\n",
    "        # Initialize results storage with hierarchical structure\n",
    "        self.metrics_result = {\n",
    "            \"bertscore\": {              # Semantic preservation metrics\n",
    "                \"precision\": 0.0,       # BERTScore precision\n",
    "                \"recall\": 0.0,          # BERTScore recall\n",
    "                # BERTScore F1 (primary semantic metric)\n",
    "                \"f1\": 0.0\n",
    "            },\n",
    "            \"meteor\": 0.0,              # Linguistic quality score\n",
    "            \"gleu\": 0.0,                # Primary grammar correction metric\n",
    "            \"stats\": {}                 # Comprehensive text statistics\n",
    "        }\n",
    "\n",
    "    def compute_gleu(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute GLEU (Generalized Language Evaluation Understanding) scores for grammar correction.\n",
    "\n",
    "        GLEU is specifically designed for grammar correction evaluation and handles multiple\n",
    "        reference corrections better than traditional BLEU. It measures fluency improvement\n",
    "        while accounting for acceptable variation in correction approaches.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner list\n",
    "                contains multiple valid corrections for the same source sentence (e.g., JFLEG\n",
    "                provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average GLEU score in self.metrics_result[\"gleu\"].\n",
    "\n",
    "        Notes:\n",
    "            - Uses NLTK's sentence_gleu function for computation\n",
    "            - Applies lowercase normalization and word tokenization\n",
    "            - Handles empty or invalid references gracefully with exception handling\n",
    "            - Scores range from 0.0 (no match) to 1.0 (perfect match)\n",
    "            - For grammar correction: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "\n",
    "        Raises:\n",
    "            Exception: Catches and handles any GLEU computation errors by assigning 0.0 score.\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for gleu score\n",
    "        gleu_scores = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # converting predictions to tokens\n",
    "            preds_tokens = nltk.word_tokenize(preds.lower())\n",
    "            # converting all reference to tokens\n",
    "            refs_tokens = [nltk.word_tokenize(ref.lower())\n",
    "                           for ref in refs if ref.strip()]\n",
    "\n",
    "            try:\n",
    "                # computing the score for the gleu\n",
    "                score = sentence_gleu(refs_tokens, preds_tokens)\n",
    "                # updating the storage\n",
    "                gleu_scores.append(score)\n",
    "            except Exception:\n",
    "                gleu_scores.append(0.0)\n",
    "\n",
    "        # computing the average gleu\n",
    "        self.metrics_result[\"gleu\"] = np.mean(gleu_scores)\n",
    "\n",
    "    def compute_bertscore(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute BERTScore metrics (precision, recall, F1) for grammar correction evaluation.\n",
    "\n",
    "        BERTScore measures semantic similarity using contextual embeddings, making it ideal\n",
    "        for evaluating whether grammar corrections preserve semantic meaning while improving\n",
    "        fluency. For each prediction, scores are computed against all available references\n",
    "        and then averaged.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Currently stores results in local variables but doesn't persist them.\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute BERTScore against each of its references\n",
    "            2. Average precision, recall, F1 across references for that prediction\n",
    "            3. Collect averaged scores across all predictions\n",
    "\n",
    "        Notes:\n",
    "            - Uses microsoft/deberta-xlarge-mnli for optimal semantic similarity detection\n",
    "            - Skips empty references automatically\n",
    "            - For grammar correction: F1 > 0.85 indicates excellent semantic preservation\n",
    "            - Each BERTScore call processes one prediction-reference pair\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for bertscore metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing bertscore\n",
    "                    score = self.bertscore.compute(predictions=[preds],\n",
    "                                                   references=[ref],\n",
    "                                                   lang=\"en\",\n",
    "                                                   model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "                    # updating the local storage\n",
    "                    precision.append(score[\"precision\"][0])\n",
    "                    recall.append(score[\"recall\"][0])\n",
    "                    f1.append(score[\"f1\"][0])\n",
    "\n",
    "            # updating bertscore mertics with average\n",
    "            precisions.append(np.mean(precision))\n",
    "            recalls.append(np.mean(recall))\n",
    "            f1s.append(np.mean(f1))\n",
    "\n",
    "        # computing the average bertscore\n",
    "        self.metrics_result[\"bertscore\"] = {\n",
    "            \"precision\": np.mean(precisions),\n",
    "            \"recall\": np.mean(recalls),\n",
    "            \"f1\": np.mean(f1s)\n",
    "        }\n",
    "\n",
    "    def compute_meteor(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute METEOR (Metric for Evaluation of Translation with Explicit ORdering) scores \n",
    "        for grammar correction evaluation.\n",
    "\n",
    "        METEOR is particularly valuable for grammar correction as it incorporates:\n",
    "        - Exact word matching\n",
    "        - Stem matching (handles morphological variations like \"running\" vs \"runs\")\n",
    "        - Synonym matching (recognizes semantically equivalent words)\n",
    "        - Word order penalties\n",
    "\n",
    "        This makes it superior to BLEU for grammar correction where morphological changes\n",
    "        and lexical substitutions are common.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average METEOR score in self.metrics_result[\"meteor\"].\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute METEOR against each of its references\n",
    "            2. Average METEOR scores across references for that prediction  \n",
    "            3. Average across all predictions for final score\n",
    "\n",
    "        Notes:\n",
    "            - METEOR scores range from 0.0 to 1.0 (higher is better)\n",
    "            - For grammar correction: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "            - Handles morphological variations better than BLEU\n",
    "            - Includes recall-oriented evaluation (unlike BLEU's precision focus)\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for meteor metrics\n",
    "        meteors = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            meteor = []\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing meteor\n",
    "                    meteor.append(self.meteor.compute(predictions=[preds],\n",
    "                                                      references=[ref])[\"meteor\"])\n",
    "\n",
    "            # updating meteor mertics with avergae\n",
    "            meteors.append(np.mean(meteor))\n",
    "\n",
    "        # computing the average meteor\n",
    "        self.metrics_result[\"meteor\"] = np.mean(meteors)\n",
    "\n",
    "    def compute_stats(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute comprehensive text statistics for grammar correction evaluation.\n",
    "\n",
    "        This method analyzes various aspects of model predictions versus reference corrections,\n",
    "        providing detailed insights into model behavior patterns, text properties, and \n",
    "        vocabulary usage. All statistics are stored in self.metrics_result[\"stats\"].\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: All statistics are stored in self.metrics_result[\"stats\"] dictionary.\n",
    "\n",
    "        Statistics Computed:\n",
    "\n",
    "            **Sample Information:**\n",
    "            - num_samples: Total number of predictions evaluated\n",
    "            - total_references: Total number of reference corrections across all sentences\n",
    "            - avg_references_per_sentence: Average references available per sentence\n",
    "\n",
    "            **Length Statistics (Word-level):**\n",
    "            - avg/min/max/std_prediction_length: Prediction length statistics in words\n",
    "            - avg/min/max/std_reference_length: Reference length statistics in words\n",
    "\n",
    "            **Character-level Statistics:**\n",
    "            - avg/std_prediction_char_length: Character count statistics for predictions\n",
    "            - avg/std_reference_char_length: Character count statistics for references\n",
    "\n",
    "            **Length Change Analysis:**\n",
    "            - avg/std_length_difference: Difference between prediction and first reference lengths\n",
    "            - positive_length_changes: Count where prediction > reference length (expansion)\n",
    "            - negative_length_changes: Count where prediction < reference length (compression)  \n",
    "            - no_length_changes: Count where prediction == reference length (preserved)\n",
    "\n",
    "            **Vocabulary Analysis:**\n",
    "            - unique_words_in_predictions: Unique word count in all predictions\n",
    "            - unique_words_in_references: Unique word count in all references\n",
    "            - vocab_overlap: Common words between predictions and references\n",
    "            - vocab_overlap_ratio: Overlap ratio (intersection/union of vocabularies)\n",
    "\n",
    "        Notes:\n",
    "            - Length differences computed against first reference for each sentence\n",
    "            - Word counting uses lowercase normalization\n",
    "            - Empty references are skipped in processing\n",
    "            - Vocabulary analysis helps assess model's lexical diversity\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for computing statistics\n",
    "        pred_lengths = []                   # predictions length\n",
    "        pred_char_lengths = []              # predictions char length\n",
    "        all_ref_lengths = []                # reference length\n",
    "        all_ref_char_lengths = []           # reference char length\n",
    "        ref_counts = []                     # np. of reference per sentence\n",
    "        length_diffs = []                   # word difference in preds and refs\n",
    "        pred_word_counts = Counter()        # unique word counts in prediction\n",
    "        ref_word_counts = Counter()         # unique word counts in reference\n",
    "        # no. of len(prediction) > len(reference)\n",
    "        positive_changes = 0\n",
    "        # no. of len(prediction) < len(reference)\n",
    "        negative_changes = 0\n",
    "        # no. of len(prediction) == len(reference)\n",
    "        no_changes = 0\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for pred, refs in zip(predictions, references):\n",
    "            # prediction statistics\n",
    "            pred_len = len(pred.split())\n",
    "            pred_char_len = len(pred)\n",
    "            pred_lengths.append(pred_len)\n",
    "            pred_char_lengths.append(pred_char_len)\n",
    "\n",
    "            # prediction word counts\n",
    "            pred_word_counts.update(pred.lower().split())\n",
    "\n",
    "            # reference statistics\n",
    "            ref_counts.append(len(refs))\n",
    "\n",
    "            # looping for all references for this prediction\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    ref_len = len(ref.split())\n",
    "                    ref_char_len = len(ref)\n",
    "                    all_ref_lengths.append(ref_len)\n",
    "                    all_ref_char_lengths.append(ref_char_len)\n",
    "                    ref_word_counts.update(ref.lower().split())\n",
    "\n",
    "            # length difference analysis (compare with first reference)\n",
    "            if refs:\n",
    "                ref_len = len(refs[0].split()) if refs[0].strip() else 0\n",
    "                length_diff = pred_len - ref_len\n",
    "                length_diffs.append(length_diff)\n",
    "\n",
    "                # counting changes\n",
    "                if length_diff > 0:\n",
    "                    positive_changes += 1\n",
    "                elif length_diff < 0:\n",
    "                    negative_changes += 1\n",
    "                else:\n",
    "                    no_changes += 1\n",
    "\n",
    "        # updating the stats in the metrics_result...\n",
    "\n",
    "        # Sample information (standardized naming)\n",
    "        self.metrics_result[\"stats\"][\"num_samples\"] = len(predictions)\n",
    "        self.metrics_result[\"stats\"][\"total_references\"] = sum(ref_counts)\n",
    "        self.metrics_result[\"stats\"][\"avg_references_per_sentence\"] = np.mean(\n",
    "            ref_counts)\n",
    "\n",
    "        # Prediction statistics\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_length\"] = np.mean(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"min_prediction_length\"] = np.min(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"max_prediction_length\"] = np.max(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_length\"] = np.std(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_char_length\"] = np.mean(\n",
    "            pred_char_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_char_length\"] = np.std(\n",
    "            pred_char_lengths)\n",
    "\n",
    "        # Reference statistics\n",
    "        if all_ref_lengths:  # Handle empty case\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = np.mean(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = np.min(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = np.max(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = np.std(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = np.mean(\n",
    "                all_ref_char_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = np.std(\n",
    "                all_ref_char_lengths)\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = 0.0\n",
    "\n",
    "        # Length difference statistics\n",
    "        if length_diffs:\n",
    "            self.metrics_result[\"stats\"][\"avg_length_difference\"] = np.mean(\n",
    "                length_diffs)\n",
    "            self.metrics_result[\"stats\"][\"std_length_difference\"] = np.std(\n",
    "                length_diffs)\n",
    "            # FIXED: Use pre-calculated variables instead of redundant sum() operations\n",
    "            self.metrics_result[\"stats\"][\"positive_length_changes\"] = positive_changes\n",
    "            self.metrics_result[\"stats\"][\"negative_length_changes\"] = negative_changes\n",
    "            self.metrics_result[\"stats\"][\"no_length_changes\"] = no_changes\n",
    "\n",
    "        # Vocabulary statistics\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_predictions\"] = len(\n",
    "            pred_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_references\"] = len(\n",
    "            ref_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"vocab_overlap\"] = len(\n",
    "            set(pred_word_counts.keys()) & set(ref_word_counts.keys()))\n",
    "\n",
    "        # Vocabulary overlap ratio\n",
    "        if len(pred_word_counts) > 0 and len(ref_word_counts) > 0:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = (\n",
    "                self.metrics_result[\"stats\"][\"vocab_overlap\"] /\n",
    "                len(set(pred_word_counts.keys()) | set(ref_word_counts.keys()))\n",
    "            )\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = 0.0\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Perform comprehensive evaluation of grammar correction predictions.\n",
    "\n",
    "        This is the main evaluation method that computes all metrics and statistics\n",
    "        for grammar correction assessment. It provides a complete analysis including\n",
    "        fluency (GLEU), semantic preservation (BERTScore), linguistic quality (METEOR),\n",
    "        and comprehensive text statistics.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete evaluation results containing:\n",
    "                - \"gleu\": GLEU score (float)\n",
    "                - \"meteor\": METEOR score (float) \n",
    "                - \"bertscore\": Dict with precision, recall, f1 scores\n",
    "                - \"stats\": Dict with comprehensive text statistics\n",
    "\n",
    "        Evaluation Metrics Computed:\n",
    "\n",
    "            **Core Grammar Correction Metrics:**\n",
    "            - GLEU: Primary metric for grammar correction fluency assessment\n",
    "            - BERTScore: Semantic preservation evaluation (precision, recall, F1)\n",
    "            - METEOR: Linguistic quality with morphological awareness\n",
    "\n",
    "            **Comprehensive Statistics:**\n",
    "            - Sample counts and reference information\n",
    "            - Length statistics (words and characters)\n",
    "            - Length change analysis\n",
    "            - Vocabulary analysis and overlap metrics\n",
    "\n",
    "        Performance Benchmarks:\n",
    "            - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "            - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent  \n",
    "            - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "        Notes:\n",
    "            - Progress information is printed during computation\n",
    "            - Results are stored in self.metrics_result and returned\n",
    "            - Statistics are displayed as formatted pandas DataFrame\n",
    "            - All computations handle edge cases gracefully\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Evaluating {len(predictions)} Predictions...\")\n",
    "        # gleu score -- primary for grammar correction\n",
    "        print(\"\\t[INFO] Computing GLEU Score...\")\n",
    "        self.compute_gleu(predictions, references)\n",
    "\n",
    "        # bertscore -- sementic preservation\n",
    "        print(\"\\t[INFO] Computing BERTScore--Precision Recall & F1...\")\n",
    "        self.compute_bertscore(predictions, references)\n",
    "\n",
    "        # meteor -- linquistic quality\n",
    "        print(\"\\t[INFO] Computing METEOR Score...\")\n",
    "        self.compute_meteor(predictions, references)\n",
    "\n",
    "        # statistics\n",
    "        print(\"\\t[INFO] Computing Comprehensive Statistics...\")\n",
    "        self.compute_stats(predictions, references)\n",
    "\n",
    "        print(\"Evaluation Complete:\")\n",
    "        # printing the metrics...\n",
    "        print(f\"\\t[INFO] GLEU: {self.metrics_result['gleu']:.4f}\")\n",
    "        print(f\"\\t[INFO] METEOR: {self.metrics_result['meteor']:.4f}\")\n",
    "        print(\"\\t[INFO] BERTSCORE:\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Precision: {self.metrics_result['bertscore']['precision']:.4f}\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Recall: {self.metrics_result['bertscore']['recall']:.4f}\")\n",
    "        print(f\"\\t\\t[INFO] F1: {self.metrics_result['bertscore']['f1']:.4f}\")\n",
    "\n",
    "        # printing the statistics...\n",
    "        print(\"\\n\\t[INFO] Statistics:\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            stats_df = pd.DataFrame(\n",
    "                list(self.metrics_result[\"stats\"].items()),\n",
    "                columns=[\"Metric\", \"Value\"]\n",
    "            )\n",
    "            print(stats_df.to_string(index=False))\n",
    "        except ImportError:\n",
    "            print(\"\\t[WARNING] Pandas not available, printing raw statistics:\")\n",
    "            for key, value in self.metrics_result[\"stats\"].items():\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "        # Return complete results for further processing\n",
    "        return self.metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202ceda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GrammarEvaluation at 0x1daf2d9f770>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GrammarEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f08f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing JFLEG Dataset Processor...\n",
      "[INFO] Max length: 256, Test split ratio: 10.0%\n",
      "[INFO] Loaded JFLEG validation split: 755 examples\n",
      "[INFO] Loaded JFLEG test split: 748 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.JFLEGDataset at 0x1daf284d2b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JFLEGDataset(\"jkas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea9a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
