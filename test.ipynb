{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8e902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df199749",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a5702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:start\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362898c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b908ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('jfleg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbfda4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd85c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       " 'So I think we could not live if older people did not develop science and technologies . ',\n",
       " 'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       " 'So I think we can not live if old people can not find the science and technology that has not been developed . ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"corrections\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef064c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9bc59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e1e3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "# MODELNAME = \"vennify/t5-base-grammar-correction\"\n",
    "MODELNAME = \"t5-small\"\n",
    "PREFIX = \"grammar: \"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fe0cdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7785a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grammar: he go to school yesterday.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532a21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"dim/grammarly_coedit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f6bf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19519,    10,     3,    88,   281,    12,   496,  4981,     5,     1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c0e8c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"validation\"][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86c61111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beccecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(T5Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8214af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5548d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCorrectionDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset for grammar correction tasks.\n",
    "\n",
    "    This dataset takes input text with grammatical errors and their corrected target text,\n",
    "    tokenizes them using the provided tokenizer, and prepares them for training a grammar\n",
    "    correction model.\n",
    "\n",
    "    Args:\n",
    "            tokenizer: The tokenizer to use for encoding the text\n",
    "            input_text (list): List of input texts containing grammatical errors\n",
    "            target_text (list): List of corresponding corrected texts\n",
    "            max_length (int, optional): Maximum sequence length for tokenization. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "            dict: Dictionary containing:\n",
    "                    - input_ids: Tokenized and padded input text\n",
    "                    - attention_mask: Attention mask for input text\n",
    "                    - labels: Target labels for training (-100 for padding tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, input_text, target_text, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_text = input_text\n",
    "        self.target_text = target_text\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        source = str(\"grammar: \" + self.input_text[id])\n",
    "        target = str(self.target_text[id])\n",
    "\n",
    "        source_tokens = self.tokenizer(\n",
    "            source, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        target_tokens = self.tokenizer(\n",
    "            target, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        labels = target_tokens[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_tokens[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": source_tokens[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "189708b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7d7ec745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = dataset[\"validation\"][\"sentence\"]\n",
    "train_labels = dataset[\"validation\"][\"corrections\"]\n",
    "\n",
    "val_source = dataset[\"test\"][\"sentence\"]\n",
    "val_labels = dataset[\"test\"][\"corrections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ecc36f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GrammarCorrectionDataset(tokenizer, train_source, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5ba47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GrammarCorrectionDataset(tokenizer, val_source, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c1a97d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-small'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a715a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f68e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1e7d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2709076b620>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cee629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[19519,    10,   216,    87,     7,    88,  2746,    12,   214,   762,\n",
      "            81,     8,  8084,     3,     6,  2746,    12,   214,   762,    81,\n",
      "             8,   601,     3,     6,     8,  1687,   672,     5,   175,   128,\n",
      "          6085,    24,   921,    54,  2454,     3,     9,   418,    13,   912,\n",
      "            68,     8,  6688,    33,    59,   435,   780,     3,     5,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [19519,    10,   128,  1552,  1539,   113,    56,   241,    12,   356,\n",
      "          2411,  1034,   143,  1552,  1256,   250,    25,    56,   163,    43,\n",
      "             3,     9,  1627,   903,   190,    17,     8,  2601,     3,     5,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  784,    31,  3845,    42,   255,  2746,    12,   214,   762,    81,\n",
      "             8,  8084,     3,     6,   601,     3,     6,    11,  1687,     3,\n",
      "             6,   175,    33,   128,  6085,    24,   921,    54,  2454,   912,\n",
      "             3,     6,    68,   150,  6688,    43,   118,   435,   780,     3,\n",
      "             5,     3,    31,     6,     3,    31,  3845,    87,     7,    88,\n",
      "          2746,    12,   214,   762,    81,     8,  8084,     3,     6,   601,\n",
      "             3,     6,  1687,     3,     6,   672,     5,     3,   117,   175,\n",
      "            33,  6085,    24,  2454,     3,     9,   418,    13,   912,     3,\n",
      "             6,    68,     8,  6688,    43,    59,   118,   435,   780,     3,\n",
      "             5,     3,    31,     6,     3,    31,  3845,    87,     7,    88,\n",
      "          2746,    12,   214,   762,    81,     8,  8084,     3,     6,   762,\n",
      "            81,   601,     3,     6,  1687,   672,     5,     3,   117,   175,\n",
      "           337,  6085,    24,  2454,     3,     9,   418,    13,   912,    68,\n",
      "             8,  6688,    33,    59,   435,   780,     3,     5,     3,    31,\n",
      "             6,     3,    31,  3845,    87,     7,    88,  2746,    12,   214,\n",
      "           762,    81,     8,  8084,     3,     6,  2746,    12,   214,   762,\n",
      "            81,   601,     3,     6,  1687,     3,     6,   672,     5,    11,\n",
      "           128,    13,   175,  6085,    24,    54,  2454,     3,     9,   418,\n",
      "            13,   912,     3,     6,    68,     8,  6688,    33,    59,   435,\n",
      "           780,     3,     5,     3,    31,   908,     1,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  784,    31, 19055,  1552,  1539,    56,   241,    12,   356,  2411,\n",
      "          1034,    12,   143,     8,  1552,  1256,   250,    25,    56,   163,\n",
      "            43,     3,     9,  1627,   903,   190,     8,  2601,     3,     5,\n",
      "             3,    31,     6,     3,    31, 19055,  1552,  9314,    56,   241,\n",
      "            12,   766,  2411,  1034,     3,     6,    84,    56,   143,     8,\n",
      "          1552,  1256,   250,    25,    56,   163,    43,     3,     9,  1627,\n",
      "           903,   190,     3,     9,  2601,  2034,     3,     5,     3,    31,\n",
      "             6,     3,    31, 19055,  1552,  9314,     3,     6,   113,    56,\n",
      "           241,    12,   356,  2411,  1034,     3,     6,   143,     8,  1552,\n",
      "          1256,   250,    25,    56,   163,    43,     3,     9,  1627,   903,\n",
      "            13,     8,  2601,     3,     5,     3,    31,     6,     3,    31,\n",
      "          5529,  1552,  1539,   113,    56,   241,    12,   356,  2411,  1034,\n",
      "           143,  1552,  1256,   250,    25,    56,   163,    43,     3,     9,\n",
      "          1627,   903,   190,    17,     8,  2601,     3,     5,     3,    31,\n",
      "           908,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6dd4934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c4542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19519,    10,   264,    27,   317,    62,    54,    59,   619,     3,\n",
       "            99,   625,   151,   228,    59,   253,   108,  1433,     7,    11,\n",
       "             3,  5822,    29,  4137,     7,    11,    79,   410,    59,  1344,\n",
       "          3138,     3,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    # type: ignore\n",
    "    str(\"grammar: \" + dataset[\"validation\"][\"sentence\"][0]),\n",
    "    max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\",\n",
    "    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31decacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ArithmeticError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9984800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d381d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_dataset = load_dataset(\"wi_locness\", \"wi\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c442023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1237a95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium size city with eighty thousand inhabitants. It has a high density population because its small territory. Despite of it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the certer of the city which is surrounded by a park. Pasteries are very common and most of them offer the special dessert from the city. There are a comercial zone along the widest street of the city where you can find all kind of establishments: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, groceries, travel agencies, supermarkets and others. Most of the shops have sales and offers at least three months of the year: January, June and August. The quality of the products and services are quite good, because there are a huge competition, however I suggest you taking care about some fakes or cheats.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cebe98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'end', 'text'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be3abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = wi_dataset[\"train\"][\"edits\"][0][\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "648f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = wi_dataset[\"train\"][\"edits\"][0][\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee3758df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium size'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0][starts[0]:ends[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3669d289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium-sized'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa8ea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wi_dataset[\"train\"][\"text\"][0]\n",
    "edits = wi_dataset[\"train\"][\"edits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf127d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': [13,\n",
       "  77,\n",
       "  104,\n",
       "  126,\n",
       "  134,\n",
       "  256,\n",
       "  306,\n",
       "  375,\n",
       "  396,\n",
       "  402,\n",
       "  476,\n",
       "  484,\n",
       "  579,\n",
       "  671,\n",
       "  774,\n",
       "  804,\n",
       "  808,\n",
       "  826,\n",
       "  838,\n",
       "  850,\n",
       "  857,\n",
       "  862,\n",
       "  868],\n",
       " 'end': [24,\n",
       "  78,\n",
       "  104,\n",
       "  133,\n",
       "  136,\n",
       "  262,\n",
       "  315,\n",
       "  379,\n",
       "  399,\n",
       "  411,\n",
       "  480,\n",
       "  498,\n",
       "  588,\n",
       "  671,\n",
       "  777,\n",
       "  807,\n",
       "  810,\n",
       "  835,\n",
       "  845,\n",
       "  856,\n",
       "  861,\n",
       "  867,\n",
       "  873],\n",
       " 'text': ['medium-sized',\n",
       "  '-',\n",
       "  ' of',\n",
       "  'Although',\n",
       "  '',\n",
       "  'center',\n",
       "  None,\n",
       "  'of',\n",
       "  'is',\n",
       "  'commercial',\n",
       "  'kinds',\n",
       "  'businesses',\n",
       "  'grocers',\n",
       "  ' in',\n",
       "  'is',\n",
       "  'is',\n",
       "  '',\n",
       "  '. However,',\n",
       "  'recommend',\n",
       "  'be',\n",
       "  'careful',\n",
       "  'of',\n",
       "  '']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "of\n",
      "careful\n",
      "be\n",
      "recommend\n",
      ". However,\n",
      "\n",
      "is\n",
      "is\n",
      " in\n",
      "grocers\n",
      "businesses\n",
      "kinds\n",
      "commercial\n",
      "is\n",
      "of\n",
      "None\n",
      "center\n",
      "\n",
      "Although\n",
      " of\n",
      "-\n",
      "medium-sized\n"
     ]
    }
   ],
   "source": [
    "edits_list = list(zip(edits[\"start\"], edits[\"end\"], edits[\"text\"]))\n",
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "str = text\n",
    "for start, end, replacement in edits_list:\n",
    "    print(replacement)\n",
    "    if replacement == None:\n",
    "        replacement = \"\"\n",
    "    str = str[:start] + replacement + str[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5846c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f70c5b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(868, 873, ''),\n",
       " (862, 867, 'of'),\n",
       " (857, 861, 'careful'),\n",
       " (850, 856, 'be'),\n",
       " (838, 845, 'recommend'),\n",
       " (826, 835, '. However,'),\n",
       " (808, 810, ''),\n",
       " (804, 807, 'is'),\n",
       " (774, 777, 'is'),\n",
       " (671, 671, ' in'),\n",
       " (579, 588, 'grocers'),\n",
       " (484, 498, 'businesses'),\n",
       " (476, 480, 'kinds'),\n",
       " (402, 411, 'commercial'),\n",
       " (396, 399, 'is'),\n",
       " (375, 379, 'of'),\n",
       " (306, 315, None),\n",
       " (256, 262, 'center'),\n",
       " (134, 136, ''),\n",
       " (126, 133, 'Although'),\n",
       " (104, 104, ' of'),\n",
       " (77, 78, '-'),\n",
       " (13, 24, 'medium-sized')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "edits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "740ccab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7dcba1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_ds = {}\n",
    "for sets in wi_dataset.keys():\n",
    "    for features in wi_dataset[sets]:\n",
    "        incorrect_text = \"\"\n",
    "        correct_text = \"\"\n",
    "        if \"text\" in features and \"edits\" in features:\n",
    "            incorrect_text = features[\"text\"]  # type: ignore\n",
    "            # print(incorrect_text)\n",
    "            correct_text = features[\"text\"]\n",
    "\n",
    "            correct_list = features[\"edits\"]\n",
    "\n",
    "            edits_list = list(zip(correct_list[\"start\"],\n",
    "                                  correct_list[\"end\"], correct_list[\"text\"]))\n",
    "            edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            for start, end, replacement in edits_list:\n",
    "                if replacement == None:\n",
    "                    replacement = \"\"\n",
    "                correct_text = correct_text[:start] + \\\n",
    "                    replacement + correct_text[end:]\n",
    "\n",
    "        if sets not in wi_ds:\n",
    "            wi_ds[sets] = {\"incorrect_text\": [], \"correct_text\": []}\n",
    "\n",
    "        if incorrect_text != correct_text:\n",
    "            wi_ds[sets][\"incorrect_text\"].append(incorrect_text)\n",
    "            wi_ds[sets][\"correct_text\"].append(correct_text)        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bc40fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds[\"train\"][\"correct_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08e8ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b89967de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1551)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(wi_ds[\"train\"][\"incorrect_text\"][i].split())\n",
    "        for i in range(len(wi_ds[\"train\"][\"incorrect_text\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca3628ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d425549",
   "metadata": {},
   "outputs": [],
   "source": [
    "paws = load_dataset(\"paws\", \"labeled_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e02e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49401/49401 [00:00<00:00, 177511.22 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 128974.15 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 137822.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "paraphrases = paws.filter(lambda x: x['label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67857519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 21829\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3536\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3539\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca191715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e59d20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ðŸ” ENVIRONMENT VERIFICATION\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:   PyTorch: 2.7.1+cpu\n",
      "INFO:__main__:   CUDA Available: False\n",
      "WARNING:__main__:   âš ï¸  No GPU detected - training will be slower\n",
      "INFO:__main__:âœ… Environment verification complete!\n",
      "INFO:__main__:==================================================\n"
     ]
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify computational environment and requirements\"\"\"\n",
    "    logger.info(\"ðŸ” ENVIRONMENT VERIFICATION\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"   PyTorch: {torch.__version__}\")\n",
    "    logger.info(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_props = torch.cuda.get_device_properties(0)\n",
    "        logger.info(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "        logger.info(f\"   GPU Memory: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "        logger.info(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"   âš ï¸  No GPU detected - training will be slower\")\n",
    "\n",
    "    # Check available memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        logger.info(f\"   GPU Memory Used: {memory_allocated:.1f} GB\")\n",
    "        logger.info(f\"   GPU Memory Reserved: {memory_reserved:.1f} GB\")\n",
    "\n",
    "    logger.info(\"âœ… Environment verification complete!\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5231032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = [\"grammar: \" +\n",
    "                sentence for sentence in dataset[\"validation\"][\"sentence\"]]\n",
    "train_target = [correction[0]\n",
    "                for correction in dataset[\"validation\"][\"corrections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2990f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokens = tokenizer(train_source, max_length=256,\n",
    "                          truncation=True, padding=False)\n",
    "target_tokens = tokenizer(train_target, max_length=256,\n",
    "                          truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5f3e1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = source_tokens\n",
    "token_dataset[\"labels\"] = target_tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6713dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1659b797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': ['sentence', 'corrections'],\n",
       " 'test': ['sentence', 'corrections']}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c79020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sources, target, max_length=256):\n",
    "    dataset = {}\n",
    "    source = [f\"grammar: {sentence}\" for sentence in sources]\n",
    "    targets = [correction[0]\n",
    "               for correction in target]\n",
    "\n",
    "    source_tokens = tokenizer(source, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    target_tokens = tokenizer(targets, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    # print(target_tokens)\n",
    "\n",
    "    dataset[\"input_ids\"] = source_tokens[\"input_ids\"]\n",
    "    dataset[\"attention_mask\"] = source_tokens[\"attention_mask\"]\n",
    "    dataset[\"labels\"] = target_tokens[\"input_ids\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "db9931e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755/755 [00:00<00:00, 7211.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = dataset[\"validation\"].map(lambda ds: preprocess(dataset[\"validation\"][\"sentence\"],\n",
    "                                                           dataset[\"validation\"][\"corrections\"]),\n",
    "                                     batched=True,\n",
    "                                     remove_columns=dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6ba38723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 748/748 [00:00<00:00, 5310.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_ds = dataset[\"test\"].map(lambda ds: preprocess(dataset[\"test\"][\"sentence\"],\n",
    "                                                   dataset[\"test\"][\"corrections\"]),\n",
    "                             batched=True,\n",
    "                             remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f2a91e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 748\n",
       "})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8d379b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir=r\"D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\",\n",
    "    # Basic setup\n",
    "    overwrite_output_dir=False,          # Overwrite output directory if exists\n",
    "    do_train=True,                       # Whether to run training\n",
    "    do_eval=False,                       # Whether to run evaluation\n",
    "    do_predict=False,                    # Whether to run predictions\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=1.0,                # Number of training epochs\n",
    "    # Max training steps (overrides epochs if set)\n",
    "    max_steps=-1,\n",
    "    per_device_train_batch_size=8,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,        # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=1,        # Steps to accumulate gradients\n",
    "\n",
    "    # Learning rate and optimization\n",
    "    learning_rate=5e-5,                  # Initial learning rate\n",
    "    weight_decay=0.0,                    # Weight decay coefficient\n",
    "    adam_beta1=0.9,                      # Beta1 for Adam optimizer\n",
    "    adam_beta2=0.999,                    # Beta2 for Adam optimizer\n",
    "    adam_epsilon=1e-8,                   # Epsilon for Adam optimizer\n",
    "    max_grad_norm=1.0,                   # Max gradient norm for clipping\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    lr_scheduler_type=\"linear\",          # Type of LR scheduler\n",
    "    warmup_ratio=0.0,                    # Ratio of warmup steps\n",
    "    warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0869c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0a9ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0930a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train/evaluate/predict\n",
    "    args=training_arg,                         # TrainingArguments instance\n",
    "    data_collator=data_collator,                # Function to collate batch data\n",
    "    train_dataset=train_ds,                # Training dataset\n",
    "    eval_dataset=val_ds,                 # Evaluation dataset\n",
    "    tokenizer=tokenizer,                    # Tokenizer for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e0ec9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 03:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=95, training_loss=1.1725575497275904, metrics={'train_runtime': 219.018, 'train_samples_per_second': 3.447, 'train_steps_per_second': 0.434, 'total_flos': 9238399647744.0, 'train_loss': 1.1725575497275904, 'epoch': 1.0})"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d7bab243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'So I think we can not live if old people could not find siences and tecnologies and they did not developped . ',\n",
       " 'corrections': ['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       "  'So I think we could not live if older people did not develop science and technologies . ',\n",
       "  'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       "  'So I think we can not live if old people can not find the science and technology that has not been developed . ']}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7ea13ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "12d44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies. \n",
      "So I think we could not live if older people did not develop science and technologies. \n",
      "So I think we can not live if old people could not find science and technologies and they did not develop. \n",
      "So I think we can not live if old people can not find the science and technology that has not been developed. \n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "punc = re.compile(r'\\s+([.!?,:;])')\n",
    "for items in dataset[\"validation\"]:\n",
    "    print(type(items))\n",
    "    source = punc.sub(r\"\\1\", items[\"sentence\"])\n",
    "    print(source)\n",
    "    targets = items[\"corrections\"]\n",
    "    for correction in targets:\n",
    "        print(re.sub(punc, r\"\\1\", correction))\n",
    "        # augmented_data.append[{\n",
    "        #     \"sentence\": f\"grammar: {source}\",\n",
    "        #     \"correction\": correction,\n",
    "        #     \"original_sentence\": source,\n",
    "        #     \"all_corrections\": targets\n",
    "        # }]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5ee74d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "be3e5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_number = r'\\b(\\d+(?:\\s+\\d+)+)\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    def __init__(self, tokenizer, augment=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augment = augment\n",
    "        # loaading the datasets\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed and normalized text, or the original input if\n",
    "                it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "            - Removes multiple consecutive dashes (-- â†’ \"\")\n",
    "            - Fixes decimal formatting (0 . 1 â†’ 0.1)\n",
    "            - Fixes fraction formatting (1 / 2 â†’ 1/2)\n",
    "            - Removes leading zeros in decimals (00.5 â†’ 0.5)\n",
    "            - Joins split numbers (1 2 3 4 â†’ 1234)\n",
    "            - Fixes punctuation spacing (word , â†’ word,)\n",
    "            - Normalizes quote spacing (\" word \" â†’ \"word\")\n",
    "            - Collapses multiple spaces to single spaces\n",
    "            - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "        \n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 â†’ 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 â†’ 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 â†’ 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def _apply_agument(self, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "96fba9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'corrections'],\n",
       "    num_rows: 755\n",
       "})"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('jfleg', split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7149758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
