{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8e902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df199749",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a5702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:start\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362898c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b908ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('jfleg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbfda4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd85c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       " 'So I think we could not live if older people did not develop science and technologies . ',\n",
       " 'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       " 'So I think we can not live if old people can not find the science and technology that has not been developed . ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"corrections\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef064c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9bc59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1e3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "# MODELNAME = \"vennify/t5-base-grammar-correction\"\n",
    "MODELNAME = \"t5-small\"\n",
    "PREFIX = \"grammar: \"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0cdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f83680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(output, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea089556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he go to school yesterday.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_text.lower().startswith(\"grammar:\")\n",
    "corrected_text[len(\"grammar: \"):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7785a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grammar: he go to school yesterday.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532a21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"dim/grammarly_coedit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f6bf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19519,    10,     3,    88,   281,    12,   496,  4981,     5,     1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0e8c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"validation\"][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c61111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beccecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(T5Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8214af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5548d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCorrectionDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset for grammar correction tasks.\n",
    "\n",
    "    This dataset takes input text with grammatical errors and their corrected target text,\n",
    "    tokenizes them using the provided tokenizer, and prepares them for training a grammar\n",
    "    correction model.\n",
    "\n",
    "    Args:\n",
    "                                    tokenizer: The tokenizer to use for encoding the text\n",
    "                                    input_text (list): List of input texts containing grammatical errors\n",
    "                                    target_text (list): List of corresponding corrected texts\n",
    "                                    max_length (int, optional): Maximum sequence length for tokenization. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "                                    dict: Dictionary containing:\n",
    "                                                                    - input_ids: Tokenized and padded input text\n",
    "                                                                    - attention_mask: Attention mask for input text\n",
    "                                                                    - labels: Target labels for training (-100 for padding tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, input_text, target_text, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_text = input_text\n",
    "        self.target_text = target_text\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        source = str(\"grammar: \" + self.input_text[id])\n",
    "        target = str(self.target_text[id])\n",
    "\n",
    "        source_tokens = self.tokenizer(\n",
    "            source, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        target_tokens = self.tokenizer(\n",
    "            target, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        labels = target_tokens[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_tokens[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": source_tokens[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "189708b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d7ec745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = dataset[\"validation\"][\"sentence\"]\n",
    "train_labels = dataset[\"validation\"][\"corrections\"]\n",
    "\n",
    "val_source = dataset[\"test\"][\"sentence\"]\n",
    "val_labels = dataset[\"test\"][\"corrections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecc36f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GrammarCorrectionDataset(tokenizer, train_source, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5ba47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GrammarCorrectionDataset(tokenizer, val_source, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c1a97d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-small'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a715a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f68e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da1e7d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a8d0a7a900>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28cee629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[19519,    10,   275,   116,    25,   455,     3,     9,   600, 11486,\n",
      "            11,   217,     8,   812,    13,    34,     3,     6,   165,  1327,\n",
      "             3,  2172,    12,   125,    25,  1509,    16,     8,  1328,    27,\n",
      "           473,   114,   190,    53,    34,    44,     3,   189,   972,   522,\n",
      "             3,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [19519,    10,     8,   430,  1053,    19,   186,   151,   217,   128,\n",
      "            13,  1215,    76,  1225,  2948,    38, 21558,   651,     3,    35,\n",
      "          5715,  6151,   725,     8,  5238,     3,     5,     1,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  784,   121,  7175,   116,    25,   455,     3,     9,  2734,  2143,\n",
      "             3,     6,    11,   217,     8,   812,    13,    34,     3,     6,\n",
      "            34,     3,    31,     7,  1327,     3,  2172,    12,   125,    25,\n",
      "           217,    16,  1328,     7,     3,     6,    78, 11008,    27,   217,\n",
      "            80,     3,     6,    27,   473,   114, 13049,    34,    16,    70,\n",
      "           522,     3,     5,    96,     6,    96,  7175,   116,    25,   455,\n",
      "             3,     9,  2734,  2143,    11,   217,     8,   812,    13,    34,\n",
      "             3,     6,    34,     3,    31,     7,  1327,     3,  2172,    12,\n",
      "           125,    25,  1509,    16,     8,  1328,    11,    27,   473,   114,\n",
      "         13049,    34,    44,    70,   522,     3,     5,    96,     6,    96,\n",
      "          7175,   116,    25,   455,     3,     9,  2734,  2143,    11,   217,\n",
      "             8,   812,    13,    34,     3,     6,    34,     3,    31,     7,\n",
      "          1327,     3,  2172,    12,   125,    25,  1509,    16,     8,  1328,\n",
      "             5,    27,   473,   114, 13049,    34,    44,    70,   522,     3,\n",
      "             5,    96,     6,    96, 10555,    25,   455,     3,     9,  2734,\n",
      "          2143,    11,   217,     8,   812,    13,    34,     3,     6,    34,\n",
      "             3,    31,     7,  1327,     3,  2172,    12,   125,    25,  1509,\n",
      "            16,     8,  1328,     3,   117,    27,   473,   114, 13049,    34,\n",
      "            44,    70,   522,     3,     5,    96,   908,     1,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  784,    31,   634,   119,  1053,    19,   186,   151,   217, 13433,\n",
      "          2948,    38,  3873,  1173,     3,     6,    11,    24,  2454,     7,\n",
      "             8,   443,   681,     3,     5,     3,    31,     6,     3,    31,\n",
      "           188,    29,  9269,  1053,    19,    24,   186,   151,   217,   128,\n",
      "         13433,  2948,    38,  3873,  2948,    11,    48,  2454,     7,     8,\n",
      "          5238,     3,     5,     3,    31,     6,     3,    31,   634,   119,\n",
      "          1053,    19,   186,   151,   217,   128,    13,     8, 13433,  2948,\n",
      "            38,  3873,    11,    48,  2454,     7,     8,  5238,     3,     5,\n",
      "             3,    31,     6,     3,    31,   188,    29,  9269,  1053,    19,\n",
      "           186,   151,   217,   128, 13433,  2948,    38,     3,  8387,    76,\n",
      "          2593,    84,  2454,     7,     8,   681,     3,     5,     3,    31,\n",
      "           908,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6dd4934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "471c4542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19519,    10,   264,    27,   317,    62,    54,    59,   619,     3,\n",
       "            99,   625,   151,   228,    59,   253,   108,  1433,     7,    11,\n",
       "             3,  5822,    29,  4137,     7,    11,    79,   410,    59,  1344,\n",
       "          3138,     3,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    # type: ignore\n",
    "    str(\"grammar: \" + dataset[\"validation\"][\"sentence\"][0]),\n",
    "    max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\",\n",
    "    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31decacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ArithmeticError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9984800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d381d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_dataset = load_dataset(\"wi_locness\", \"wi\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c442023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1237a95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium size city with eighty thousand inhabitants. It has a high density population because its small territory. Despite of it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the certer of the city which is surrounded by a park. Pasteries are very common and most of them offer the special dessert from the city. There are a comercial zone along the widest street of the city where you can find all kind of establishments: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, groceries, travel agencies, supermarkets and others. Most of the shops have sales and offers at least three months of the year: January, June and August. The quality of the products and services are quite good, because there are a huge competition, however I suggest you taking care about some fakes or cheats.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cebe98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'end', 'text'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be3abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = wi_dataset[\"train\"][\"edits\"][0][\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "648f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = wi_dataset[\"train\"][\"edits\"][0][\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee3758df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium size'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0][starts[0]:ends[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3669d289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium-sized'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa8ea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wi_dataset[\"train\"][\"text\"][0]\n",
    "edits = wi_dataset[\"train\"][\"edits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf127d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': [13,\n",
       "  77,\n",
       "  104,\n",
       "  126,\n",
       "  134,\n",
       "  256,\n",
       "  306,\n",
       "  375,\n",
       "  396,\n",
       "  402,\n",
       "  476,\n",
       "  484,\n",
       "  579,\n",
       "  671,\n",
       "  774,\n",
       "  804,\n",
       "  808,\n",
       "  826,\n",
       "  838,\n",
       "  850,\n",
       "  857,\n",
       "  862,\n",
       "  868],\n",
       " 'end': [24,\n",
       "  78,\n",
       "  104,\n",
       "  133,\n",
       "  136,\n",
       "  262,\n",
       "  315,\n",
       "  379,\n",
       "  399,\n",
       "  411,\n",
       "  480,\n",
       "  498,\n",
       "  588,\n",
       "  671,\n",
       "  777,\n",
       "  807,\n",
       "  810,\n",
       "  835,\n",
       "  845,\n",
       "  856,\n",
       "  861,\n",
       "  867,\n",
       "  873],\n",
       " 'text': ['medium-sized',\n",
       "  '-',\n",
       "  ' of',\n",
       "  'Although',\n",
       "  '',\n",
       "  'center',\n",
       "  None,\n",
       "  'of',\n",
       "  'is',\n",
       "  'commercial',\n",
       "  'kinds',\n",
       "  'businesses',\n",
       "  'grocers',\n",
       "  ' in',\n",
       "  'is',\n",
       "  'is',\n",
       "  '',\n",
       "  '. However,',\n",
       "  'recommend',\n",
       "  'be',\n",
       "  'careful',\n",
       "  'of',\n",
       "  '']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "262bed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "of\n",
      "careful\n",
      "be\n",
      "recommend\n",
      ". However,\n",
      "\n",
      "is\n",
      "is\n",
      " in\n",
      "grocers\n",
      "businesses\n",
      "kinds\n",
      "commercial\n",
      "is\n",
      "of\n",
      "None\n",
      "center\n",
      "\n",
      "Although\n",
      " of\n",
      "-\n",
      "medium-sized\n"
     ]
    }
   ],
   "source": [
    "edits_list = list(zip(edits[\"start\"], edits[\"end\"], edits[\"text\"]))\n",
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "str = text\n",
    "for start, end, replacement in edits_list:\n",
    "    print(replacement)\n",
    "    if replacement == None:\n",
    "        replacement = \"\"\n",
    "    str = str[:start] + replacement + str[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5846c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f70c5b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(868, 873, ''),\n",
       " (862, 867, 'of'),\n",
       " (857, 861, 'careful'),\n",
       " (850, 856, 'be'),\n",
       " (838, 845, 'recommend'),\n",
       " (826, 835, '. However,'),\n",
       " (808, 810, ''),\n",
       " (804, 807, 'is'),\n",
       " (774, 777, 'is'),\n",
       " (671, 671, ' in'),\n",
       " (579, 588, 'grocers'),\n",
       " (484, 498, 'businesses'),\n",
       " (476, 480, 'kinds'),\n",
       " (402, 411, 'commercial'),\n",
       " (396, 399, 'is'),\n",
       " (375, 379, 'of'),\n",
       " (306, 315, None),\n",
       " (256, 262, 'center'),\n",
       " (134, 136, ''),\n",
       " (126, 133, 'Although'),\n",
       " (104, 104, ' of'),\n",
       " (77, 78, '-'),\n",
       " (13, 24, 'medium-sized')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "edits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "740ccab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dcba1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_ds = {}\n",
    "for sets in wi_dataset.keys():\n",
    "    for features in wi_dataset[sets]:\n",
    "        incorrect_text = \"\"\n",
    "        correct_text = \"\"\n",
    "        if \"text\" in features and \"edits\" in features:\n",
    "            incorrect_text = features[\"text\"]  # type: ignore\n",
    "            # print(incorrect_text)\n",
    "            correct_text = features[\"text\"]\n",
    "\n",
    "            correct_list = features[\"edits\"]\n",
    "\n",
    "            edits_list = list(zip(correct_list[\"start\"],\n",
    "                                  correct_list[\"end\"], correct_list[\"text\"]))\n",
    "            edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            for start, end, replacement in edits_list:\n",
    "                if replacement == None:\n",
    "                    replacement = \"\"\n",
    "                correct_text = correct_text[:start] + \\\n",
    "                    replacement + correct_text[end:]\n",
    "\n",
    "        if sets not in wi_ds:\n",
    "            wi_ds[sets] = {\"incorrect_text\": [], \"correct_text\": []}\n",
    "\n",
    "        if incorrect_text != correct_text:\n",
    "            wi_ds[sets][\"incorrect_text\"].append(incorrect_text)\n",
    "            wi_ds[sets][\"correct_text\"].append(correct_text)        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bc40fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds[\"train\"][\"correct_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08e8ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b89967de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1551)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(wi_ds[\"train\"][\"incorrect_text\"][i].split())\n",
    "        for i in range(len(wi_ds[\"train\"][\"incorrect_text\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca3628ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d425549",
   "metadata": {},
   "outputs": [],
   "source": [
    "paws = load_dataset(\"paws\", \"labeled_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e02e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrases = paws.filter(lambda x: x['label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67857519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 21829\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3536\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3539\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca191715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e59d20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:🔍 ENVIRONMENT VERIFICATION\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:   PyTorch: 2.7.1+cpu\n",
      "INFO:__main__:   CUDA Available: False\n",
      "WARNING:__main__:   ⚠️  No GPU detected - training will be slower\n",
      "INFO:__main__:✅ Environment verification complete!\n",
      "INFO:__main__:==================================================\n"
     ]
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify computational environment and requirements\"\"\"\n",
    "    logger.info(\"🔍 ENVIRONMENT VERIFICATION\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"   PyTorch: {torch.__version__}\")\n",
    "    logger.info(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_props = torch.cuda.get_device_properties(0)\n",
    "        logger.info(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "        logger.info(f\"   GPU Memory: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "        logger.info(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"   ⚠️  No GPU detected - training will be slower\")\n",
    "\n",
    "    # Check available memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        logger.info(f\"   GPU Memory Used: {memory_allocated:.1f} GB\")\n",
    "        logger.info(f\"   GPU Memory Reserved: {memory_reserved:.1f} GB\")\n",
    "\n",
    "    logger.info(\"✅ Environment verification complete!\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5231032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = [\"grammar: \" +\n",
    "                sentence for sentence in dataset[\"validation\"][\"sentence\"]]\n",
    "train_target = [correction[0]\n",
    "                for correction in dataset[\"validation\"][\"corrections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2990f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokens = tokenizer(train_source, max_length=256,\n",
    "                          truncation=True, padding=False)\n",
    "target_tokens = tokenizer(train_target, max_length=256,\n",
    "                          truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f3e1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = source_tokens\n",
    "token_dataset[\"labels\"] = target_tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6713dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1659b797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': ['sentence', 'corrections'],\n",
       " 'test': ['sentence', 'corrections']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c79020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sources, target, max_length=256):\n",
    "    dataset = {}\n",
    "    source = [f\"grammar: {sentence}\" for sentence in sources]\n",
    "    targets = [correction[0]\n",
    "               for correction in target]\n",
    "\n",
    "    source_tokens = tokenizer(source, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    target_tokens = tokenizer(targets, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    # print(target_tokens)\n",
    "\n",
    "    dataset[\"input_ids\"] = source_tokens[\"input_ids\"]\n",
    "    dataset[\"attention_mask\"] = source_tokens[\"attention_mask\"]\n",
    "    dataset[\"labels\"] = target_tokens[\"input_ids\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db9931e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset[\"validation\"].map(lambda ds: preprocess(dataset[\"validation\"][\"sentence\"],\n",
    "                                                           dataset[\"validation\"][\"corrections\"]),\n",
    "                                     batched=True,\n",
    "                                     remove_columns=dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ba38723",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = dataset[\"test\"].map(lambda ds: preprocess(dataset[\"test\"][\"sentence\"],\n",
    "                                                   dataset[\"test\"][\"corrections\"]),\n",
    "                             batched=True,\n",
    "                             remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2a91e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 748\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d379b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir=r\"D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\",\n",
    "    # Basic setup\n",
    "    overwrite_output_dir=False,          # Overwrite output directory if exists\n",
    "    do_train=True,                       # Whether to run training\n",
    "    do_eval=False,                       # Whether to run evaluation\n",
    "    do_predict=False,                    # Whether to run predictions\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=1.0,                # Number of training epochs\n",
    "    # Max training steps (overrides epochs if set)\n",
    "    max_steps=-1,\n",
    "    per_device_train_batch_size=8,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,        # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=1,        # Steps to accumulate gradients\n",
    "\n",
    "    # Learning rate and optimization\n",
    "    learning_rate=5e-5,                  # Initial learning rate\n",
    "    weight_decay=0.0,                    # Weight decay coefficient\n",
    "    adam_beta1=0.9,                      # Beta1 for Adam optimizer\n",
    "    adam_beta2=0.999,                    # Beta2 for Adam optimizer\n",
    "    adam_epsilon=1e-8,                   # Epsilon for Adam optimizer\n",
    "    max_grad_norm=1.0,                   # Max gradient norm for clipping\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    lr_scheduler_type=\"linear\",          # Type of LR scheduler\n",
    "    warmup_ratio=0.0,                    # Ratio of warmup steps\n",
    "    warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0869c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a9ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0930a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train/evaluate/predict\n",
    "    args=training_arg,                         # TrainingArguments instance\n",
    "    data_collator=data_collator,                # Function to collate batch data\n",
    "    train_dataset=train_ds,                # Training dataset\n",
    "    eval_dataset=val_ds,                 # Evaluation dataset\n",
    "    tokenizer=tokenizer,                    # Tokenizer for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0ec9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79757696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7bab243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'So I think we can not live if old people could not find siences and tecnologies and they did not developped . ',\n",
       " 'corrections': ['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       "  'So I think we could not live if older people did not develop science and technologies . ',\n",
       "  'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       "  'So I think we can not live if old people can not find the science and technology that has not been developed . ']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ea13ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12d44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies. \n",
      "So I think we could not live if older people did not develop science and technologies. \n",
      "So I think we can not live if old people could not find science and technologies and they did not develop. \n",
      "So I think we can not live if old people can not find the science and technology that has not been developed. \n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "punc = re.compile(r'\\s+([.!?,:;])')\n",
    "for items in dataset[\"validation\"]:\n",
    "    print(type(items))\n",
    "    source = punc.sub(r\"\\1\", items[\"sentence\"])\n",
    "    print(source)\n",
    "    targets = items[\"corrections\"]\n",
    "    for correction in targets:\n",
    "        print(re.sub(punc, r\"\\1\", correction))\n",
    "        if correction.strip():\n",
    "            augmented_data.append({\n",
    "                \"sentence\": f\"grammar: {source}\",\n",
    "                \"correction\": correction,\n",
    "                \"original_sentence\": source,\n",
    "                \"all_corrections\": targets\n",
    "            })\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ee74d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be3e5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_number = r'\\b(\\d+(?:\\s+\\d+)+)\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "183a5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    \"\"\"\n",
    "    A comprehensive dataset processor for JFLEG (JHU FLuency-Extended GUG) grammar correction data.\n",
    "\n",
    "    This class handles the complete pipeline for preparing JFLEG data for T5-based grammar correction\n",
    "    training, including data loading, preprocessing, augmentation, tokenization, and train/validation/test\n",
    "    splitting. The JFLEG dataset contains 1,511 examples with 4 human-written corrections each, focusing\n",
    "    on fluency improvements rather than minimal edits.\n",
    "\n",
    "    Key Features:\n",
    "            - Comprehensive text preprocessing to handle formatting issues\n",
    "            - Data augmentation using all 4 JFLEG corrections per sentence\n",
    "            - Proper tokenization for T5 sequence-to-sequence training\n",
    "            - Train/validation/test splitting with preserved evaluation metadata\n",
    "            - Temperature-scaled mixing support for multi-task learning\n",
    "\n",
    "    Dataset Sources:\n",
    "            - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "            - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "    Attributes:\n",
    "            tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "            max_length (int): Maximum sequence length for tokenization\n",
    "            test_split_ratio (float): Proportion of validation data to use for testing\n",
    "            train_data (Dataset): JFLEG validation split used for training\n",
    "            validation_data (Dataset): JFLEG test split used for validation/testing\n",
    "\n",
    "    Example:\n",
    "            >>> from transformers import T5Tokenizer\n",
    "            >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> dataset = JFLEGDataset(tokenizer, max_length=256, test_split_ratio=0.10)\n",
    "            >>> train_data, val_data, test_data = dataset.create_train_val_test_datasets()\n",
    "\n",
    "    References:\n",
    "            - JFLEG Paper: Napoles et al. (2017) \"JFLEG: A Fluency Corpus and Benchmark \n",
    "              for Grammatical Error Correction\"\n",
    "            - Dataset: https://huggingface.co/datasets/jhu-clsp/jfleg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length=256, test_split_ratio=0.10):\n",
    "        \"\"\"\n",
    "        Initialize the JFLEG dataset processor with specified configuration.\n",
    "\n",
    "        Sets up the dataset processor with the provided tokenizer and configuration\n",
    "        parameters, then loads the raw JFLEG datasets for subsequent processing.\n",
    "\n",
    "        Args:\n",
    "                tokenizer (T5Tokenizer): HuggingFace T5 tokenizer instance for text processing.\n",
    "                        Must be a properly initialized T5 tokenizer (e.g., from t5-base).\n",
    "                max_length (int, optional): Maximum sequence length for tokenization. \n",
    "                        Sequences longer than this will be truncated. Defaults to 256.\n",
    "                        Recommended range: 128-512 depending on GPU memory constraints.\n",
    "                test_split_ratio (float, optional): Proportion of validation data to reserve \n",
    "                        for final testing. Must be between 0.0 and 1.0. Defaults to 0.10 (10%).\n",
    "                        The remaining validation data will be used for model validation during training.\n",
    "\n",
    "        Raises:\n",
    "                ValueError: If test_split_ratio is not between 0.0 and 1.0\n",
    "                TypeError: If tokenizer is not a valid T5Tokenizer instance\n",
    "\n",
    "        Note:\n",
    "                The JFLEG dataset splits are used as follows:\n",
    "                - JFLEG 'validation' split → Training data (with augmentation)\n",
    "                - JFLEG 'test' split → Validation and test data (split according to test_split_ratio)\n",
    "\n",
    "                This approach follows standard practice since JFLEG's validation split is larger\n",
    "                and more suitable for training, while the test split is reserved for evaluation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        # Validate test_split_ratio\n",
    "        if not 0.0 <= test_split_ratio <= 1.0:\n",
    "            raise ValueError(\n",
    "                f\"test_split_ratio must be between 0.0 and 1.0, got {test_split_ratio}\")\n",
    "\n",
    "        # Load the JFLEG datasets\n",
    "        print(f\"[INFO] Initializing JFLEG Dataset Processor...\")\n",
    "        print(\n",
    "            f\"[INFO] Max length: {max_length}, Test split ratio: {test_split_ratio:.1%}\")\n",
    "\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG validation split: {len(self.train_data)} examples\")\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG test split: {len(self.validation_data)} examples\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "                        text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "                        str: The preprocessed and normalized text, or the original input if\n",
    "                                        it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "                        - Removes multiple consecutive dashes (-- → \"\")\n",
    "                        - Fixes decimal formatting (0 . 1 → 0.1)\n",
    "                        - Fixes fraction formatting (1 / 2 → 1/2)\n",
    "                        - Removes leading zeros in decimals (00.5 → 0.5)\n",
    "                        - Joins split numbers (1 2 3 4 → 1234)\n",
    "                        - Fixes punctuation spacing (word , → word,)\n",
    "                        - Normalizes quote spacing (\" word \" → \"word\")\n",
    "                        - Collapses multiple spaces to single spaces\n",
    "                        - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "\n",
    "        # if not text or not isinstance(text, str):\n",
    "        #     return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 → 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 → 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 → 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_augmentation(self, data, augment=True):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to JFLEG dataset using all available corrections.\n",
    "\n",
    "        This function processes JFLEG examples to create augmented data by utilizing\n",
    "        all 4 human-written corrections per sentence. Each original sentence is paired with\n",
    "        each of its corrections to create multiple training examples, significantly increasing\n",
    "        the dataset size and providing the model with diverse correction targets.\n",
    "\n",
    "        Args:\n",
    "                        data (List[Dict]): List of JFLEG dataset examples, where each example contains:\n",
    "                                        - 'sentence' (str): Original grammatically incorrect sentence\n",
    "                                        - 'corrections' (List[str]): List of 4 human-written corrections\n",
    "                        augment (bool, optional): Whether to use all corrections for augmentation.\n",
    "                                        - If True: Creates 4 examples per input (uses all corrections)\n",
    "                                        - If False: Creates 1 example per input (uses only first correction)\n",
    "                                        Default is True.\n",
    "\n",
    "        Returns:\n",
    "                        List[Dict]: Augmented dataset where each dictionary contains:\n",
    "                                        - 'input' (str): Preprocessed input with \"grammar: \" prefix\n",
    "                                        - 'target' (str): Preprocessed target correction\n",
    "                                        - 'processed_sentence' (str): Preprocessed original sentence\n",
    "                                        - 'processed_corrections' (List[str]): All 4 preprocessed corrections for evaluation\n",
    "                                        - 'raw_original' (str): Unprocessed original sentence (for debugging)\n",
    "                                        - 'raw_corrections' (List[str]): Unprocessed corrections (for debugging)\n",
    "        \"\"\"\n",
    "        # storage for augmented data\n",
    "        augmented_data = []\n",
    "        for items in data:\n",
    "            # getting original sentence -- incorrect\n",
    "            original_sentence = items[\"sentence\"]\n",
    "            # formatting the incorrect sentence\n",
    "            processed_sentence = self._preprocess(original_sentence)\n",
    "\n",
    "            # getting all the original corrected sentences\n",
    "            corrections = items[\"corrections\"]\n",
    "\n",
    "            # formatting all the corrected sentences -- evaluation\n",
    "            processed_corrections = []\n",
    "            # looping over all 4 corrections\n",
    "            for correction in corrections:\n",
    "                if correction.strip():  # Skip empty corrections\n",
    "                    # storing all the processed corrections\n",
    "                    processed_corrections.append(self._preprocess(correction))\n",
    "\n",
    "            # looping over processed corrections\n",
    "            for processed_correction in processed_corrections:\n",
    "                # creating a dataset\n",
    "                augmented_data.append({\n",
    "                    \"input\": f\"grammar: {processed_sentence}\",\n",
    "                    \"target\": processed_correction,\n",
    "                    \"processed_sentence\": processed_sentence,\n",
    "                    \"processed_corrections\": processed_corrections,\n",
    "                    \"raw_original\": original_sentence,\n",
    "                    \"raw_corrections\": corrections\n",
    "                })\n",
    "                # checking if to augment or not\n",
    "                if not augment:\n",
    "                    break\n",
    "        # displaying the length of data\n",
    "        print(\"[INFO] Length of Dataset is: \", len(augmented_data))\n",
    "        return augmented_data\n",
    "\n",
    "    def _apply_tokenization(self, data):\n",
    "        \"\"\"\n",
    "        Apply tokenization to preprocessed JFLEG dataset examples for T5 model training.\n",
    "\n",
    "        This function converts text data (input sentences and target corrections) into \n",
    "        tokenized format suitable for T5 model training. It processes both the input \n",
    "        grammar correction task and the target correction, creating the necessary \n",
    "        input_ids, attention_mask, and labels required by the HuggingFace Trainer.\n",
    "\n",
    "        Args:\n",
    "                        data (Dict): A single preprocessed example containing:\n",
    "                        - 'input' (str): Preprocessed input text with \"grammar: \" prefix\n",
    "                        - 'target' (str): Preprocessed target correction text\n",
    "                        - 'processed_sentence' (str): Preprocessed original sentence (preserved but not tokenized)\n",
    "                        - 'processed_corrections' (List[str]): All preprocessed corrections (preserved but not tokenized)\n",
    "                        - 'raw_original' (str): Raw original sentence (preserved but not tokenized)\n",
    "                        - 'raw_corrections' (List[str]): Raw corrections (preserved but not tokenized)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                        Dict: Tokenized example ready for model training containing:\n",
    "                                        - 'input_ids' (List[int]): Token IDs for the input sequence\n",
    "                                        - 'attention_mask' (List[int]): Attention mask for input (1 for real tokens, 0 for padding)\n",
    "                                        - 'labels' (List[int]): Token IDs for the target sequence (used for loss computation)\n",
    "\n",
    "        Tokenization Settings:\n",
    "                        - max_length (int): Maximum sequence length (defined by self.max_length)\n",
    "                        - truncation (bool): True - truncates sequences longer than max_length\n",
    "                        - padding (bool): False - no padding applied (Trainer handles dynamic padding)\n",
    "                        - return_tensors: None - returns Python lists instead of PyTorch tensors\n",
    "        \"\"\"\n",
    "        # tokenizing the input of the dataset\n",
    "        input_encodings = self.tokenizer(data[\"input\"],\n",
    "                                         max_length=self.max_length,\n",
    "                                         truncation=True,\n",
    "                                         padding=False,  # trainer handles the dynamic padding\n",
    "                                         return_tensors=None)  # returns lists not tensor\n",
    "        # tokenizing the target of the dataset\n",
    "        target_encodings = self.tokenizer(data[\"target\"],\n",
    "                                          max_length=self.max_length,\n",
    "                                          truncation=True,\n",
    "                                          padding=False,  # trainer handles the dynamic padding\n",
    "                                          return_tensors=None)  # returns lists not tensor\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]\n",
    "        }\n",
    "\n",
    "    def create_train_val_test_datasets(self):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets with proper augmentation and tokenization.\n",
    "\n",
    "        This function orchestrates the complete data processing pipeline for JFLEG grammar \n",
    "        correction training. It applies data augmentation, converts to HuggingFace datasets,\n",
    "        applies tokenization, and splits the data into appropriate train/validation/test sets\n",
    "        while preserving essential evaluation metadata.\n",
    "\n",
    "        Processing Pipeline:\n",
    "                        1. Apply augmentation to training data (4x expansion using all corrections)\n",
    "                        2. Apply augmentation to validation data (no expansion, uses first correction only)\n",
    "                        3. Convert Python lists to HuggingFace Datasets\n",
    "                        4. Apply tokenization using .map() for efficiency\n",
    "                        5. Split validation data into validation and test sets (90%/10%)\n",
    "                        6. Preserve evaluation metadata for proper GLEU scoring\n",
    "\n",
    "        Data Sources:\n",
    "                        - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "                        - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "        Returns:\n",
    "                        Tuple[Dataset, List[Dict], List[Dict]]: A tuple containing:\n",
    "                                        - train_dataset (Dataset): HuggingFace Dataset with tokenized training examples\n",
    "                                        - val_data (List[Dict]): List of tokenized validation examples with metadata\n",
    "                                        - test_data (List[Dict]): List of tokenized test examples with metadata\n",
    "\n",
    "        Data Augmentation Strategy:\n",
    "                        - Training: augment=True (uses all 4 JFLEG corrections per sentence)\n",
    "                        - Validation: augment=False (uses only first correction per sentence)\n",
    "        \"\"\"\n",
    "\n",
    "        from datasets import Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        print(\"[INFO] Creating datasets with augmentation and tokenization...\")\n",
    "\n",
    "        # Step 1: Apply augmentation (returns Python lists)\n",
    "        print(\"[INFO] Applying augmentation to training data...\")\n",
    "        train_augmented_list = self._apply_augmentation(\n",
    "            self.train_data, augment=True)\n",
    "\n",
    "        print(\"[INFO] Applying augmentation to validation data...\")\n",
    "        val_augmented_list = self._apply_augmentation(\n",
    "            self.validation_data, augment=False)\n",
    "\n",
    "        # Step 2: Convert Python lists to HuggingFace Datasets\n",
    "        train_augmented_data = Dataset.from_list(train_augmented_list)\n",
    "        val_augmented_data = Dataset.from_list(val_augmented_list)\n",
    "\n",
    "        # Step 3: Apply tokenization using map\n",
    "        print(\"\\n[INFO] Tokenizing training data...\")\n",
    "        train_augmented_map_data = train_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Training Data\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Tokenizing validation data...\")\n",
    "        val_augmented_map_data = val_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Validation Data\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Split validation dataset into validation and test sets\n",
    "        print(\n",
    "            f\"\\n[INFO] Splitting Validation Data ({100-self.test_split_ratio*100:.0f}%/{self.test_split_ratio*100:.0f}%)...\")\n",
    "        val_data, test_data = train_test_split(\n",
    "            list(val_augmented_map_data),\n",
    "            test_size=self.test_split_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Convert Python validation and test lists to HuggingFace Datasets\n",
    "        val_data = Dataset.from_list(val_data)\n",
    "        test_data = Dataset.from_list(test_data)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nDataset Creation Complete:\")\n",
    "        print(f\"\\t[INFO] Training Dataset:   {len(train_augmented_map_data)}\")\n",
    "        print(f\"\\t[INFO] Validation Dataset: {len(val_data)}\")\n",
    "        print(f\"\\t[INFO] Test Dataset:       {len(test_data)}\")\n",
    "\n",
    "        return train_augmented_map_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "15c221d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing JFLEG Dataset Processor...\n",
      "[INFO] Max length: 256, Test split ratio: 10.0%\n",
      "[INFO] Loaded JFLEG validation split: 755 examples\n",
      "[INFO] Loaded JFLEG test split: 748 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = JFLEGDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea7efd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating datasets with augmentation and tokenization...\n",
      "[INFO] Applying augmentation to training data...\n",
      "[INFO] Length of Dataset is:  3016\n",
      "[INFO] Applying augmentation to validation data...\n",
      "[INFO] Length of Dataset is:  747\n",
      "\n",
      "[INFO] Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e788e90d9480b8335c7d213c68c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Training Data:   0%|          | 0/3016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b969af2361942ada304522e01f5c1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Validation Data:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Splitting Validation Data (90%/10%)...\n",
      "\n",
      "Dataset Creation Complete:\n",
      "\t[INFO] Training Dataset:   3016\n",
      "\t[INFO] Validation Dataset: 672\n",
      "\t[INFO] Test Dataset:       75\n"
     ]
    }
   ],
   "source": [
    "train, val, test = dataset.create_train_val_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "544eddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3016\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5cc8cead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 672\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "43a17b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 75\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "889badba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:this is a test for logger\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"this is a test for logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85561ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import nltk\n",
    "from nltk.translate.gleu_score import sentence_gleu, corpus_gleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38629390",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17f19c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da3c2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarEvaluation:\n",
    "    \"\"\"\n",
    "    Comprehensive Grammar Correction Evaluation Framework.\n",
    "\n",
    "    This class provides a complete evaluation suite for grammar correction systems,\n",
    "    implementing industry-standard metrics specifically designed for assessing\n",
    "    grammatical error correction quality. It combines fluency assessment (GLEU),\n",
    "    semantic preservation (BERTScore), linguistic quality (METEOR), and comprehensive\n",
    "    text statistics.\n",
    "\n",
    "    The evaluation framework is designed for transformer-based models like T5, BERT,\n",
    "    and other sequence-to-sequence architectures fine-tuned on datasets such as JFLEG,\n",
    "    BEA-2019, or custom grammar correction corpora.\n",
    "\n",
    "    Attributes:\n",
    "        bertscore: HuggingFace BERTScore evaluator for semantic similarity\n",
    "        meteor: HuggingFace METEOR evaluator for linguistic quality\n",
    "        metrics_result (Dict): Storage for all computed evaluation metrics\n",
    "\n",
    "    Performance Benchmarks:\n",
    "        - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "        - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent\n",
    "        - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "    References:\n",
    "        - GLEU: Napoles et al. (2017) \"JFLEG: A fluency corpus and benchmark\"\n",
    "        - BERTScore: Zhang et al. (2020) \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        - METEOR: Banerjee & Lavie (2005) \"METEOR: An automatic metric for MT evaluation\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Grammar Evaluation framework.\n",
    "\n",
    "        Sets up evaluation metrics and initializes the results storage structure.\n",
    "        Loads pre-trained models for BERTScore and METEOR evaluation from HuggingFace.\n",
    "\n",
    "        Initializes:\n",
    "            - BERTScore evaluator with microsoft/deberta-xlarge-mnli model\n",
    "            - METEOR evaluator with default configuration\n",
    "            - Results dictionary with nested structure for all metrics\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If required packages (pandas) are not installed\n",
    "\n",
    "        Note:\n",
    "            First initialization may take time to download evaluation models.\n",
    "            Internet connection required for downloading pre-trained models.\n",
    "        \"\"\"\n",
    "        # Load HuggingFace evaluation metrics\n",
    "        self.bertscore = load(\"bertscore\")  # Semantic similarity evaluation\n",
    "        self.meteor = load('meteor')        # Linguistic quality evaluation\n",
    "\n",
    "        # Initialize results storage with hierarchical structure\n",
    "        self.metrics_result = {\n",
    "            \"bertscore\": {              # Semantic preservation metrics\n",
    "                \"precision\": 0.0,       # BERTScore precision\n",
    "                \"recall\": 0.0,          # BERTScore recall\n",
    "                # BERTScore F1 (primary semantic metric)\n",
    "                \"f1\": 0.0\n",
    "            },\n",
    "            \"meteor\": 0.0,              # Linguistic quality score\n",
    "            \"gleu\": 0.0,                # Primary grammar correction metric\n",
    "            \"stats\": {}                 # Comprehensive text statistics\n",
    "        }\n",
    "\n",
    "    def compute_gleu(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute GLEU (Generalized Language Evaluation Understanding) scores for grammar correction.\n",
    "\n",
    "        GLEU is specifically designed for grammar correction evaluation and handles multiple\n",
    "        reference corrections better than traditional BLEU. It measures fluency improvement\n",
    "        while accounting for acceptable variation in correction approaches.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner list\n",
    "                contains multiple valid corrections for the same source sentence (e.g., JFLEG\n",
    "                provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average GLEU score in self.metrics_result[\"gleu\"].\n",
    "\n",
    "        Notes:\n",
    "            - Uses NLTK's sentence_gleu function for computation\n",
    "            - Applies lowercase normalization and word tokenization\n",
    "            - Handles empty or invalid references gracefully with exception handling\n",
    "            - Scores range from 0.0 (no match) to 1.0 (perfect match)\n",
    "            - For grammar correction: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "\n",
    "        Raises:\n",
    "            Exception: Catches and handles any GLEU computation errors by assigning 0.0 score.\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for gleu score\n",
    "        gleu_scores = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # converting predictions to tokens\n",
    "            preds_tokens = nltk.word_tokenize(preds.lower())\n",
    "            # converting all reference to tokens\n",
    "            refs_tokens = [nltk.word_tokenize(ref.lower())\n",
    "                           for ref in refs if ref.strip()]\n",
    "\n",
    "            try:\n",
    "                # computing the score for the gleu\n",
    "                score = sentence_gleu(refs_tokens, preds_tokens)\n",
    "                # updating the storage\n",
    "                gleu_scores.append(score)\n",
    "            except Exception:\n",
    "                gleu_scores.append(0.0)\n",
    "\n",
    "        # computing the average gleu\n",
    "        self.metrics_result[\"gleu\"] = np.mean(gleu_scores)\n",
    "\n",
    "    def compute_bertscore(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute BERTScore metrics (precision, recall, F1) for grammar correction evaluation.\n",
    "\n",
    "        BERTScore measures semantic similarity using contextual embeddings, making it ideal\n",
    "        for evaluating whether grammar corrections preserve semantic meaning while improving\n",
    "        fluency. For each prediction, scores are computed against all available references\n",
    "        and then averaged.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Currently stores results in local variables but doesn't persist them.\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute BERTScore against each of its references\n",
    "            2. Average precision, recall, F1 across references for that prediction\n",
    "            3. Collect averaged scores across all predictions\n",
    "\n",
    "        Notes:\n",
    "            - Uses microsoft/deberta-xlarge-mnli for optimal semantic similarity detection\n",
    "            - Skips empty references automatically\n",
    "            - For grammar correction: F1 > 0.85 indicates excellent semantic preservation\n",
    "            - Each BERTScore call processes one prediction-reference pair\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for bertscore metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing bertscore\n",
    "                    score = self.bertscore.compute(predictions=[preds],\n",
    "                                                   references=[ref],\n",
    "                                                   lang=\"en\",\n",
    "                                                   model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "                    # updating the local storage\n",
    "                    precision.append(score[\"precision\"][0])\n",
    "                    recall.append(score[\"recall\"][0])\n",
    "                    f1.append(score[\"f1\"][0])\n",
    "\n",
    "            # updating bertscore mertics with average\n",
    "            precisions.append(np.mean(precision))\n",
    "            recalls.append(np.mean(recall))\n",
    "            f1s.append(np.mean(f1))\n",
    "\n",
    "        # computing the average bertscore\n",
    "        self.metrics_result[\"bertscore\"] = {\n",
    "            \"precision\": np.mean(precisions),\n",
    "            \"recall\": np.mean(recalls),\n",
    "            \"f1\": np.mean(f1s)\n",
    "        }\n",
    "\n",
    "    def compute_meteor(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute METEOR (Metric for Evaluation of Translation with Explicit ORdering) scores \n",
    "        for grammar correction evaluation.\n",
    "\n",
    "        METEOR is particularly valuable for grammar correction as it incorporates:\n",
    "        - Exact word matching\n",
    "        - Stem matching (handles morphological variations like \"running\" vs \"runs\")\n",
    "        - Synonym matching (recognizes semantically equivalent words)\n",
    "        - Word order penalties\n",
    "\n",
    "        This makes it superior to BLEU for grammar correction where morphological changes\n",
    "        and lexical substitutions are common.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average METEOR score in self.metrics_result[\"meteor\"].\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute METEOR against each of its references\n",
    "            2. Average METEOR scores across references for that prediction  \n",
    "            3. Average across all predictions for final score\n",
    "\n",
    "        Notes:\n",
    "            - METEOR scores range from 0.0 to 1.0 (higher is better)\n",
    "            - For grammar correction: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "            - Handles morphological variations better than BLEU\n",
    "            - Includes recall-oriented evaluation (unlike BLEU's precision focus)\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for meteor metrics\n",
    "        meteors = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            meteor = []\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing meteor\n",
    "                    meteor.append(self.meteor.compute(predictions=[preds],\n",
    "                                                      references=[ref])[\"meteor\"])\n",
    "\n",
    "            # updating meteor mertics with avergae\n",
    "            meteors.append(np.mean(meteor))\n",
    "\n",
    "        # computing the average meteor\n",
    "        self.metrics_result[\"meteor\"] = np.mean(meteors)\n",
    "\n",
    "    def compute_stats(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute comprehensive text statistics for grammar correction evaluation.\n",
    "\n",
    "        This method analyzes various aspects of model predictions versus reference corrections,\n",
    "        providing detailed insights into model behavior patterns, text properties, and \n",
    "        vocabulary usage. All statistics are stored in self.metrics_result[\"stats\"].\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: All statistics are stored in self.metrics_result[\"stats\"] dictionary.\n",
    "\n",
    "        Statistics Computed:\n",
    "\n",
    "            **Sample Information:**\n",
    "            - num_samples: Total number of predictions evaluated\n",
    "            - total_references: Total number of reference corrections across all sentences\n",
    "            - avg_references_per_sentence: Average references available per sentence\n",
    "\n",
    "            **Length Statistics (Word-level):**\n",
    "            - avg/min/max/std_prediction_length: Prediction length statistics in words\n",
    "            - avg/min/max/std_reference_length: Reference length statistics in words\n",
    "\n",
    "            **Character-level Statistics:**\n",
    "            - avg/std_prediction_char_length: Character count statistics for predictions\n",
    "            - avg/std_reference_char_length: Character count statistics for references\n",
    "\n",
    "            **Length Change Analysis:**\n",
    "            - avg/std_length_difference: Difference between prediction and first reference lengths\n",
    "            - positive_length_changes: Count where prediction > reference length (expansion)\n",
    "            - negative_length_changes: Count where prediction < reference length (compression)  \n",
    "            - no_length_changes: Count where prediction == reference length (preserved)\n",
    "\n",
    "            **Vocabulary Analysis:**\n",
    "            - unique_words_in_predictions: Unique word count in all predictions\n",
    "            - unique_words_in_references: Unique word count in all references\n",
    "            - vocab_overlap: Common words between predictions and references\n",
    "            - vocab_overlap_ratio: Overlap ratio (intersection/union of vocabularies)\n",
    "\n",
    "        Notes:\n",
    "            - Length differences computed against first reference for each sentence\n",
    "            - Word counting uses lowercase normalization\n",
    "            - Empty references are skipped in processing\n",
    "            - Vocabulary analysis helps assess model's lexical diversity\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for computing statistics\n",
    "        pred_lengths = []                   # predictions length\n",
    "        pred_char_lengths = []              # predictions char length\n",
    "        all_ref_lengths = []                # reference length\n",
    "        all_ref_char_lengths = []           # reference char length\n",
    "        ref_counts = []                     # np. of reference per sentence\n",
    "        length_diffs = []                   # word difference in preds and refs\n",
    "        pred_word_counts = Counter()        # unique word counts in prediction\n",
    "        ref_word_counts = Counter()         # unique word counts in reference\n",
    "        # no. of len(prediction) > len(reference)\n",
    "        positive_changes = 0\n",
    "        # no. of len(prediction) < len(reference)\n",
    "        negative_changes = 0\n",
    "        # no. of len(prediction) == len(reference)\n",
    "        no_changes = 0\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for pred, refs in zip(predictions, references):\n",
    "            # prediction statistics\n",
    "            pred_len = len(pred.split())\n",
    "            pred_char_len = len(pred)\n",
    "            pred_lengths.append(pred_len)\n",
    "            pred_char_lengths.append(pred_char_len)\n",
    "\n",
    "            # prediction word counts\n",
    "            pred_word_counts.update(pred.lower().split())\n",
    "\n",
    "            # reference statistics\n",
    "            ref_counts.append(len(refs))\n",
    "\n",
    "            # looping for all references for this prediction\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    ref_len = len(ref.split())\n",
    "                    ref_char_len = len(ref)\n",
    "                    all_ref_lengths.append(ref_len)\n",
    "                    all_ref_char_lengths.append(ref_char_len)\n",
    "                    ref_word_counts.update(ref.lower().split())\n",
    "\n",
    "            # length difference analysis (compare with first reference)\n",
    "            if refs:\n",
    "                ref_len = len(refs[0].split()) if refs[0].strip() else 0\n",
    "                length_diff = pred_len - ref_len\n",
    "                length_diffs.append(length_diff)\n",
    "\n",
    "                # counting changes\n",
    "                if length_diff > 0:\n",
    "                    positive_changes += 1\n",
    "                elif length_diff < 0:\n",
    "                    negative_changes += 1\n",
    "                else:\n",
    "                    no_changes += 1\n",
    "\n",
    "        # updating the stats in the metrics_result...\n",
    "\n",
    "        # Sample information (standardized naming)\n",
    "        self.metrics_result[\"stats\"][\"num_samples\"] = len(predictions)\n",
    "        self.metrics_result[\"stats\"][\"total_references\"] = sum(ref_counts)\n",
    "        self.metrics_result[\"stats\"][\"avg_references_per_sentence\"] = np.mean(\n",
    "            ref_counts)\n",
    "\n",
    "        # Prediction statistics\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_length\"] = np.mean(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"min_prediction_length\"] = np.min(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"max_prediction_length\"] = np.max(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_length\"] = np.std(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_char_length\"] = np.mean(\n",
    "            pred_char_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_char_length\"] = np.std(\n",
    "            pred_char_lengths)\n",
    "\n",
    "        # Reference statistics\n",
    "        if all_ref_lengths:  # Handle empty case\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = np.mean(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = np.min(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = np.max(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = np.std(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = np.mean(\n",
    "                all_ref_char_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = np.std(\n",
    "                all_ref_char_lengths)\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = 0.0\n",
    "\n",
    "        # Length difference statistics\n",
    "        if length_diffs:\n",
    "            self.metrics_result[\"stats\"][\"avg_length_difference\"] = np.mean(\n",
    "                length_diffs)\n",
    "            self.metrics_result[\"stats\"][\"std_length_difference\"] = np.std(\n",
    "                length_diffs)\n",
    "            # FIXED: Use pre-calculated variables instead of redundant sum() operations\n",
    "            self.metrics_result[\"stats\"][\"positive_length_changes\"] = positive_changes\n",
    "            self.metrics_result[\"stats\"][\"negative_length_changes\"] = negative_changes\n",
    "            self.metrics_result[\"stats\"][\"no_length_changes\"] = no_changes\n",
    "\n",
    "        # Vocabulary statistics\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_predictions\"] = len(\n",
    "            pred_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_references\"] = len(\n",
    "            ref_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"vocab_overlap\"] = len(\n",
    "            set(pred_word_counts.keys()) & set(ref_word_counts.keys()))\n",
    "\n",
    "        # Vocabulary overlap ratio\n",
    "        if len(pred_word_counts) > 0 and len(ref_word_counts) > 0:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = (\n",
    "                self.metrics_result[\"stats\"][\"vocab_overlap\"] /\n",
    "                len(set(pred_word_counts.keys()) | set(ref_word_counts.keys()))\n",
    "            )\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = 0.0\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Perform comprehensive evaluation of grammar correction predictions.\n",
    "\n",
    "        This is the main evaluation method that computes all metrics and statistics\n",
    "        for grammar correction assessment. It provides a complete analysis including\n",
    "        fluency (GLEU), semantic preservation (BERTScore), linguistic quality (METEOR),\n",
    "        and comprehensive text statistics.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete evaluation results containing:\n",
    "                - \"gleu\": GLEU score (float)\n",
    "                - \"meteor\": METEOR score (float) \n",
    "                - \"bertscore\": Dict with precision, recall, f1 scores\n",
    "                - \"stats\": Dict with comprehensive text statistics\n",
    "\n",
    "        Evaluation Metrics Computed:\n",
    "\n",
    "            **Core Grammar Correction Metrics:**\n",
    "            - GLEU: Primary metric for grammar correction fluency assessment\n",
    "            - BERTScore: Semantic preservation evaluation (precision, recall, F1)\n",
    "            - METEOR: Linguistic quality with morphological awareness\n",
    "\n",
    "            **Comprehensive Statistics:**\n",
    "            - Sample counts and reference information\n",
    "            - Length statistics (words and characters)\n",
    "            - Length change analysis\n",
    "            - Vocabulary analysis and overlap metrics\n",
    "\n",
    "        Performance Benchmarks:\n",
    "            - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "            - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent  \n",
    "            - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "        Notes:\n",
    "            - Progress information is printed during computation\n",
    "            - Results are stored in self.metrics_result and returned\n",
    "            - Statistics are displayed as formatted pandas DataFrame\n",
    "            - All computations handle edge cases gracefully\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Evaluating {len(predictions)} Predictions...\")\n",
    "        # gleu score -- primary for grammar correction\n",
    "        print(\"\\t[INFO] Computing GLEU Score...\")\n",
    "        self.compute_gleu(predictions, references)\n",
    "\n",
    "        # bertscore -- sementic preservation\n",
    "        print(\"\\t[INFO] Computing BERTScore--Precision Recall & F1...\")\n",
    "        self.compute_bertscore(predictions, references)\n",
    "\n",
    "        # meteor -- linquistic quality\n",
    "        print(\"\\t[INFO] Computing METEOR Score...\")\n",
    "        self.compute_meteor(predictions, references)\n",
    "\n",
    "        # statistics\n",
    "        print(\"\\t[INFO] Computing Comprehensive Statistics...\")\n",
    "        self.compute_stats(predictions, references)\n",
    "\n",
    "        print(\"Evaluation Complete:\")\n",
    "        # printing the metrics...\n",
    "        print(f\"\\t[INFO] GLEU: {self.metrics_result['gleu']:.4f}\")\n",
    "        print(f\"\\t[INFO] METEOR: {self.metrics_result['meteor']:.4f}\")\n",
    "        print(\"\\t[INFO] BERTSCORE:\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Precision: {self.metrics_result['bertscore']['precision']:.4f}\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Recall: {self.metrics_result['bertscore']['recall']:.4f}\")\n",
    "        print(f\"\\t\\t[INFO] F1: {self.metrics_result['bertscore']['f1']:.4f}\")\n",
    "\n",
    "        # printing the statistics...\n",
    "        print(\"\\n\\t[INFO] Statistics:\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            stats_df = pd.DataFrame(\n",
    "                list(self.metrics_result[\"stats\"].items()),\n",
    "                columns=[\"Metric\", \"Value\"]\n",
    "            )\n",
    "            print(stats_df.to_string(index=False))\n",
    "        except ImportError:\n",
    "            print(\"\\t[WARNING] Pandas not available, printing raw statistics:\")\n",
    "            for key, value in self.metrics_result[\"stats\"].items():\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "        # Return complete results for further processing\n",
    "        return self.metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fcf2e9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "meteor = load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca80f83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [1.0],\n",
       " 'recall': [1.0],\n",
       " 'f1': [1.0],\n",
       " 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"hello there\"]\n",
    "references = [[\"hello there\", \"hi there\"]]\n",
    "bertscore.compute(predictions=predictions,\n",
    "                  references=[references[0][0]],\n",
    "                  lang=\"en\",\n",
    "                  model_type=\"microsoft/deberta-xlarge-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21424b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': np.float64(0.9375)}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor.compute(predictions=predictions, references=[\n",
    "               references[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc133ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}\n",
      "{'precision': [0.944251537322998], 'recall': [0.944251537322998], 'f1': [0.944251537322998], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}\n",
      "0.972125768661499 0.972125768661499 0.972125768661499\n"
     ]
    }
   ],
   "source": [
    "for preds, refs in zip(predictions, references):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            score = (bertscore.compute(predictions=[preds],\n",
    "                                       references=[ref],\n",
    "                                       lang=\"en\",\n",
    "                                       model_type=\"microsoft/deberta-xlarge-mnli\"))\n",
    "            print(score)\n",
    "            precision.append(score[\"precision\"][0])\n",
    "            recall.append(score[\"recall\"][0])\n",
    "            f1.append(score[\"f1\"][0])\n",
    "    print(np.mean(precision), np.mean(recall), np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "322a56fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello there', ['hello there', 'hi there'])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e89ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'there']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f13ae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference statistics\n",
    "all_ref_lengths = []\n",
    "ref_counts = []\n",
    "\n",
    "for refs in val[\"processed_corrections\"]:\n",
    "    ref_counts.append(len(refs))\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            all_ref_lengths.append(len(ref.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b0a0526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import EarlyStoppingCallback\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8379c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5LoRATrainer:\n",
    "    \"\"\"\n",
    "    Parameter-efficient fine-tuning trainer for T5 models using LoRA for grammar correction.\n",
    "\n",
    "    Implements complete training pipeline with research-proven hyperparameters, comprehensive \n",
    "    evaluation metrics (GLEU, BERTScore, METEOR), and multi-format logging for academic projects.\n",
    "\n",
    "    Key Features:\n",
    "        - 99%+ parameter reduction with LoRA (Low-Rank Adaptation)\n",
    "        - Research-validated hyperparameters (no search required)\n",
    "        - Comprehensive evaluation framework with graceful fallback\n",
    "        - Multi-format outputs (JSON, CSV, TXT) for analysis and reporting\n",
    "        - Cross-platform compatibility and robust error handling\n",
    "\n",
    "    Performance: 50-60+ GLEU, 0.80-0.90 BERTScore F1, 15-25 min training (T5-base, GPU)\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initialize trainer with model, tokenizer, and LoRA configuration\n",
    "        _setup_lora: Apply LoRA to model and create data collator\n",
    "        clean_prediction: Remove task prefixes from model outputs\n",
    "        compute_metrics: Calculate grammar correction evaluation metrics\n",
    "        save_training_log: Generate comprehensive training documentation\n",
    "        trainer: Execute complete training pipeline with optimal hyperparameters\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "        model (PeftModel): T5 model wrapped with LoRA adapters\n",
    "        output_dir (str): Directory for saving training artifacts\n",
    "        lora_rank (int): LoRA adaptation rank (default: 8)\n",
    "        lora_alpha (int): LoRA scaling parameter (default: 16)\n",
    "        lora_dropout (float): LoRA dropout rate (default: 0.1)\n",
    "        device (str): Training device (\"cuda\" or \"cpu\")\n",
    "        model_name (str): Original model identifier\n",
    "        grammar_evaluator (GrammarEvaluation): Evaluation framework instance\n",
    "        lora_config (LoraConfig): LoRA configuration object\n",
    "        data_collator (DataCollatorForSeq2Seq): Dynamic padding collator\n",
    "\n",
    "    References:\n",
    "        - LoRA Paper: https://arxiv.org/abs/2106.09685\n",
    "        - T5 Paper: https://arxiv.org/abs/1910.10683\n",
    "        - JFLEG Dataset: https://arxiv.org/abs/1702.04066\n",
    "        - BERTScore: https://arxiv.org/abs/1904.09675\n",
    "        - HuggingFace PEFT: https://huggingface.co/docs/peft/\n",
    "\n",
    "    Example:\n",
    "        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-base\") \n",
    "        >>> trainer = T5LoRATrainer(tokenizer, model)\n",
    "        >>> trainer, result = trainer.trainer(train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 model,\n",
    "                 output_dir=r\"D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\",\n",
    "                 lora_rank=8,\n",
    "                 lora_alpha=16,\n",
    "                 lora_dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the T5 LoRA Trainer with pre-loaded model and tokenizer.\n",
    "\n",
    "        This constructor sets up the training environment for parameter-efficient fine-tuning\n",
    "        using Low-Rank Adaptation (LoRA) on T5 models for grammar correction tasks.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: Pre-initialized T5 tokenizer for text processing and encoding.\n",
    "                Should be loaded using T5Tokenizer.from_pretrained().\n",
    "            model: Pre-initialized T5 model for conditional generation.\n",
    "                Should be loaded using T5ForConditionalGeneration.from_pretrained().\n",
    "            output_dir (str, optional): Directory path for saving model checkpoints, logs,\n",
    "                and final trained model. Defaults to Windows assignment directory.\n",
    "                Will be created if it doesn't exist.\n",
    "            lora_rank (int, optional): Rank of LoRA adaptation matrices. Lower values use\n",
    "                less memory but may reduce model capacity. Defaults to 8.\n",
    "            lora_alpha (int, optional): LoRA scaling parameter. Typically 2x the rank.\n",
    "                Controls the magnitude of LoRA updates. Defaults to 16.\n",
    "            lora_dropout (float, optional): Dropout rate for LoRA layers to prevent\n",
    "                overfitting. Should be between 0.0-0.3. Defaults to 0.1.\n",
    "\n",
    "        Attributes:\n",
    "            tokenizer: T5 tokenizer for text processing\n",
    "            model: T5 model that will have LoRA applied\n",
    "            output_dir: Path where training artifacts will be saved\n",
    "            lora_rank: LoRA adaptation rank\n",
    "            lora_alpha: LoRA scaling factor\n",
    "            lora_dropout: LoRA dropout rate\n",
    "            device: Automatically detected device (\"cuda\" or \"cpu\")\n",
    "            grammar_evaluator: Comprehensive evaluation class for grammar correction metrics\n",
    "\n",
    "        Notes:\n",
    "            - LoRA configuration is stored but LoRA setup happens in _setup_lora()\n",
    "            - Device detection prioritizes CUDA if available for faster training\n",
    "            - Default output directory is Windows-specific; adjust for other OS\n",
    "            - Model name is automatically extracted from model.config.name_or_path\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If model doesn't have config.name_or_path attribute\n",
    "            OSError: If output_dir path is invalid or lacks write permissions\n",
    "        \"\"\"\n",
    "        # store tokenizer and model references\n",
    "        self.tokenizer = tokenizer              # T5 tokenizer for text processing\n",
    "        self.model = model                      # T5 model for grammar correction\n",
    "\n",
    "        # training configuration\n",
    "        self.output_dir = output_dir            # directory for saving training artifacts\n",
    "\n",
    "        # LoRA hyperparameters for parameter-efficient fine-tuning\n",
    "        self.lora_rank = lora_rank              # rank of LoRA adaptation matrices\n",
    "        # LoRA scaling parameter (typically 2x rank)\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout        # dropout rate for LoRA layers\n",
    "\n",
    "        # automatic device detection for optimal performance\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # name of the model\n",
    "        self.model_name = self.model.config.name_or_path\n",
    "        # initializing grammer evaluation class\n",
    "        self.grammar_evaluator = GrammarEvaluation()\n",
    "        # print initialization information for user feedback\n",
    "        print(f\"[INFO] Initializing T5 LoRA Trainer\")\n",
    "        # extract model name automatically\n",
    "        print(f\"\\t[INFO] Model: {self.model_name}\")\n",
    "        print(f\"\\t[INFO] Device: {self.device}\")\n",
    "        print(\n",
    "            f\"\\t[INFO] LoRA Config: rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout}\")\n",
    "        print(f\"\\t[INFO] Output Directory: {output_dir}\")\n",
    "\n",
    "        # Note: LoRA setup and model modification happens in _setup_lora() method\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self._setup_lora()\n",
    "        print(\"\\nT5 LoRA Trainer initialized successfully!\")\n",
    "\n",
    "    def _setup_lora(self):\n",
    "        \"\"\"\n",
    "        Apply LoRA configuration to T5 model and create data collator.\n",
    "\n",
    "        Configures and applies LoRA adapters to query and value attention matrices,\n",
    "        enabling parameter-efficient fine-tuning with ~99% parameter reduction.\n",
    "\n",
    "        Attributes Modified:\n",
    "            - model: Wrapped with LoRA adapters (becomes PeftModel)\n",
    "            - lora_config: LoRA configuration object created and stored\n",
    "            - data_collator: DataCollatorForSeq2Seq for dynamic padding\n",
    "\n",
    "        Side Effects:\n",
    "            - Model moved to self.device\n",
    "            - Prints trainable parameter statistics\n",
    "            - Prints setup confirmation\n",
    "\n",
    "        LoRA Config:\n",
    "            - task_type: SEQ_2_SEQ_LM\n",
    "            - target_modules: [\"q\", \"v\"] (query and value attention)\n",
    "            - r: self.lora_rank, alpha: self.lora_alpha, dropout: self.lora_dropout\n",
    "\n",
    "        References:\n",
    "            Hu et al. (2021) \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "            HuggingFace PEFT Documentation: https://huggingface.co/docs/peft/\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"[INFO] Apply LoRA to T5...\")\n",
    "\n",
    "        # setup LoRA configuration using instance parameters\n",
    "        self.lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,    # sequence-to-sequence language modeling\n",
    "            inference_mode=False,               # enable training mode\n",
    "            r=self.lora_rank,                   # rank of LoRA adaptation matrices\n",
    "            lora_alpha=self.lora_alpha,         # scaling factor for LoRA updates\n",
    "            lora_dropout=self.lora_dropout,     # dropout rate for regularization\n",
    "            # Query and Value attention matrices\n",
    "            target_modules=[\"q\", \"v\"],\n",
    "            bias=\"none\"                         # don't adapt bias parameters\n",
    "        )\n",
    "\n",
    "        # apply LoRA adapters to the model (wraps model with PEFT)\n",
    "        self.model = get_peft_model(self.model, self.lora_config)\n",
    "        # move enhanced model to appropriate device for training\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # create data collator once for reuse (dynamic padding)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,           # use LoRA-enhanced model\n",
    "            padding=True,               # enable dynamic padding\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # print trainable parameters statistics for verification\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "        print(\n",
    "            f\"\\t[INFO] LoRA applied successfully to {len(self.lora_config.target_modules)} target modules\")\n",
    "        print(f\"\\t[INFO] Model moved to device: {self.device}\")\n",
    "        print(f\"\\t[INFO] Data collator configured for dynamic padding\")\n",
    "\n",
    "    def clean_prediction(self, prediction):\n",
    "        \"\"\"\n",
    "        Clean prediction text by removing task prefixes and extra whitespace.\n",
    "\n",
    "        This method removes task-specific prefixes (like \"grammar:\") that the model\n",
    "        might include in its predictions, ensuring clean text for evaluation.\n",
    "        Can be used both during training evaluation and external testing.\n",
    "\n",
    "        Args:\n",
    "            prediction (str): Raw model prediction text\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned prediction without task prefix\n",
    "        \"\"\"\n",
    "\n",
    "        # handle None or empty inputs\n",
    "        if not prediction:\n",
    "            return \"\"\n",
    "\n",
    "        # remove whitespace\n",
    "        cleaned = str(prediction).strip()\n",
    "\n",
    "        # define possible task prefixes to remove\n",
    "        prefixes_to_remove = [\n",
    "            \"grammar:\",\n",
    "            \"correct:\",\n",
    "            \"fix:\",\n",
    "            \"paraphrase:\",\n",
    "            \"translate:\",\n",
    "        ]\n",
    "\n",
    "        # remove any matching prefix (case-insensitive)\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if cleaned.lower().startswith(prefix.lower()):\n",
    "                cleaned = cleaned[len(prefix):].strip()\n",
    "                break  # only remove first matching prefix\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"\n",
    "        Compute comprehensive evaluation metrics for grammar correction during training.\n",
    "\n",
    "        This method processes model predictions and reference labels to calculate grammar\n",
    "        correction metrics including GLEU, BERTScore, and METEOR. It implements a robust\n",
    "        two-tier evaluation system: comprehensive metrics when possible, with automatic\n",
    "        fallback to basic metrics if the comprehensive evaluation fails.\n",
    "\n",
    "        Args:\n",
    "            eval_preds (tuple): Tuple containing (predictions, labels) from Trainer evaluation.\n",
    "                predictions (np.ndarray or tuple): Model output logits or generated sequences.\n",
    "                    If tuple, uses first element (generated sequences).\n",
    "                labels (np.ndarray): Reference labels with -100 for padded positions.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics with the following structure:\n",
    "\n",
    "            Comprehensive evaluation (when successful):\n",
    "                - gleu (float): GLEU score for grammar correction fluency\n",
    "                - meteor (float): METEOR score for linguistic quality\n",
    "                - bertscore_f1 (float): BERTScore F1 for semantic preservation\n",
    "                - bertscore_precision (float): BERTScore precision\n",
    "                - bertscore_recall (float): BERTScore recall\n",
    "                - prediction_length (float): Average prediction length in words\n",
    "                - reference_length (float): Average reference length in words\n",
    "                - vocab_overlap_ratio (float): Vocabulary overlap between preds/refs\n",
    "                - positive_length_changes (int): Count of predictions longer than references\n",
    "                - evaluation_type (str): \"comprehensive\"\n",
    "                - num_samples (int): Number of evaluated samples\n",
    "\n",
    "            Basic fallback evaluation (when comprehensive fails):\n",
    "                - prediction_length (float): Average prediction length in words\n",
    "                - reference_length (float): Average reference length in words\n",
    "                - length_difference (float): Average length difference (pred - ref)\n",
    "                - positive_length_changes (int): Count of longer predictions\n",
    "                - predictions_count (int): Total number of predictions\n",
    "                - gleu, meteor, bertscore_f1 (float): Set to 0.0 (placeholders)\n",
    "                - vocab_overlap_ratio (float): Set to 0.0 (placeholder)\n",
    "                - evaluation_type (str): \"basic_fallback\"\n",
    "                - num_samples (int): Number of evaluated samples\n",
    "\n",
    "        Process:\n",
    "            1. Handle tuple outputs, decode predictions and labels\n",
    "            2. Clean predictions (remove task prefixes)\n",
    "            3. Attempt comprehensive evaluation via self.grammar_evaluator\n",
    "            4. Fallback to basic metrics if comprehensive evaluation fails\n",
    "        \"\"\"\n",
    "\n",
    "        # getting labels and predictions\n",
    "        predictions, labels = eval_preds\n",
    "\n",
    "        # handle tuple generated output\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        # replacing -100 labels with padded tokens for decoding -- required\n",
    "        # dynamic padding creates -100 in labels, which is not valid token ID\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "\n",
    "        # decoding the predictions and labels\n",
    "        decoded_preds = self.tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True)\n",
    "        decoded_labels = self.tokenizer.batch_decode(\n",
    "            labels, skip_special_tokens=True)\n",
    "\n",
    "        # cleaning predictions (remove task prefixes) and labels\n",
    "        decoded_preds = [self.clean_prediction(pred) for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        try:\n",
    "            # calculating the comprehensive evaluation mertics\n",
    "            print(\"\\t[INFO] Computing Comprehensive Evaluation Metrics...\")\n",
    "\n",
    "            # convert to format expected by GrammarEvaluation (list of lists for references)\n",
    "            # single reference per predictio\n",
    "            references = [[label] for label in decoded_labels]\n",
    "\n",
    "            # calling the GrammarEvaluation class\n",
    "            results = self.grammar_evaluator.evaluate(predictions=decoded_preds,\n",
    "                                                      references=references)\n",
    "\n",
    "            print(\"\\t[INFO] Comprehensive Evaluation Successful\")\n",
    "\n",
    "            # return comprehensive metrics for trainer\n",
    "            return {\n",
    "                # primary grammar correction metrics\n",
    "                \"gleu\": results[\"gleu\"],\n",
    "                \"meteor\": results[\"meteor\"],\n",
    "                \"bertscore_f1\": results[\"bertscore\"][\"f1\"],\n",
    "                \"bertscore_precision\": results[\"bertscore\"][\"precision\"],\n",
    "                \"bertscore_recall\": results[\"bertscore\"][\"recall\"],\n",
    "\n",
    "                # key statistics from comprehensive analysis\n",
    "                \"prediction_length\": results[\"stats\"][\"avg_prediction_length\"],\n",
    "                \"reference_length\": results[\"stats\"][\"avg_reference_length\"],\n",
    "                \"vocab_overlap_ratio\": results[\"stats\"][\"vocab_overlap_ratio\"],\n",
    "                \"positive_length_changes\": results[\"stats\"][\"positive_length_changes\"],\n",
    "\n",
    "                # metadata\n",
    "                \"evaluation_type\": \"comprehensive\",\n",
    "                \"num_samples\": results[\"stats\"][\"num_samples\"]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # fallback to basic metrics if comprehensive evaluation fails\n",
    "            print(f\"\\t[WARNING] Comprehensive Evaluation Failed: {e}\")\n",
    "            print(\"\\t[INFO] Falling back to Basic Metrics...\")\n",
    "\n",
    "        # calculate basic metrics manually (fallback)\n",
    "        pred_lengths = [len(pred.split()) for pred in decoded_preds]\n",
    "        ref_lengths = [len(ref.split()) for ref in decoded_labels]\n",
    "\n",
    "        # basic length change analysis\n",
    "        length_diffs = [len(pred.split()) - len(ref.split())\n",
    "                        for pred, ref in zip(decoded_preds, decoded_labels)]\n",
    "        positive_changes = sum(1 for diff in length_diffs if diff > 0)\n",
    "\n",
    "        print(\"\\t[INFO] Basic metrics computed successfully\")\n",
    "\n",
    "        # return basic fallback metrics\n",
    "        return {\n",
    "            # basic monitoring metrics\n",
    "            \"prediction_length\": np.mean(pred_lengths),\n",
    "            \"reference_length\": np.mean(ref_lengths),\n",
    "            \"length_difference\": np.mean(length_diffs),\n",
    "            \"positive_length_changes\": positive_changes,\n",
    "            \"predictions_count\": len(decoded_preds),\n",
    "\n",
    "            # placeholder values for missing comprehensive metrics\n",
    "            \"gleu\": 0.0,\n",
    "            \"meteor\": 0.0,\n",
    "            \"bertscore_f1\": 0.0,\n",
    "            \"vocab_overlap_ratio\": 0.0,\n",
    "\n",
    "            # metadata\n",
    "            \"evaluation_type\": \"basic_fallback\",\n",
    "            \"num_samples\": len(decoded_preds)\n",
    "        }\n",
    "\n",
    "    def save_training_log(self, trainer, training_time_minutes):\n",
    "        \"\"\"\n",
    "        Save comprehensive training logs in multiple formats.\n",
    "\n",
    "        Args:\n",
    "            trainer (transformers.Trainer): Trained Trainer with log history\n",
    "            training_time_minutes (float): Total training time in minutes\n",
    "\n",
    "        Returns:\n",
    "            str: Path to main JSON log file\n",
    "\n",
    "        Creates Files in self.output_dir:\n",
    "            - training_log.json: Structured training data and metadata\n",
    "            - training_metrics.csv: Training progress (if pandas available)\n",
    "            - evaluation_metrics.csv: Evaluation metrics (if pandas available)\n",
    "            - training_summary.txt: Human-readable summary for reports\n",
    "\n",
    "        Log Contents:\n",
    "            - Training metadata: model, LoRA config, timing, device\n",
    "            - Training progress: complete loss/metrics history by epoch\n",
    "            - Final metrics: last recorded values for all metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # extracting history from trainer\n",
    "        history = trainer.state.log_history\n",
    "\n",
    "        # separating training and validation logs\n",
    "        train_logs = [\n",
    "            log for log in history if \"train_loss\" in log and \"epoch\" in log]\n",
    "        eval_logs = [\n",
    "            log for log in history if \"eval_loss\" in log and \"epoch\" in log]\n",
    "\n",
    "        # creating comprehensive training log\n",
    "        training_log = {\n",
    "            \"training_metadata\": {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"lora_config\": {\n",
    "                    \"rank\": self.lora_rank,\n",
    "                    \"alpha\": self.lora_alpha,\n",
    "                    \"dropout\": self.lora_dropout,\n",
    "                    \"target_modules\": [\"q\", \"v\"]\n",
    "                },\n",
    "                \"training_time_minutes\": training_time_minutes,\n",
    "                \"device\": self.device,\n",
    "                \"output_directory\": self.output_dir\n",
    "            },\n",
    "            \"training_progress\": {\n",
    "                \"train_logs\": train_logs,\n",
    "                \"eval_logs\": eval_logs,\n",
    "                \"total_epochs\": len(train_logs),\n",
    "                \"total_eval_steps\": len(eval_logs)\n",
    "            },\n",
    "            \"final_metrics\": {}\n",
    "        }\n",
    "\n",
    "        # extracting final metrics if available\n",
    "        if eval_logs:\n",
    "            final_eval = eval_logs[-1]\n",
    "            for key, value in final_eval.items():\n",
    "                if key.startswith('eval_') and isinstance(value, (int, float)):\n",
    "                    training_log[\"final_metrics\"][key] = value\n",
    "\n",
    "        if train_logs:\n",
    "            final_train = train_logs[-1]\n",
    "            training_log[\"final_metrics\"][\"final_train_loss\"] = final_train.get(\n",
    "                \"train_loss\", \"N/A\")\n",
    "\n",
    "        # saving detailed training log as JSON\n",
    "        log_file = os.path.join(self.output_dir, \"training_log.json\")\n",
    "        with open(log_file, \"w\") as f:\n",
    "            json.dump(training_log, f, indent=2)\n",
    "\n",
    "            # Save CSV format for easy analysis\n",
    "        try:\n",
    "            import pandas as pd\n",
    "\n",
    "            # Training metrics CSV\n",
    "            if train_logs:\n",
    "                train_df = pd.DataFrame(train_logs)\n",
    "                train_df.to_csv(os.path.join(self.output_dir,\n",
    "                                \"training_metrics.csv\"), index=False)\n",
    "                print(f\"\\t[INFO] Training metrics saved to: training_metrics.csv\")\n",
    "\n",
    "            # Evaluation metrics CSV\n",
    "            if eval_logs:\n",
    "                eval_df = pd.DataFrame(eval_logs)\n",
    "                eval_df.to_csv(os.path.join(self.output_dir,\n",
    "                               \"evaluation_metrics.csv\"), index=False)\n",
    "                print(\n",
    "                    f\"\\t[INFO] Evaluation metrics saved to: evaluation_metrics.csv\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(f\"\\t[WARNING] Pandas not available, skipping CSV export\")\n",
    "\n",
    "        # save simple text summary\n",
    "        summary_file = os.path.join(self.output_dir, \"training_summary.txt\")\n",
    "        with open(summary_file, \"w\") as f:\n",
    "            f.write(\"T5 LoRA Grammar Correction Training Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Model: {self.model_name}\\n\")\n",
    "            f.write(f\"Training Time: {training_time_minutes:.1f} minutes\\n\")\n",
    "            f.write(f\"Device: {self.device}\\n\")\n",
    "            f.write(f\"LoRA Rank: {self.lora_rank}\\n\")\n",
    "            f.write(f\"LoRA Alpha: {self.lora_alpha}\\n\")\n",
    "            f.write(f\"Total Epochs: {len(train_logs)}\\n\\n\")\n",
    "\n",
    "            if training_log[\"final_metrics\"]:\n",
    "                f.write(\"Final Metrics:\\n\")\n",
    "                for key, value in training_log[\"final_metrics\"].items():\n",
    "                    if isinstance(value, float):\n",
    "                        f.write(f\"  {key}: {value:.4f}\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "        print(f\"\\t[INFO] Training logs saved:\")\n",
    "        print(f\"\\t\\t- Detailed log: training_log.json\")\n",
    "        print(f\"\\t\\t- Text summary: training_summary.txt\")\n",
    "        print(f\"\\t\\t- Output directory: {self.output_dir}\")\n",
    "\n",
    "    def trainer(self,\n",
    "                train_dataset,\n",
    "                val_dataset,\n",
    "                learning_rate=1e-4,\n",
    "                batch_size=4,\n",
    "                num_epochs=3,\n",
    "                warmup_ratio=0.1,\n",
    "                weight_decay=0.01):\n",
    "        \"\"\"\n",
    "        Execute complete T5 LoRA training pipeline with research-proven hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            train_dataset (Dataset): Tokenized training data with input_ids, attention_mask, labels\n",
    "            val_dataset (Dataset): Tokenized validation data in same format\n",
    "            learning_rate (float, optional): AdamW learning rate. Defaults to 1e-4.\n",
    "            batch_size (int, optional): Per-device batch size. Defaults to 4.\n",
    "            num_epochs (int, optional): Training epochs. Defaults to 3.\n",
    "            warmup_ratio (float, optional): LR warmup proportion. Defaults to 0.1.\n",
    "            weight_decay (float, optional): L2 regularization. Defaults to 0.01.\n",
    "\n",
    "        Returns:\n",
    "            transformers.Trainer: Trained Trainer object with fine-tuned LoRA model\n",
    "\n",
    "        Training Configuration:\n",
    "            - Evaluation strategy: Every epoch\n",
    "            - Early stopping: 2 epochs patience on eval_gleu\n",
    "            - Mixed precision: fp16 if CUDA available\n",
    "            - Gradient accumulation: 2 steps (effective batch size = batch_size * 2)\n",
    "            - Best model selection: Highest eval_gleu score\n",
    "\n",
    "        Created Files:\n",
    "            - Model files: LoRA adapters + tokenizer in self.output_dir\n",
    "            - Training logs: JSON, CSV, TXT formats via save_training_log()\n",
    "            - Hyperparameters: hyperparameters_and_results.json\n",
    "\n",
    "        Hyperparameter Sources:\n",
    "            - Learning rate: LoRA paper (Hu et al., 2021)\n",
    "            - Batch size: T5 paper + memory constraints\n",
    "            - Epochs: LoRA convergence studies\n",
    "            - Warmup: Transformer fine-tuning best practices\n",
    "\n",
    "        References:\n",
    "            - Hu et al. (2021): \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "            - Raffel et al. (2019): \"T5: Text-to-Text Transfer Transformer\"\n",
    "            - Napoles et al. (2017): \"JFLEG: A Fluency Corpus for Grammar Correction\"\n",
    "            - Zhang et al. (2020): \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"[INFO] Starting Training with Hyperparameter:\")\n",
    "        print(f\"\\t[INFO] Learning Rate: {learning_rate} (LoRA optimal range)\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Batch Size: {batch_size} (effective: {batch_size * 2} with grad accum)\")\n",
    "        print(f\"\\t[INFO] Epochs: {num_epochs} (LoRA converges fast)\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Warmup Ratio: {warmup_ratio} (standard transformer practice)\")\n",
    "        print(f\"\\t[INFO] Weight Decay: {weight_decay} (light regularization)\")\n",
    "\n",
    "        # Create training arguments with research-proven hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=os.path.join(self.output_dir, \"logs\"),\n",
    "            logging_steps=50,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_gleu\",\n",
    "            greater_is_better=True,\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            gradient_accumulation_steps=2,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=True,\n",
    "            label_names=[\"labels\"],  # explicit for PEFT models\n",
    "\n",
    "            # best hyperparameters\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        # create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        # start training\n",
    "        start_time = time.time()\n",
    "        print(f\"[INFO] Starting Training...\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Dataset size: {len(train_dataset):,} training examples\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Dataset size: {len(val_dataset):,} validation examples\")\n",
    "\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # calculate actual training time\n",
    "        training_time = (time.time() - start_time) / 60\n",
    "\n",
    "        # save model and tokenizer\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "        # save comprehensive training logs\n",
    "        self.save_training_log(trainer, training_time)\n",
    "\n",
    "        # print comprehensive results\n",
    "        print(f\"[INFO] Training Completed Successfully!\")\n",
    "        print(f\"\\t[INFO] Actual training time: {training_time:.1f} minutes\")\n",
    "        print(\n",
    "            f\"\\t[INFO] Final training loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "\n",
    "        # print evaluation metrics if available\n",
    "        eval_metrics = [\"eval_gleu\", \"eval_meteor\", \"eval_bertscore_f1\"]\n",
    "        for metric in eval_metrics:\n",
    "            if metric in train_result.metrics:\n",
    "                print(\n",
    "                    f\"\\t[INFO] Final {metric}: {train_result.metrics[metric]:.4f}\")\n",
    "\n",
    "        print(f\"\\t[INFO] Model saved to: {self.output_dir}\")\n",
    "        print(f\"\\t[INFO] Tokenizer saved to: {self.output_dir}\")\n",
    "        print(f\"\\t[INFO] Training logs saved to: {self.output_dir}\")\n",
    "\n",
    "        # Save training summary for reference\n",
    "        training_summary = {\n",
    "            \"hyperparameters\": {\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"warmup_ratio\": warmup_ratio,\n",
    "                \"weight_decay\": weight_decay\n",
    "            },\n",
    "            \"training_results\": train_result.metrics,\n",
    "            \"training_time_minutes\": training_time,\n",
    "            \"dataset_size\": len(train_dataset),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"lora_config\": {\n",
    "                \"rank\": self.lora_rank,\n",
    "                \"alpha\": self.lora_alpha,\n",
    "                \"dropout\": self.lora_dropout\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.output_dir, \"hyperparameters_and_results.json\"), \"w\") as f:\n",
    "            json.dump(training_summary, f, indent=2)\n",
    "\n",
    "        print(\n",
    "            f\"\\t[INFO] Hyperparameters and results saved to: hyperparameters_and_results.json\")\n",
    "\n",
    "        return trainer, train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ead710ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing T5 LoRA Trainer\n",
      "\t[INFO] Model: t5-small\n",
      "\t[INFO] Device: cpu\n",
      "\t[INFO] LoRA Config: rank=8, alpha=16, dropout=0.1\n",
      "\t[INFO] Output Directory: D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\n",
      "[INFO] Apply LoRA to T5...\n",
      "trainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850\n",
      "\t[INFO] LoRA applied successfully to 2 target modules\n",
      "\t[INFO] Model moved to device: cpu\n",
      "\t[INFO] Data collator configured for dynamic padding\n",
      "\n",
      "T5 LoRA Trainer initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "t = T5LoRATrainer(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d3bd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.trainer(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0448fbac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5LoRATrainer' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m.log_history()\n",
      "\u001b[31mAttributeError\u001b[39m: 'T5LoRATrainer' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "t.state.log_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db909c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48b89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
