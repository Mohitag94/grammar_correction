{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8e902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df199749",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a5702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:start\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362898c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b908ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('jfleg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbfda4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd85c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       " 'So I think we could not live if older people did not develop science and technologies . ',\n",
       " 'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       " 'So I think we can not live if old people can not find the science and technology that has not been developed . ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"corrections\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef064c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9bc59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1e3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "# MODELNAME = \"vennify/t5-base-grammar-correction\"\n",
    "MODELNAME = \"t5-small\"\n",
    "PREFIX = \"grammar: \"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0cdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7785a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grammar: he go to school yesterday.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532a21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"dim/grammarly_coedit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f6bf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19519,    10,     3,    88,   281,    12,   496,  4981,     5,     1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c0e8c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"validation\"][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86c61111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beccecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(T5Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8214af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5548d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarCorrectionDataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset for grammar correction tasks.\n",
    "\n",
    "    This dataset takes input text with grammatical errors and their corrected target text,\n",
    "    tokenizes them using the provided tokenizer, and prepares them for training a grammar\n",
    "    correction model.\n",
    "\n",
    "    Args:\n",
    "                                    tokenizer: The tokenizer to use for encoding the text\n",
    "                                    input_text (list): List of input texts containing grammatical errors\n",
    "                                    target_text (list): List of corresponding corrected texts\n",
    "                                    max_length (int, optional): Maximum sequence length for tokenization. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "                                    dict: Dictionary containing:\n",
    "                                                                    - input_ids: Tokenized and padded input text\n",
    "                                                                    - attention_mask: Attention mask for input text\n",
    "                                                                    - labels: Target labels for training (-100 for padding tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, input_text, target_text, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_text = input_text\n",
    "        self.target_text = target_text\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        source = str(\"grammar: \" + self.input_text[id])\n",
    "        target = str(self.target_text[id])\n",
    "\n",
    "        source_tokens = self.tokenizer(\n",
    "            source, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        target_tokens = self.tokenizer(\n",
    "            target, return_tensors=\"pt\", max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "        labels = target_tokens[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_tokens[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": source_tokens[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189708b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 755\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'corrections'],\n",
       "        num_rows: 748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7ec745",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = dataset[\"validation\"][\"sentence\"]\n",
    "train_labels = dataset[\"validation\"][\"corrections\"]\n",
    "\n",
    "val_source = dataset[\"test\"][\"sentence\"]\n",
    "val_labels = dataset[\"test\"][\"corrections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecc36f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GrammarCorrectionDataset(tokenizer, train_source, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5ba47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GrammarCorrectionDataset(tokenizer, val_source, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1a97d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-small'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a715a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f68e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da1e7d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e528926900>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28cee629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[19519,    10,    86,    82,  3474,     3,     6,    27,   317, 20356,\n",
      "            33,   143,   494,  1727,   231,   394,   145,    79,   310,    33,\n",
      "             3,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [19519,    10,  2855,    19,     8,  1873,    13,  1645,    11,     8,\n",
      "           866,    13,  7404,    16,    84,     3,     9,   568,    65,  1103,\n",
      "          1124,   112,    42,   160,  1645,    16,   182,   359,  3149,    13,\n",
      "           280,   114,   161,    11,   237,  2022,   717,     3,     5,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  784,    31,  1570,    82,  3474,     3,     6,    27,   317, 20356,\n",
      "           143,   494,  1727,   231,   394,   145,    79,   310,    33,     3,\n",
      "             5,     3,    31,     6,     3,    31,  1570,    82,  3474,     3,\n",
      "             6,    27,   317, 20356,   143,   494,  1727,   231,   394,   145,\n",
      "            79,   310,    33,     3,     5,     3,    31,     6,     3,    31,\n",
      "          1570,    82,  3474,     3,     6,    27,   317, 20356,   143,   494,\n",
      "          1727,   231,   394,   145,    79,   310,    33,     3,     5,     3,\n",
      "            31,     6,     3,    31,  1570,    82,  3474,     3,     6,    27,\n",
      "           317, 20356,    33,   492,   494,  1727,     3,     9,   418,   394,\n",
      "           145,    79,   310,    33,     3,     5,     3,    31,   908,     1,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  784,    31, 18846,    23,   106,    19,     8,  1873,    13,  1645,\n",
      "             3,     6,    11,     8,   866,    13,  7404,    16,    84,     3,\n",
      "             9,   568,    65,  1103,     3,     6,   795,    72,  1645,    16,\n",
      "            66,  3149,    13,   280,   114,   161,    11,  2022,   717,     3,\n",
      "             5,     3,    31,     6,     3,    31, 18846,    23,   106,    19,\n",
      "             8,  1873,    13,  1645,    11,     8,   866,    13,  7404,    16,\n",
      "            84,     3,     9,   568,    65,  1103,  1124,   112,    42,   160,\n",
      "          1645,    16,   182,   359,  3149,    13,   280,   114,   161,    11,\n",
      "           237,  2022,   717,     3,     5,     3,    31,     6,     3,    31,\n",
      "         18846,    23,   106,    19,     8,   843,    12,   186,  1645,     3,\n",
      "             6,    11,     8,   381,    13,  7404,    16,    84,     3,     9,\n",
      "           568,    19,   214,  1361,   122,   179,  2603,     7,   112,    42,\n",
      "           160,  1645,    16,   186,   359,  3149,    13,   280,     3,     6,\n",
      "           224,    38,   161,    11,   237,   116,    34,   639,    12,  2022,\n",
      "           717,     3,     5,     3,    31,     6,     3,    31, 18846,    23,\n",
      "           106,    19,     8,  1873,    13,  1645,     3,     6,    11,     8,\n",
      "           866,    13,  7404,    16,    84,     3,     9,   568,    65,  1103,\n",
      "          1124,   112,    42,   160,  1645,    16,   182,   359,  3149,    13,\n",
      "           280,   114,   161,    11,   237,  2022,   717,     3,     5,     3,\n",
      "            31,   908,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6dd4934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So I think we can not live if old people could not find siences and tecnologies and they did not developped . '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"sentence\"][0]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "471c4542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19519,    10,   264,    27,   317,    62,    54,    59,   619,     3,\n",
       "            99,   625,   151,   228,    59,   253,   108,  1433,     7,    11,\n",
       "             3,  5822,    29,  4137,     7,    11,    79,   410,    59,  1344,\n",
       "          3138,     3,     5,     1,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    # type: ignore\n",
    "    str(\"grammar: \" + dataset[\"validation\"][\"sentence\"][0]),\n",
    "    max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\",\n",
    "    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31decacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ArithmeticError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9984800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: he go to school yesterday.\n",
      "Corrected: grammar: he go to school yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with a task prefix\n",
    "input_text = PREFIX + \"he go to school yesterday.\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(\n",
    "    input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "\n",
    "# Generate output (corrected text)\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=128,\n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=2.5  # Penalize repetitive output\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "corrected_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Original: he go to school yesterday.\")\n",
    "print(f\"Corrected: {corrected_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d381d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_dataset = load_dataset(\"wi_locness\", \"wi\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c442023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'userid', 'cefr', 'text', 'edits'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1237a95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium size city with eighty thousand inhabitants. It has a high density population because its small territory. Despite of it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the certer of the city which is surrounded by a park. Pasteries are very common and most of them offer the special dessert from the city. There are a comercial zone along the widest street of the city where you can find all kind of establishments: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, groceries, travel agencies, supermarkets and others. Most of the shops have sales and offers at least three months of the year: January, June and August. The quality of the products and services are quite good, because there are a huge competition, however I suggest you taking care about some fakes or cheats.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cebe98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start', 'end', 'text'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be3abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = wi_dataset[\"train\"][\"edits\"][0][\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "648f154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = wi_dataset[\"train\"][\"edits\"][0][\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee3758df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium size'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"text\"][0][starts[0]:ends[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3669d289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium-sized'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset[\"train\"][\"edits\"][0][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa8ea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wi_dataset[\"train\"][\"text\"][0]\n",
    "edits = wi_dataset[\"train\"][\"edits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf127d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': [13,\n",
       "  77,\n",
       "  104,\n",
       "  126,\n",
       "  134,\n",
       "  256,\n",
       "  306,\n",
       "  375,\n",
       "  396,\n",
       "  402,\n",
       "  476,\n",
       "  484,\n",
       "  579,\n",
       "  671,\n",
       "  774,\n",
       "  804,\n",
       "  808,\n",
       "  826,\n",
       "  838,\n",
       "  850,\n",
       "  857,\n",
       "  862,\n",
       "  868],\n",
       " 'end': [24,\n",
       "  78,\n",
       "  104,\n",
       "  133,\n",
       "  136,\n",
       "  262,\n",
       "  315,\n",
       "  379,\n",
       "  399,\n",
       "  411,\n",
       "  480,\n",
       "  498,\n",
       "  588,\n",
       "  671,\n",
       "  777,\n",
       "  807,\n",
       "  810,\n",
       "  835,\n",
       "  845,\n",
       "  856,\n",
       "  861,\n",
       "  867,\n",
       "  873],\n",
       " 'text': ['medium-sized',\n",
       "  '-',\n",
       "  ' of',\n",
       "  'Although',\n",
       "  '',\n",
       "  'center',\n",
       "  None,\n",
       "  'of',\n",
       "  'is',\n",
       "  'commercial',\n",
       "  'kinds',\n",
       "  'businesses',\n",
       "  'grocers',\n",
       "  ' in',\n",
       "  'is',\n",
       "  'is',\n",
       "  '',\n",
       "  '. However,',\n",
       "  'recommend',\n",
       "  'be',\n",
       "  'careful',\n",
       "  'of',\n",
       "  '']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262bed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "of\n",
      "careful\n",
      "be\n",
      "recommend\n",
      ". However,\n",
      "\n",
      "is\n",
      "is\n",
      " in\n",
      "grocers\n",
      "businesses\n",
      "kinds\n",
      "commercial\n",
      "is\n",
      "of\n",
      "None\n",
      "center\n",
      "\n",
      "Although\n",
      " of\n",
      "-\n",
      "medium-sized\n"
     ]
    }
   ],
   "source": [
    "edits_list = list(zip(edits[\"start\"], edits[\"end\"], edits[\"text\"]))\n",
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "str = text\n",
    "for start, end, replacement in edits_list:\n",
    "    print(replacement)\n",
    "    if replacement == None:\n",
    "        replacement = \"\"\n",
    "    str = str[:start] + replacement + str[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5846c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f70c5b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(868, 873, ''),\n",
       " (862, 867, 'of'),\n",
       " (857, 861, 'careful'),\n",
       " (850, 856, 'be'),\n",
       " (838, 845, 'recommend'),\n",
       " (826, 835, '. However,'),\n",
       " (808, 810, ''),\n",
       " (804, 807, 'is'),\n",
       " (774, 777, 'is'),\n",
       " (671, 671, ' in'),\n",
       " (579, 588, 'grocers'),\n",
       " (484, 498, 'businesses'),\n",
       " (476, 480, 'kinds'),\n",
       " (402, 411, 'commercial'),\n",
       " (396, 399, 'is'),\n",
       " (375, 379, 'of'),\n",
       " (306, 315, None),\n",
       " (256, 262, 'center'),\n",
       " (134, 136, ''),\n",
       " (126, 133, 'Although'),\n",
       " (104, 104, ' of'),\n",
       " (77, 78, '-'),\n",
       " (13, 24, 'medium-sized')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "edits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "740ccab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dcba1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_ds = {}\n",
    "for sets in wi_dataset.keys():\n",
    "    for features in wi_dataset[sets]:\n",
    "        incorrect_text = \"\"\n",
    "        correct_text = \"\"\n",
    "        if \"text\" in features and \"edits\" in features:\n",
    "            incorrect_text = features[\"text\"]  # type: ignore\n",
    "            # print(incorrect_text)\n",
    "            correct_text = features[\"text\"]\n",
    "\n",
    "            correct_list = features[\"edits\"]\n",
    "\n",
    "            edits_list = list(zip(correct_list[\"start\"],\n",
    "                                  correct_list[\"end\"], correct_list[\"text\"]))\n",
    "            edits_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            for start, end, replacement in edits_list:\n",
    "                if replacement == None:\n",
    "                    replacement = \"\"\n",
    "                correct_text = correct_text[:start] + \\\n",
    "                    replacement + correct_text[end:]\n",
    "\n",
    "        if sets not in wi_ds:\n",
    "            wi_ds[sets] = {\"incorrect_text\": [], \"correct_text\": []}\n",
    "\n",
    "        if incorrect_text != correct_text:\n",
    "            wi_ds[sets][\"incorrect_text\"].append(incorrect_text)\n",
    "            wi_ds[sets][\"correct_text\"].append(correct_text)        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bc40fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My town is a medium-sized city with eighty thousand inhabitants. It has a high-density population because of its small territory. Although  it is an industrial city, there are many shops and department stores.  I recommend visiting the artificial lake in the center of the city which is surrounded by a park.  are very common and most of them offer the special dessert of the city. There is a commercial zone along the widest street of the city where you can find all kinds of businesses: banks, bars, chemists, cinemas, pet shops, restaurants, fast food restaurants, grocers, travel agencies, supermarkets and others. Most of the shops have sales and offers in at least three months of the year: January, June and August. The quality of the products and services is quite good, because there is huge competition. However, I recommend you be careful of fakes or cheats.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds[\"train\"][\"correct_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08e8ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b89967de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1551)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(wi_ds[\"train\"][\"incorrect_text\"][i].split())\n",
    "        for i in range(len(wi_ds[\"train\"][\"incorrect_text\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca3628ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi_ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d425549",
   "metadata": {},
   "outputs": [],
   "source": [
    "paws = load_dataset(\"paws\", \"labeled_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e02e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrases = paws.filter(lambda x: x['label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67857519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 21829\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3536\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 3539\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca191715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e59d20be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîç ENVIRONMENT VERIFICATION\n",
      "INFO:__main__:==================================================\n",
      "INFO:__main__:   PyTorch: 2.7.1+cpu\n",
      "INFO:__main__:   CUDA Available: False\n",
      "WARNING:__main__:   ‚ö†Ô∏è  No GPU detected - training will be slower\n",
      "INFO:__main__:‚úÖ Environment verification complete!\n",
      "INFO:__main__:==================================================\n"
     ]
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify computational environment and requirements\"\"\"\n",
    "    logger.info(\"üîç ENVIRONMENT VERIFICATION\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"   PyTorch: {torch.__version__}\")\n",
    "    logger.info(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_props = torch.cuda.get_device_properties(0)\n",
    "        logger.info(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
    "        logger.info(f\"   GPU Memory: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "        logger.info(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        logger.warning(\"   ‚ö†Ô∏è  No GPU detected - training will be slower\")\n",
    "\n",
    "    # Check available memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        logger.info(f\"   GPU Memory Used: {memory_allocated:.1f} GB\")\n",
    "        logger.info(f\"   GPU Memory Reserved: {memory_reserved:.1f} GB\")\n",
    "\n",
    "    logger.info(\"‚úÖ Environment verification complete!\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5231032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = [\"grammar: \" +\n",
    "                sentence for sentence in dataset[\"validation\"][\"sentence\"]]\n",
    "train_target = [correction[0]\n",
    "                for correction in dataset[\"validation\"][\"corrections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2990f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokens = tokenizer(train_source, max_length=256,\n",
    "                          truncation=True, padding=False)\n",
    "target_tokens = tokenizer(train_target, max_length=256,\n",
    "                          truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f3e1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = source_tokens\n",
    "token_dataset[\"labels\"] = target_tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6713dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1659b797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': ['sentence', 'corrections'],\n",
       " 'test': ['sentence', 'corrections']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c79020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sources, target, max_length=256):\n",
    "    dataset = {}\n",
    "    source = [f\"grammar: {sentence}\" for sentence in sources]\n",
    "    targets = [correction[0]\n",
    "               for correction in target]\n",
    "\n",
    "    source_tokens = tokenizer(source, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    target_tokens = tokenizer(targets, max_length=max_length,\n",
    "                              truncation=True, padding=False,\n",
    "                              return_tensors=None)\n",
    "    # print(target_tokens)\n",
    "\n",
    "    dataset[\"input_ids\"] = source_tokens[\"input_ids\"]\n",
    "    dataset[\"attention_mask\"] = source_tokens[\"attention_mask\"]\n",
    "    dataset[\"labels\"] = target_tokens[\"input_ids\"]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db9931e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de948997c71247f7b0b2af95d4f2e6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = dataset[\"validation\"].map(lambda ds: preprocess(dataset[\"validation\"][\"sentence\"],\n",
    "                                                           dataset[\"validation\"][\"corrections\"]),\n",
    "                                     batched=True,\n",
    "                                     remove_columns=dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ba38723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539f36009c25414bb0814301467ab3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_ds = dataset[\"test\"].map(lambda ds: preprocess(dataset[\"test\"][\"sentence\"],\n",
    "                                                   dataset[\"test\"][\"corrections\"]),\n",
    "                             batched=True,\n",
    "                             remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2a91e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 748\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d379b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir=r\"D:\\MScDataScience\\9.Research_Methods\\Assignment\\Assignment2\\Checkpoints\",\n",
    "    # Basic setup\n",
    "    overwrite_output_dir=False,          # Overwrite output directory if exists\n",
    "    do_train=True,                       # Whether to run training\n",
    "    do_eval=False,                       # Whether to run evaluation\n",
    "    do_predict=False,                    # Whether to run predictions\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=1.0,                # Number of training epochs\n",
    "    # Max training steps (overrides epochs if set)\n",
    "    max_steps=-1,\n",
    "    per_device_train_batch_size=8,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,        # Batch size per device during evaluation\n",
    "    gradient_accumulation_steps=1,        # Steps to accumulate gradients\n",
    "\n",
    "    # Learning rate and optimization\n",
    "    learning_rate=5e-5,                  # Initial learning rate\n",
    "    weight_decay=0.0,                    # Weight decay coefficient\n",
    "    adam_beta1=0.9,                      # Beta1 for Adam optimizer\n",
    "    adam_beta2=0.999,                    # Beta2 for Adam optimizer\n",
    "    adam_epsilon=1e-8,                   # Epsilon for Adam optimizer\n",
    "    max_grad_norm=1.0,                   # Max gradient norm for clipping\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    lr_scheduler_type=\"linear\",          # Type of LR scheduler\n",
    "    warmup_ratio=0.0,                    # Ratio of warmup steps\n",
    "    warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0869c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a9ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0930a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train/evaluate/predict\n",
    "    args=training_arg,                         # TrainingArguments instance\n",
    "    data_collator=data_collator,                # Function to collate batch data\n",
    "    train_dataset=train_ds,                # Training dataset\n",
    "    eval_dataset=val_ds,                 # Evaluation dataset\n",
    "    tokenizer=tokenizer,                    # Tokenizer for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0ec9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 02:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=95, training_loss=1.1725575497275904, metrics={'train_runtime': 146.0693, 'train_samples_per_second': 5.169, 'train_steps_per_second': 0.65, 'total_flos': 9238399647744.0, 'train_loss': 1.1725575497275904, 'epoch': 1.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7bab243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'So I think we can not live if old people could not find siences and tecnologies and they did not developped . ',\n",
       " 'corrections': ['So I think we would not be alive if our ancestors did not develop sciences and technologies . ',\n",
       "  'So I think we could not live if older people did not develop science and technologies . ',\n",
       "  'So I think we can not live if old people could not find science and technologies and they did not develop . ',\n",
       "  'So I think we can not live if old people can not find the science and technology that has not been developed . ']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea13ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12d44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies. \n",
      "So I think we could not live if older people did not develop science and technologies. \n",
      "So I think we can not live if old people could not find science and technologies and they did not develop. \n",
      "So I think we can not live if old people can not find the science and technology that has not been developed. \n"
     ]
    }
   ],
   "source": [
    "augmented_data = []\n",
    "punc = re.compile(r'\\s+([.!?,:;])')\n",
    "for items in dataset[\"validation\"]:\n",
    "    print(type(items))\n",
    "    source = punc.sub(r\"\\1\", items[\"sentence\"])\n",
    "    print(source)\n",
    "    targets = items[\"corrections\"]\n",
    "    for correction in targets:\n",
    "        print(re.sub(punc, r\"\\1\", correction))\n",
    "        if correction.strip():\n",
    "            augmented_data.append({\n",
    "                \"sentence\": f\"grammar: {source}\",\n",
    "                \"correction\": correction,\n",
    "                \"original_sentence\": source,\n",
    "                \"all_corrections\": targets\n",
    "            })\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ee74d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be3e5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_number = r'\\b(\\d+(?:\\s+\\d+)+)\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "183a5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFLEGDataset:\n",
    "    \"\"\"\n",
    "    A comprehensive dataset processor for JFLEG (JHU FLuency-Extended GUG) grammar correction data.\n",
    "\n",
    "    This class handles the complete pipeline for preparing JFLEG data for T5-based grammar correction\n",
    "    training, including data loading, preprocessing, augmentation, tokenization, and train/validation/test\n",
    "    splitting. The JFLEG dataset contains 1,511 examples with 4 human-written corrections each, focusing\n",
    "    on fluency improvements rather than minimal edits.\n",
    "\n",
    "    Key Features:\n",
    "            - Comprehensive text preprocessing to handle formatting issues\n",
    "            - Data augmentation using all 4 JFLEG corrections per sentence\n",
    "            - Proper tokenization for T5 sequence-to-sequence training\n",
    "            - Train/validation/test splitting with preserved evaluation metadata\n",
    "            - Temperature-scaled mixing support for multi-task learning\n",
    "\n",
    "    Dataset Sources:\n",
    "            - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "            - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "    Attributes:\n",
    "            tokenizer (T5Tokenizer): T5 tokenizer for text processing\n",
    "            max_length (int): Maximum sequence length for tokenization\n",
    "            test_split_ratio (float): Proportion of validation data to use for testing\n",
    "            train_data (Dataset): JFLEG validation split used for training\n",
    "            validation_data (Dataset): JFLEG test split used for validation/testing\n",
    "\n",
    "    Example:\n",
    "            >>> from transformers import T5Tokenizer\n",
    "            >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "            >>> dataset = JFLEGDataset(tokenizer, max_length=256, test_split_ratio=0.10)\n",
    "            >>> train_data, val_data, test_data = dataset.create_train_val_test_datasets()\n",
    "\n",
    "    References:\n",
    "            - JFLEG Paper: Napoles et al. (2017) \"JFLEG: A Fluency Corpus and Benchmark \n",
    "              for Grammatical Error Correction\"\n",
    "            - Dataset: https://huggingface.co/datasets/jhu-clsp/jfleg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_length=256, test_split_ratio=0.10):\n",
    "        \"\"\"\n",
    "        Initialize the JFLEG dataset processor with specified configuration.\n",
    "\n",
    "        Sets up the dataset processor with the provided tokenizer and configuration\n",
    "        parameters, then loads the raw JFLEG datasets for subsequent processing.\n",
    "\n",
    "        Args:\n",
    "                tokenizer (T5Tokenizer): HuggingFace T5 tokenizer instance for text processing.\n",
    "                        Must be a properly initialized T5 tokenizer (e.g., from t5-base).\n",
    "                max_length (int, optional): Maximum sequence length for tokenization. \n",
    "                        Sequences longer than this will be truncated. Defaults to 256.\n",
    "                        Recommended range: 128-512 depending on GPU memory constraints.\n",
    "                test_split_ratio (float, optional): Proportion of validation data to reserve \n",
    "                        for final testing. Must be between 0.0 and 1.0. Defaults to 0.10 (10%).\n",
    "                        The remaining validation data will be used for model validation during training.\n",
    "\n",
    "        Raises:\n",
    "                ValueError: If test_split_ratio is not between 0.0 and 1.0\n",
    "                TypeError: If tokenizer is not a valid T5Tokenizer instance\n",
    "\n",
    "        Note:\n",
    "                The JFLEG dataset splits are used as follows:\n",
    "                - JFLEG 'validation' split ‚Üí Training data (with augmentation)\n",
    "                - JFLEG 'test' split ‚Üí Validation and test data (split according to test_split_ratio)\n",
    "\n",
    "                This approach follows standard practice since JFLEG's validation split is larger\n",
    "                and more suitable for training, while the test split is reserved for evaluation.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_split_ratio = test_split_ratio\n",
    "\n",
    "        # Validate test_split_ratio\n",
    "        if not 0.0 <= test_split_ratio <= 1.0:\n",
    "            raise ValueError(\n",
    "                f\"test_split_ratio must be between 0.0 and 1.0, got {test_split_ratio}\")\n",
    "\n",
    "        # Load the JFLEG datasets\n",
    "        print(f\"[INFO] Initializing JFLEG Dataset Processor...\")\n",
    "        print(\n",
    "            f\"[INFO] Max length: {max_length}, Test split ratio: {test_split_ratio:.1%}\")\n",
    "\n",
    "        self.train_data = load_dataset(\"jfleg\", split=\"validation\")\n",
    "        self.validation_data = load_dataset(\"jfleg\", split=\"test\")\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG validation split: {len(self.train_data)} examples\")\n",
    "        print(\n",
    "            f\"[INFO] Loaded JFLEG test split: {len(self.validation_data)} examples\")\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess and normalize text by fixing common formatting issues.\n",
    "\n",
    "        This method performs comprehensive text cleaning to handle poorly formatted\n",
    "        text, such as OCR output or text with inconsistent spacing. It fixes issues\n",
    "        with numbers, punctuation, quotes, and whitespace normalization.\n",
    "\n",
    "        Args:\n",
    "                        text (str): The input text to preprocess. Can be None or empty string.\n",
    "\n",
    "        Returns:\n",
    "                        str: The preprocessed and normalized text, or the original input if\n",
    "                                        it's not a valid string.\n",
    "\n",
    "        Transformations performed:\n",
    "                        - Removes multiple consecutive dashes (-- ‚Üí \"\")\n",
    "                        - Fixes decimal formatting (0 . 1 ‚Üí 0.1)\n",
    "                        - Fixes fraction formatting (1 / 2 ‚Üí 1/2)\n",
    "                        - Removes leading zeros in decimals (00.5 ‚Üí 0.5)\n",
    "                        - Joins split numbers (1 2 3 4 ‚Üí 1234)\n",
    "                        - Fixes punctuation spacing (word , ‚Üí word,)\n",
    "                        - Normalizes quote spacing (\" word \" ‚Üí \"word\")\n",
    "                        - Collapses multiple spaces to single spaces\n",
    "                        - Strips leading and trailing whitespace\n",
    "        \"\"\"\n",
    "\n",
    "        # if not text or not isinstance(text, str):\n",
    "        #     return text\n",
    "\n",
    "        # Step 1: Remove unwanted characters (double dashes, etc.)\n",
    "        text = re.sub(r\"-{2,}\", \"\", text)\n",
    "\n",
    "        # Step 2: Fix decimal numbers (0 . 1 ‚Üí 0.1)\n",
    "        text = re.sub(r\"(\\d+)\\s+\\.\\s+(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 3: Fix fractions (1 / 2 ‚Üí 1/2)\n",
    "        text = re.sub(r\"(\\d+)\\s+/\\s+(\\d+)\", r\"\\1/\\2\", text)\n",
    "\n",
    "        # Step 4: Fix leading zeros in decimals (00 . 5 ‚Üí 0.5)\n",
    "        text = re.sub(r\"\\b0+(\\d+)\\.(\\d+)\", r\"\\1.\\2\", text)\n",
    "\n",
    "        # Step 5: Split number handling (any length)\n",
    "        text = re.sub(r\"\\b(\\d+(?:\\s+\\d+)+)\\b\",\n",
    "                      lambda m: m.group(1).replace(\" \", \"\"), text)\n",
    "\n",
    "        # Step 6: Fix punctuation spacing (, . ! ? : ;)\n",
    "        text = re.sub(r\"\\s+([,.!?:;])\", r\"\\1\", text)\n",
    "\n",
    "        # Step 7: Fix double quote spacing\n",
    "        text = re.sub(r'\\s+\"', '\"', text)  # Remove space before quote\n",
    "        text = re.sub(r'\"\\s+', '\"', text)  # Remove space after quote\n",
    "\n",
    "        # Step 8: Normalize multiple spaces to single space\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        # Step 9: Remove leading/trailing spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _apply_augmentation(self, data, augment=True):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to JFLEG dataset using all available corrections.\n",
    "\n",
    "        This function processes JFLEG examples to create augmented data by utilizing\n",
    "        all 4 human-written corrections per sentence. Each original sentence is paired with\n",
    "        each of its corrections to create multiple training examples, significantly increasing\n",
    "        the dataset size and providing the model with diverse correction targets.\n",
    "\n",
    "        Args:\n",
    "                        data (List[Dict]): List of JFLEG dataset examples, where each example contains:\n",
    "                                        - 'sentence' (str): Original grammatically incorrect sentence\n",
    "                                        - 'corrections' (List[str]): List of 4 human-written corrections\n",
    "                        augment (bool, optional): Whether to use all corrections for augmentation.\n",
    "                                        - If True: Creates 4 examples per input (uses all corrections)\n",
    "                                        - If False: Creates 1 example per input (uses only first correction)\n",
    "                                        Default is True.\n",
    "\n",
    "        Returns:\n",
    "                        List[Dict]: Augmented dataset where each dictionary contains:\n",
    "                                        - 'input' (str): Preprocessed input with \"grammar: \" prefix\n",
    "                                        - 'target' (str): Preprocessed target correction\n",
    "                                        - 'processed_sentence' (str): Preprocessed original sentence\n",
    "                                        - 'processed_corrections' (List[str]): All 4 preprocessed corrections for evaluation\n",
    "                                        - 'raw_original' (str): Unprocessed original sentence (for debugging)\n",
    "                                        - 'raw_corrections' (List[str]): Unprocessed corrections (for debugging)\n",
    "        \"\"\"\n",
    "        # storage for augmented data\n",
    "        augmented_data = []\n",
    "        for items in data:\n",
    "            # getting original sentence -- incorrect\n",
    "            original_sentence = items[\"sentence\"]\n",
    "            # formatting the incorrect sentence\n",
    "            processed_sentence = self._preprocess(original_sentence)\n",
    "\n",
    "            # getting all the original corrected sentences\n",
    "            corrections = items[\"corrections\"]\n",
    "\n",
    "            # formatting all the corrected sentences -- evaluation\n",
    "            processed_corrections = []\n",
    "            # looping over all 4 corrections\n",
    "            for correction in corrections:\n",
    "                if correction.strip():  # Skip empty corrections\n",
    "                    # storing all the processed corrections\n",
    "                    processed_corrections.append(self._preprocess(correction))\n",
    "\n",
    "            # looping over processed corrections\n",
    "            for processed_correction in processed_corrections:\n",
    "                # creating a dataset\n",
    "                augmented_data.append({\n",
    "                    \"input\": f\"grammar: {processed_sentence}\",\n",
    "                    \"target\": processed_correction,\n",
    "                    \"processed_sentence\": processed_sentence,\n",
    "                    \"processed_corrections\": processed_corrections,\n",
    "                    \"raw_original\": original_sentence,\n",
    "                    \"raw_corrections\": corrections\n",
    "                })\n",
    "                # checking if to augment or not\n",
    "                if not augment:\n",
    "                    break\n",
    "        # displaying the length of data\n",
    "        print(\"[INFO] Length of Dataset is: \", len(augmented_data))\n",
    "        return augmented_data\n",
    "\n",
    "    def _apply_tokenization(self, data):\n",
    "        \"\"\"\n",
    "        Apply tokenization to preprocessed JFLEG dataset examples for T5 model training.\n",
    "\n",
    "        This function converts text data (input sentences and target corrections) into \n",
    "        tokenized format suitable for T5 model training. It processes both the input \n",
    "        grammar correction task and the target correction, creating the necessary \n",
    "        input_ids, attention_mask, and labels required by the HuggingFace Trainer.\n",
    "\n",
    "        Args:\n",
    "                        data (Dict): A single preprocessed example containing:\n",
    "                        - 'input' (str): Preprocessed input text with \"grammar: \" prefix\n",
    "                        - 'target' (str): Preprocessed target correction text\n",
    "                        - 'processed_sentence' (str): Preprocessed original sentence (preserved but not tokenized)\n",
    "                        - 'processed_corrections' (List[str]): All preprocessed corrections (preserved but not tokenized)\n",
    "                        - 'raw_original' (str): Raw original sentence (preserved but not tokenized)\n",
    "                        - 'raw_corrections' (List[str]): Raw corrections (preserved but not tokenized)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "                        Dict: Tokenized example ready for model training containing:\n",
    "                                        - 'input_ids' (List[int]): Token IDs for the input sequence\n",
    "                                        - 'attention_mask' (List[int]): Attention mask for input (1 for real tokens, 0 for padding)\n",
    "                                        - 'labels' (List[int]): Token IDs for the target sequence (used for loss computation)\n",
    "\n",
    "        Tokenization Settings:\n",
    "                        - max_length (int): Maximum sequence length (defined by self.max_length)\n",
    "                        - truncation (bool): True - truncates sequences longer than max_length\n",
    "                        - padding (bool): False - no padding applied (Trainer handles dynamic padding)\n",
    "                        - return_tensors: None - returns Python lists instead of PyTorch tensors\n",
    "        \"\"\"\n",
    "        # tokenizing the input of the dataset\n",
    "        input_encodings = self.tokenizer(data[\"input\"],\n",
    "                                         max_length=self.max_length,\n",
    "                                         truncation=True,\n",
    "                                         padding=False,  # trainer handles the dynamic padding\n",
    "                                         return_tensors=None)  # returns lists not tensor\n",
    "        # tokenizing the target of the dataset\n",
    "        target_encodings = self.tokenizer(data[\"target\"],\n",
    "                                          max_length=self.max_length,\n",
    "                                          truncation=True,\n",
    "                                          padding=False,  # trainer handles the dynamic padding\n",
    "                                          return_tensors=None)  # returns lists not tensor\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]\n",
    "        }\n",
    "\n",
    "    def create_train_val_test_datasets(self):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets with proper augmentation and tokenization.\n",
    "\n",
    "        This function orchestrates the complete data processing pipeline for JFLEG grammar \n",
    "        correction training. It applies data augmentation, converts to HuggingFace datasets,\n",
    "        applies tokenization, and splits the data into appropriate train/validation/test sets\n",
    "        while preserving essential evaluation metadata.\n",
    "\n",
    "        Processing Pipeline:\n",
    "                        1. Apply augmentation to training data (4x expansion using all corrections)\n",
    "                        2. Apply augmentation to validation data (no expansion, uses first correction only)\n",
    "                        3. Convert Python lists to HuggingFace Datasets\n",
    "                        4. Apply tokenization using .map() for efficiency\n",
    "                        5. Split validation data into validation and test sets (90%/10%)\n",
    "                        6. Preserve evaluation metadata for proper GLEU scoring\n",
    "\n",
    "        Data Sources:\n",
    "                        - Training: JFLEG validation split with 4x augmentation (~6,044 examples)\n",
    "                        - Validation/Test: JFLEG test split without augmentation, then split 90%/10%\n",
    "\n",
    "        Returns:\n",
    "                        Tuple[Dataset, List[Dict], List[Dict]]: A tuple containing:\n",
    "                                        - train_dataset (Dataset): HuggingFace Dataset with tokenized training examples\n",
    "                                        - val_data (List[Dict]): List of tokenized validation examples with metadata\n",
    "                                        - test_data (List[Dict]): List of tokenized test examples with metadata\n",
    "\n",
    "        Data Augmentation Strategy:\n",
    "                        - Training: augment=True (uses all 4 JFLEG corrections per sentence)\n",
    "                        - Validation: augment=False (uses only first correction per sentence)\n",
    "        \"\"\"\n",
    "\n",
    "        from datasets import Dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        print(\"[INFO] Creating datasets with augmentation and tokenization...\")\n",
    "\n",
    "        # Step 1: Apply augmentation (returns Python lists)\n",
    "        print(\"[INFO] Applying augmentation to training data...\")\n",
    "        train_augmented_list = self._apply_augmentation(\n",
    "            self.train_data, augment=True)\n",
    "\n",
    "        print(\"[INFO] Applying augmentation to validation data...\")\n",
    "        val_augmented_list = self._apply_augmentation(\n",
    "            self.validation_data, augment=False)\n",
    "\n",
    "        # Step 2: Convert Python lists to HuggingFace Datasets\n",
    "        train_augmented_data = Dataset.from_list(train_augmented_list)\n",
    "        val_augmented_data = Dataset.from_list(val_augmented_list)\n",
    "\n",
    "        # Step 3: Apply tokenization using map\n",
    "        print(\"\\n[INFO] Tokenizing training data...\")\n",
    "        train_augmented_map_data = train_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Training Data\"\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Tokenizing validation data...\")\n",
    "        val_augmented_map_data = val_augmented_data.map(\n",
    "            lambda example: self._apply_tokenization(example),\n",
    "            batched=False,\n",
    "            remove_columns=[\"input\", \"target\"],\n",
    "            desc=\"Tokenizing Validation Data\"\n",
    "        )\n",
    "\n",
    "        # Step 4: Split validation dataset into validation and test sets\n",
    "        print(\n",
    "            f\"\\n[INFO] Splitting Validation Data ({100-self.test_split_ratio*100:.0f}%/{self.test_split_ratio*100:.0f}%)...\")\n",
    "        val_data, test_data = train_test_split(\n",
    "            list(val_augmented_map_data),\n",
    "            test_size=self.test_split_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Convert Python validation and test lists to HuggingFace Datasets\n",
    "        val_data = Dataset.from_list(val_data)\n",
    "        test_data = Dataset.from_list(test_data)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nDataset Creation Complete:\")\n",
    "        print(f\"\\t[INFO] Training Dataset:   {len(train_augmented_map_data)}\")\n",
    "        print(f\"\\t[INFO] Validation Dataset: {len(val_data)}\")\n",
    "        print(f\"\\t[INFO] Test Dataset:       {len(test_data)}\")\n",
    "\n",
    "        return train_augmented_map_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15c221d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing JFLEG Dataset Processor...\n",
      "[INFO] Max length: 256, Test split ratio: 10.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded JFLEG validation split: 755 examples\n",
      "[INFO] Loaded JFLEG test split: 748 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = JFLEGDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea7efd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating datasets with augmentation and tokenization...\n",
      "[INFO] Applying augmentation to training data...\n",
      "[INFO] Length of Dataset is:  3016\n",
      "[INFO] Applying augmentation to validation data...\n",
      "[INFO] Length of Dataset is:  747\n",
      "\n",
      "[INFO] Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c56e3149344a1a9c949188c32beaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Training Data:   0%|          | 0/3016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4c1d72f9e04020aa7a27391cdc24e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Validation Data:   0%|          | 0/747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Splitting Validation Data (90%/10%)...\n",
      "\n",
      "Dataset Creation Complete:\n",
      "\t[INFO] Training Dataset:   3016\n",
      "\t[INFO] Validation Dataset: 672\n",
      "\t[INFO] Test Dataset:       75\n"
     ]
    }
   ],
   "source": [
    "train, val, test = dataset.create_train_val_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "544eddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3016\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5cc8cead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 672\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43a17b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['processed_sentence', 'processed_corrections', 'raw_original', 'raw_corrections', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 75\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "889badba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:this is a test for logger\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"this is a test for logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85561ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import nltk\n",
    "from nltk.translate.gleu_score import sentence_gleu, corpus_gleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38629390",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17f19c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da3c2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarEvaluation:\n",
    "    \"\"\"\n",
    "    Comprehensive Grammar Correction Evaluation Framework.\n",
    "\n",
    "    This class provides a complete evaluation suite for grammar correction systems,\n",
    "    implementing industry-standard metrics specifically designed for assessing\n",
    "    grammatical error correction quality. It combines fluency assessment (GLEU),\n",
    "    semantic preservation (BERTScore), linguistic quality (METEOR), and comprehensive\n",
    "    text statistics.\n",
    "\n",
    "    The evaluation framework is designed for transformer-based models like T5, BERT,\n",
    "    and other sequence-to-sequence architectures fine-tuned on datasets such as JFLEG,\n",
    "    BEA-2019, or custom grammar correction corpora.\n",
    "\n",
    "    Attributes:\n",
    "        bertscore: HuggingFace BERTScore evaluator for semantic similarity\n",
    "        meteor: HuggingFace METEOR evaluator for linguistic quality\n",
    "        metrics_result (Dict): Storage for all computed evaluation metrics\n",
    "\n",
    "    Performance Benchmarks:\n",
    "        - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "        - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent\n",
    "        - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "    References:\n",
    "        - GLEU: Napoles et al. (2017) \"JFLEG: A fluency corpus and benchmark\"\n",
    "        - BERTScore: Zhang et al. (2020) \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "        - METEOR: Banerjee & Lavie (2005) \"METEOR: An automatic metric for MT evaluation\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Grammar Evaluation framework.\n",
    "\n",
    "        Sets up evaluation metrics and initializes the results storage structure.\n",
    "        Loads pre-trained models for BERTScore and METEOR evaluation from HuggingFace.\n",
    "\n",
    "        Initializes:\n",
    "            - BERTScore evaluator with microsoft/deberta-xlarge-mnli model\n",
    "            - METEOR evaluator with default configuration\n",
    "            - Results dictionary with nested structure for all metrics\n",
    "\n",
    "        Raises:\n",
    "            ImportError: If required packages (pandas) are not installed\n",
    "\n",
    "        Note:\n",
    "            First initialization may take time to download evaluation models.\n",
    "            Internet connection required for downloading pre-trained models.\n",
    "        \"\"\"\n",
    "        # Load HuggingFace evaluation metrics\n",
    "        self.bertscore = load(\"bertscore\")  # Semantic similarity evaluation\n",
    "        self.meteor = load('meteor')        # Linguistic quality evaluation\n",
    "\n",
    "        # Initialize results storage with hierarchical structure\n",
    "        self.metrics_result = {\n",
    "            \"bertscore\": {              # Semantic preservation metrics\n",
    "                \"precision\": 0.0,       # BERTScore precision\n",
    "                \"recall\": 0.0,          # BERTScore recall\n",
    "                # BERTScore F1 (primary semantic metric)\n",
    "                \"f1\": 0.0\n",
    "            },\n",
    "            \"meteor\": 0.0,              # Linguistic quality score\n",
    "            \"gleu\": 0.0,                # Primary grammar correction metric\n",
    "            \"stats\": {}                 # Comprehensive text statistics\n",
    "        }\n",
    "\n",
    "    def compute_gleu(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute GLEU (Generalized Language Evaluation Understanding) scores for grammar correction.\n",
    "\n",
    "        GLEU is specifically designed for grammar correction evaluation and handles multiple\n",
    "        reference corrections better than traditional BLEU. It measures fluency improvement\n",
    "        while accounting for acceptable variation in correction approaches.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner list\n",
    "                contains multiple valid corrections for the same source sentence (e.g., JFLEG\n",
    "                provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average GLEU score in self.metrics_result[\"gleu\"].\n",
    "\n",
    "        Notes:\n",
    "            - Uses NLTK's sentence_gleu function for computation\n",
    "            - Applies lowercase normalization and word tokenization\n",
    "            - Handles empty or invalid references gracefully with exception handling\n",
    "            - Scores range from 0.0 (no match) to 1.0 (perfect match)\n",
    "            - For grammar correction: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "\n",
    "        Raises:\n",
    "            Exception: Catches and handles any GLEU computation errors by assigning 0.0 score.\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for gleu score\n",
    "        gleu_scores = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # converting predictions to tokens\n",
    "            preds_tokens = nltk.word_tokenize(preds.lower())\n",
    "            # converting all reference to tokens\n",
    "            refs_tokens = [nltk.word_tokenize(ref.lower())\n",
    "                           for ref in refs if ref.strip()]\n",
    "\n",
    "            try:\n",
    "                # computing the score for the gleu\n",
    "                score = sentence_gleu(refs_tokens, preds_tokens)\n",
    "                # updating the storage\n",
    "                gleu_scores.append(score)\n",
    "            except Exception:\n",
    "                gleu_scores.append(0.0)\n",
    "\n",
    "        # computing the average gleu\n",
    "        self.metrics_result[\"gleu\"] = np.mean(gleu_scores)\n",
    "\n",
    "    def compute_bertscore(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute BERTScore metrics (precision, recall, F1) for grammar correction evaluation.\n",
    "\n",
    "        BERTScore measures semantic similarity using contextual embeddings, making it ideal\n",
    "        for evaluating whether grammar corrections preserve semantic meaning while improving\n",
    "        fluency. For each prediction, scores are computed against all available references\n",
    "        and then averaged.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: Currently stores results in local variables but doesn't persist them.\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute BERTScore against each of its references\n",
    "            2. Average precision, recall, F1 across references for that prediction\n",
    "            3. Collect averaged scores across all predictions\n",
    "\n",
    "        Notes:\n",
    "            - Uses microsoft/deberta-xlarge-mnli for optimal semantic similarity detection\n",
    "            - Skips empty references automatically\n",
    "            - For grammar correction: F1 > 0.85 indicates excellent semantic preservation\n",
    "            - Each BERTScore call processes one prediction-reference pair\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for bertscore metrics\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing bertscore\n",
    "                    score = self.bertscore.compute(predictions=[preds],\n",
    "                                                   references=[ref],\n",
    "                                                   lang=\"en\",\n",
    "                                                   model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "                    # updating the local storage\n",
    "                    precision.append(score[\"precision\"][0])\n",
    "                    recall.append(score[\"recall\"][0])\n",
    "                    f1.append(score[\"f1\"][0])\n",
    "\n",
    "            # updating bertscore mertics with average\n",
    "            precisions.append(np.mean(precision))\n",
    "            recalls.append(np.mean(recall))\n",
    "            f1s.append(np.mean(f1))\n",
    "\n",
    "        # computing the average bertscore\n",
    "        self.metrics_result[\"bertscore\"] = {\n",
    "            \"precision\": np.mean(precisions),\n",
    "            \"recall\": np.mean(recalls),\n",
    "            \"f1\": np.mean(f1s)\n",
    "        }\n",
    "\n",
    "    def compute_meteor(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute METEOR (Metric for Evaluation of Translation with Explicit ORdering) scores \n",
    "        for grammar correction evaluation.\n",
    "\n",
    "        METEOR is particularly valuable for grammar correction as it incorporates:\n",
    "        - Exact word matching\n",
    "        - Stem matching (handles morphological variations like \"running\" vs \"runs\")\n",
    "        - Synonym matching (recognizes semantically equivalent words)\n",
    "        - Word order penalties\n",
    "\n",
    "        This makes it superior to BLEU for grammar correction where morphological changes\n",
    "        and lexical substitutions are common.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the average METEOR score in self.metrics_result[\"meteor\"].\n",
    "\n",
    "        Process:\n",
    "            1. For each prediction, compute METEOR against each of its references\n",
    "            2. Average METEOR scores across references for that prediction  \n",
    "            3. Average across all predictions for final score\n",
    "\n",
    "        Notes:\n",
    "            - METEOR scores range from 0.0 to 1.0 (higher is better)\n",
    "            - For grammar correction: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "            - Handles morphological variations better than BLEU\n",
    "            - Includes recall-oriented evaluation (unlike BLEU's precision focus)\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for meteor metrics\n",
    "        meteors = []\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for preds, refs in zip(predictions, references):\n",
    "            # storage for per prediction against its 4 references\n",
    "            meteor = []\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    # computing meteor\n",
    "                    meteor.append(self.meteor.compute(predictions=[preds],\n",
    "                                                      references=[ref])[\"meteor\"])\n",
    "\n",
    "            # updating meteor mertics with avergae\n",
    "            meteors.append(np.mean(meteor))\n",
    "\n",
    "        # computing the average meteor\n",
    "        self.metrics_result[\"meteor\"] = np.mean(meteors)\n",
    "\n",
    "    def compute_stats(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Compute comprehensive text statistics for grammar correction evaluation.\n",
    "\n",
    "        This method analyzes various aspects of model predictions versus reference corrections,\n",
    "        providing detailed insights into model behavior patterns, text properties, and \n",
    "        vocabulary usage. All statistics are stored in self.metrics_result[\"stats\"].\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            None: All statistics are stored in self.metrics_result[\"stats\"] dictionary.\n",
    "\n",
    "        Statistics Computed:\n",
    "\n",
    "            **Sample Information:**\n",
    "            - num_samples: Total number of predictions evaluated\n",
    "            - total_references: Total number of reference corrections across all sentences\n",
    "            - avg_references_per_sentence: Average references available per sentence\n",
    "\n",
    "            **Length Statistics (Word-level):**\n",
    "            - avg/min/max/std_prediction_length: Prediction length statistics in words\n",
    "            - avg/min/max/std_reference_length: Reference length statistics in words\n",
    "\n",
    "            **Character-level Statistics:**\n",
    "            - avg/std_prediction_char_length: Character count statistics for predictions\n",
    "            - avg/std_reference_char_length: Character count statistics for references\n",
    "\n",
    "            **Length Change Analysis:**\n",
    "            - avg/std_length_difference: Difference between prediction and first reference lengths\n",
    "            - positive_length_changes: Count where prediction > reference length (expansion)\n",
    "            - negative_length_changes: Count where prediction < reference length (compression)  \n",
    "            - no_length_changes: Count where prediction == reference length (preserved)\n",
    "\n",
    "            **Vocabulary Analysis:**\n",
    "            - unique_words_in_predictions: Unique word count in all predictions\n",
    "            - unique_words_in_references: Unique word count in all references\n",
    "            - vocab_overlap: Common words between predictions and references\n",
    "            - vocab_overlap_ratio: Overlap ratio (intersection/union of vocabularies)\n",
    "\n",
    "        Notes:\n",
    "            - Length differences computed against first reference for each sentence\n",
    "            - Word counting uses lowercase normalization\n",
    "            - Empty references are skipped in processing\n",
    "            - Vocabulary analysis helps assess model's lexical diversity\n",
    "        \"\"\"\n",
    "\n",
    "        # storage for computing statistics\n",
    "        pred_lengths = []                   # predictions length\n",
    "        pred_char_lengths = []              # predictions char length\n",
    "        all_ref_lengths = []                # reference length\n",
    "        all_ref_char_lengths = []           # reference char length\n",
    "        ref_counts = []                     # np. of reference per sentence\n",
    "        length_diffs = []                   # word difference in preds and refs\n",
    "        pred_word_counts = Counter()        # unique word counts in prediction\n",
    "        ref_word_counts = Counter()         # unique word counts in reference\n",
    "        # no. of len(prediction) > len(reference)\n",
    "        positive_changes = 0\n",
    "        # no. of len(prediction) < len(reference)\n",
    "        negative_changes = 0\n",
    "        # no. of len(prediction) == len(reference)\n",
    "        no_changes = 0\n",
    "\n",
    "        # looping over all predictions and references\n",
    "        for pred, refs in zip(predictions, references):\n",
    "            # prediction statistics\n",
    "            pred_len = len(pred.split())\n",
    "            pred_char_len = len(pred)\n",
    "            pred_lengths.append(pred_len)\n",
    "            pred_char_lengths.append(pred_char_len)\n",
    "\n",
    "            # prediction word counts\n",
    "            pred_word_counts.update(pred.lower().split())\n",
    "\n",
    "            # reference statistics\n",
    "            ref_counts.append(len(refs))\n",
    "\n",
    "            # looping for all references for this prediction\n",
    "            for ref in refs:\n",
    "                if ref.strip():\n",
    "                    ref_len = len(ref.split())\n",
    "                    ref_char_len = len(ref)\n",
    "                    all_ref_lengths.append(ref_len)\n",
    "                    all_ref_char_lengths.append(ref_char_len)\n",
    "                    ref_word_counts.update(ref.lower().split())\n",
    "\n",
    "            # length difference analysis (compare with first reference)\n",
    "            if refs:\n",
    "                ref_len = len(refs[0].split()) if refs[0].strip() else 0\n",
    "                length_diff = pred_len - ref_len\n",
    "                length_diffs.append(length_diff)\n",
    "\n",
    "                # counting changes\n",
    "                if length_diff > 0:\n",
    "                    positive_changes += 1\n",
    "                elif length_diff < 0:\n",
    "                    negative_changes += 1\n",
    "                else:\n",
    "                    no_changes += 1\n",
    "\n",
    "        # updating the stats in the metrics_result...\n",
    "\n",
    "        # Sample information (standardized naming)\n",
    "        self.metrics_result[\"stats\"][\"num_samples\"] = len(predictions)\n",
    "        self.metrics_result[\"stats\"][\"total_references\"] = sum(ref_counts)\n",
    "        self.metrics_result[\"stats\"][\"avg_references_per_sentence\"] = np.mean(\n",
    "            ref_counts)\n",
    "\n",
    "        # Prediction statistics\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_length\"] = np.mean(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"min_prediction_length\"] = np.min(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"max_prediction_length\"] = np.max(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_length\"] = np.std(\n",
    "            pred_lengths)\n",
    "        self.metrics_result[\"stats\"][\"avg_prediction_char_length\"] = np.mean(\n",
    "            pred_char_lengths)\n",
    "        self.metrics_result[\"stats\"][\"std_prediction_char_length\"] = np.std(\n",
    "            pred_char_lengths)\n",
    "\n",
    "        # Reference statistics\n",
    "        if all_ref_lengths:  # Handle empty case\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = np.mean(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = np.min(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = np.max(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = np.std(\n",
    "                all_ref_lengths)\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = np.mean(\n",
    "                all_ref_char_lengths)\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = np.std(\n",
    "                all_ref_char_lengths)\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"min_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"max_reference_length\"] = 0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"avg_reference_char_length\"] = 0.0\n",
    "            self.metrics_result[\"stats\"][\"std_reference_char_length\"] = 0.0\n",
    "\n",
    "        # Length difference statistics\n",
    "        if length_diffs:\n",
    "            self.metrics_result[\"stats\"][\"avg_length_difference\"] = np.mean(\n",
    "                length_diffs)\n",
    "            self.metrics_result[\"stats\"][\"std_length_difference\"] = np.std(\n",
    "                length_diffs)\n",
    "            # FIXED: Use pre-calculated variables instead of redundant sum() operations\n",
    "            self.metrics_result[\"stats\"][\"positive_length_changes\"] = positive_changes\n",
    "            self.metrics_result[\"stats\"][\"negative_length_changes\"] = negative_changes\n",
    "            self.metrics_result[\"stats\"][\"no_length_changes\"] = no_changes\n",
    "\n",
    "        # Vocabulary statistics\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_predictions\"] = len(\n",
    "            pred_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"unique_words_in_references\"] = len(\n",
    "            ref_word_counts)\n",
    "        self.metrics_result[\"stats\"][\"vocab_overlap\"] = len(\n",
    "            set(pred_word_counts.keys()) & set(ref_word_counts.keys()))\n",
    "\n",
    "        # Vocabulary overlap ratio\n",
    "        if len(pred_word_counts) > 0 and len(ref_word_counts) > 0:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = (\n",
    "                self.metrics_result[\"stats\"][\"vocab_overlap\"] /\n",
    "                len(set(pred_word_counts.keys()) | set(ref_word_counts.keys()))\n",
    "            )\n",
    "        else:\n",
    "            self.metrics_result[\"stats\"][\"vocab_overlap_ratio\"] = 0.0\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        \"\"\"\n",
    "        Perform comprehensive evaluation of grammar correction predictions.\n",
    "\n",
    "        This is the main evaluation method that computes all metrics and statistics\n",
    "        for grammar correction assessment. It provides a complete analysis including\n",
    "        fluency (GLEU), semantic preservation (BERTScore), linguistic quality (METEOR),\n",
    "        and comprehensive text statistics.\n",
    "\n",
    "        Args:\n",
    "            predictions (List[str]): List of model-generated grammar corrections.\n",
    "            references (List[List[str]]): List of reference correction lists. Each inner \n",
    "                list contains multiple valid corrections for the same source sentence \n",
    "                (e.g., JFLEG provides 4 human corrections per sentence).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete evaluation results containing:\n",
    "                - \"gleu\": GLEU score (float)\n",
    "                - \"meteor\": METEOR score (float) \n",
    "                - \"bertscore\": Dict with precision, recall, f1 scores\n",
    "                - \"stats\": Dict with comprehensive text statistics\n",
    "\n",
    "        Evaluation Metrics Computed:\n",
    "\n",
    "            **Core Grammar Correction Metrics:**\n",
    "            - GLEU: Primary metric for grammar correction fluency assessment\n",
    "            - BERTScore: Semantic preservation evaluation (precision, recall, F1)\n",
    "            - METEOR: Linguistic quality with morphological awareness\n",
    "\n",
    "            **Comprehensive Statistics:**\n",
    "            - Sample counts and reference information\n",
    "            - Length statistics (words and characters)\n",
    "            - Length change analysis\n",
    "            - Vocabulary analysis and overlap metrics\n",
    "\n",
    "        Performance Benchmarks:\n",
    "            - GLEU: >0.50 acceptable, >0.55 good, >0.60 excellent\n",
    "            - BERTScore F1: >0.75 acceptable, >0.80 good, >0.85 excellent  \n",
    "            - METEOR: >0.40 acceptable, >0.45 good, >0.55 excellent\n",
    "\n",
    "        Notes:\n",
    "            - Progress information is printed during computation\n",
    "            - Results are stored in self.metrics_result and returned\n",
    "            - Statistics are displayed as formatted pandas DataFrame\n",
    "            - All computations handle edge cases gracefully\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Evaluating {len(predictions)} Predictions...\")\n",
    "        # gleu score -- primary for grammar correction\n",
    "        print(\"\\t[INFO] Computing GLEU Score...\")\n",
    "        self.compute_gleu(predictions, references)\n",
    "\n",
    "        # bertscore -- sementic preservation\n",
    "        print(\"\\t[INFO] Computing BERTScore--Precision Recall & F1...\")\n",
    "        self.compute_bertscore(predictions, references)\n",
    "\n",
    "        # meteor -- linquistic quality\n",
    "        print(\"\\t[INFO] Computing METEOR Score...\")\n",
    "        self.compute_meteor(predictions, references)\n",
    "\n",
    "        # statistics\n",
    "        print(\"\\t[INFO] Computing Comprehensive Statistics...\")\n",
    "        self.compute_stats(predictions, references)\n",
    "\n",
    "        print(\"Evaluation Complete:\")\n",
    "        # printing the metrics...\n",
    "        print(f\"\\t[INFO] GLEU: {self.metrics_result['gleu']:.4f}\")\n",
    "        print(f\"\\t[INFO] METEOR: {self.metrics_result['meteor']:.4f}\")\n",
    "        print(\"\\t[INFO] BERTSCORE:\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Precision: {self.metrics_result['bertscore']['precision']:.4f}\")\n",
    "        print(\n",
    "            f\"\\t\\t[INFO] Recall: {self.metrics_result['bertscore']['recall']:.4f}\")\n",
    "        print(f\"\\t\\t[INFO] F1: {self.metrics_result['bertscore']['f1']:.4f}\")\n",
    "\n",
    "        # printing the statistics...\n",
    "        print(\"\\n\\t[INFO] Statistics:\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            stats_df = pd.DataFrame(\n",
    "                list(self.metrics_result[\"stats\"].items()),\n",
    "                columns=[\"Metric\", \"Value\"]\n",
    "            )\n",
    "            print(stats_df.to_string(index=False))\n",
    "        except ImportError:\n",
    "            print(\"\\t[WARNING] Pandas not available, printing raw statistics:\")\n",
    "            for key, value in self.metrics_result[\"stats\"].items():\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "\n",
    "        # Return complete results for further processing\n",
    "        return self.metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fcf2e9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "meteor = load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca80f83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [1.0],\n",
       " 'recall': [1.0],\n",
       " 'f1': [1.0],\n",
       " 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"hello there\"]\n",
    "references = [[\"hello there\", \"hi there\"]]\n",
    "bertscore.compute(predictions=predictions,\n",
    "                  references=[references[0][0]],\n",
    "                  lang=\"en\",\n",
    "                  model_type=\"microsoft/deberta-xlarge-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "21424b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': np.float64(0.9375)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor.compute(predictions=predictions, references=[\n",
    "               references[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc133ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}\n",
      "{'precision': [0.944251537322998], 'recall': [0.944251537322998], 'f1': [0.944251537322998], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.52.4)'}\n",
      "0.972125768661499 0.972125768661499 0.972125768661499\n"
     ]
    }
   ],
   "source": [
    "for preds, refs in zip(predictions, references):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            score = (bertscore.compute(predictions=[preds],\n",
    "                                       references=[ref],\n",
    "                                       lang=\"en\",\n",
    "                                       model_type=\"microsoft/deberta-xlarge-mnli\"))\n",
    "            print(score)\n",
    "            precision.append(score[\"precision\"][0])\n",
    "            recall.append(score[\"recall\"][0])\n",
    "            f1.append(score[\"f1\"][0])\n",
    "    print(np.mean(precision), np.mean(recall), np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "322a56fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello there', ['hello there', 'hi there'])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e89ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'there']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference statistics\n",
    "all_ref_lengths = []\n",
    "ref_counts = []\n",
    "\n",
    "for refs in val[\"processed_corrections\"]:\n",
    "    ref_counts.append(len(refs))\n",
    "    for ref in refs:\n",
    "        if ref.strip():\n",
    "            all_ref_lengths.append(len(ref.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5LoRATrainer:\n",
    "    def __init__(model_name=\"t5-small\", ):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
